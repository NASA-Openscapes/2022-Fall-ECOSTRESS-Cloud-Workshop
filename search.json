[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "Welcome to the 2022 Fall ECOSTRESS Cloud Workshop hosted by NASA’s Land Processes Distributed Activate Archive Center(LP DAAC) with support from NASA Openscapes.\nThe workshop will take place in-person on November 15, 2022 from 2:25pm-5:15pm PST (UTC -8).\nGET STARTED:\n- Deploy Jupyter Lab instance in 2i2c\n- Access notebooks without cloning"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "About",
    "text": "About\n\nWorkshop Goal\nThe goal of the workshop is expose ECOSTRESS data users to ECOSTRESS version 2 (v2) data products in the cloud. Learning objectives focus on how to find and access ECOSTRESS v2 data from Earthdata Cloud either by downloading or accessing the data in the cloud. The LP DAAC is the NASA archive for ECOSTRESS data products. ECOSTRESS v2 data products will hosted in the NASA Earthdata Cloud, hosted in AWS.\n\n\nWorkshop Description\nThe workshop will demonstrate how to find, access, and download ECOSTRESS v2 data from the Earthdata Cloud. Participants will learn how to search for and download data from NASA’s Earthdata Search Client, a graphical user interface (GUI) for search, discovery, and download application for also EOSDIS data assets. Participants will also learn about the Application for Extracting and Exploring Analysis Ready Samples (AppEEARS) to get the ECOSTRESS V2 data. Additionally, participants will learn how to perform in-could data search, access, and processing routines where no data download is required, and data analysis can take place next to the data in the cloud.\n\n\nWorkshop Outcomes\nParticipants should be able to find and access ECOSTRESS v2 data in the NASA Earthdata Cloud (hosted in AWS). Workshop materials will be available for future reference following the completion of the workshop/ECOSTRESS Science Team meeting."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n2022 Fall ECOSTRESS Cloud Workshop is hosted by NASA’s LP DAAC with support from the NASA Openscapes Project, with cloud computing infrastructure by 2i2c."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The ECOSTRESS Cloud Workshop take place on November 15th.\nNote, hands-on exercises will be executed from a Jupyter Lab instance in 2i2c. Please pass along your Github Username to get access."
  },
  {
    "objectID": "schedule.html#workshop-schedule",
    "href": "schedule.html#workshop-schedule",
    "title": "Schedule",
    "section": "Workshop Schedule",
    "text": "Workshop Schedule\n\n\n\nEvent\nLeads/Instructors\n\n\n\n\nEarthdata Cloud Overview\nMahsa Jami (LP DAAC)\n\n\nEarthdata Search Client (GUI)\nMahsa Jami (LP DAAC)\n\n\nAppEEARS\nMahsa Jami (LP DAAC)\n\n\nGetting set up\nMahsa Jami (LP DAAC)\n\n\nEarthdata Cloud: Search and Discovery - CMR API\nMahsa Jami (LP DAAC)\n\n\nEarthdata Authentication: Set up netrc file\nMahsa Jami (LP DAAC)\n\n\nEarthdata Cloud: Data Access\nMahsa Jami (LP DAAC)\n\n\nSalt Marsh Use Case\nGregory Halverson (JPL)\n\n\n\n\nThank you!\nJupyterHub: close out.\n\nclose out your JupyterHub instance if you are finished for the day, following these instructions.\n\nYou will continue to have access to the 2i2c JupyterHub in AWS for two weeks following the ECOSTRESS Cloud Workshop. You may use that time to continue work and all learn more about migrating data accass routines and science workflows to the Cloud. This cloud compute environment is supported by the NASA Openscapes project."
  },
  {
    "objectID": "schedule.html#getting-help-during-the-workshop",
    "href": "schedule.html#getting-help-during-the-workshop",
    "title": "Schedule",
    "section": "Getting help during the Workshop",
    "text": "Getting help during the Workshop\nPlease use the webex chat platform to ask any questions you have during the workshop."
  },
  {
    "objectID": "further-resources.html",
    "href": "further-resources.html",
    "title": "Additional resources",
    "section": "",
    "text": "One stop for PO.DAAC Cloud Information: Cloud Data page with About, Cloud Datasets, Access Data, FAQs, Resources, and Migration information\nAsk questions or find resources: PO.DAAC in the CLOUD Forum\nCloud user migration overview, guidance, and resources: PO.DAAC Webinar\nSearch and get access links: Earthdata Search Client and guide\nSearch and get access links: PO.DAAC Cloud Earthdata Search Portal\nBrowse cloud data in web-based browser: CMR Virtual Browse and guiding video\nScripted data search end-point: Earthdata Common Metadata Repository (CMR) API\nEnable data download or access: Obtain Earthdata Login Account\nDownload data regularly: PO.DAAC Data Subscriber Access video and PO.DAAC Data Subscriber instructions\nBulk Download guide\nOPeNDAP in the cloud\nPO.DAAC scripts and notebooks: PO.DAAC Github\nHow to get started in the AWS cloud: Earthdata Cloud Primer documents\n2021 NASA Cloud Hackathon - November 2021, co-hosted by PODAAC, NSIDC DAAC, and LPDAAC. Additional support is provided by ASDC, GESDISC, IMPACT, and Openscapes.\nNASA Earthdata: How to Cloud\nSetting up Jupyter Notebooks in a user EC2 instance in AWS - helpful blog post for setting up jupyter notebooks in an EC2 instance in AWS. (Builds on the Cloud Primer tutorials, which are missing that next step)"
  },
  {
    "objectID": "further-resources.html#additional-tutorials",
    "href": "further-resources.html#additional-tutorials",
    "title": "Additional resources",
    "section": "Additional tutorials",
    "text": "Additional tutorials\n\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links\nDirect access to ECCO data in S3 (from us-west-2) - Direct S3 access example with netCDF data\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nCalculate black-sky, white-sky, and actual albedo (MCD43A) from MCD43A1 BRDF Parameters using R\nXarray Zonal Statistics"
  },
  {
    "objectID": "prerequisites/index.html",
    "href": "prerequisites/index.html",
    "title": "Prerequisites",
    "section": "",
    "text": "To follow along hands-on during the Workshop, please do the following (20 minutes). All software or accounts are free.\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com. Follow optional advice on choosing your username\nYour GitHub username is used to enable you access to a cloud environment during the workshop.\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov\nRemember your username and password; you will need to download or access cloud data during the workshop and beyond.\n\nNetrc file\n\nCreate a netrc file using your Earthdata login credentials using the instruction.\nThis file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\nLaptop or tablet\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2.\n\nSlack\n\nWhile we will be taking questions in person, we will also be communicating and recording questions via Slack. To take part in the conversations please ask Christine Lee or Gregory Halverson to add you to the ECOSTRESS Slack workspace."
  },
  {
    "objectID": "cloud-paradigm.html",
    "href": "cloud-paradigm.html",
    "title": "NASA and the Cloud Paradigm",
    "section": "",
    "text": "Slides that introduce NASA Earthdata Cloud & the Cloud Paradigm."
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Overview",
    "section": "",
    "text": "These tutorials are a combination of narrative, links, code, and outputs. They have been developed for live demos during the Workshop, and are available for self-paced learning.\nHands-on exercises will be executed from a Jupyter Lab instance in 2i2c. Please pass along your Github Username to get access.\nTutorials are markdown (.md) and Jupyter (.ipynb) notebooks, and are available on GitHub:\nhttps://github.com/NASA-Openscapes/2022-Fall-ECOSTRESS-Cloud-Workshop/."
  },
  {
    "objectID": "tutorials/additional_resources/Finding_collection_s3_location.html",
    "href": "tutorials/additional_resources/Finding_collection_s3_location.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "We need to determine the path for our products of interest. We can do this through several mechanisms.\n\n\nThe easiest of which is through the PO.DAAC Cloud Dataset Listing page: https://podaac.jpl.nasa.gov/cloud-datasets\n\n\n\nS3 Data Locations from Portal\n\n\nFor each dataset, the ‘Data Access’ tab will have various information, but will always contain the S3 paths listed specifically. Data files will always be found under the ‘protected’ bucket.\n\n\n\nFrom the Earthdata Search Client (search.earthdata.nasa.gov), collection level information can be found by clicking the ‘i’ on a collection search result. An example of this is seen below:\n\n\n\nS3 Data Locations from Search 1\n\n\nOnce on the collection inforamtion screen, the S3 bucket locations can be found by scrolling to the bottom of the information panel. The SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1 example is shown below.\n\n\n\nS3 Data Locations from Search 2\n\n\n\n\n\nOne can query the collection identifier to get information from CMR:\nhttps://cmr.earthdata.nasa.gov/search/concepts/C2152045877-POCLOUD.umm_json\nThe identifier is found on the PO.DAAC Cloud Data Set Listing page entries, called ‘Collection Concept ID’\nResults returned will look like the following:\n{\n    ...\n    \"DirectDistributionInformation\": {\n        \"Region\": \"us-west-2\",\n        \"S3BucketAndObjectPrefixNames\": [\n            \"podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/\",\n            \"podaac-ops-cumulus-public/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/\"\n        ],\n        \"S3CredentialsAPIEndpoint\": \"https://archive.podaac.earthdata.nasa.gov/s3credentials\",\n        \"S3CredentialsAPIDocumentationURL\": \"https://archive.podaac.earthdata.nasa.gov/s3credentialsREADME\"\n    },\n    ...\n}"
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html",
    "href": "tutorials/additional_resources/Data_Discovery_CMR-STAC_API.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "Exercise: 30 min\n\n\n\n\nIn this example we will access the NASA’s Harmonized Landsat Sentinel-2 (HLS) version 2 assets, which are archived in cloud optimized geoTIFF (COG) format in the LP DAAC Cumulus cloud space. The COGs can be used like any other geoTIFF file, but have some added features that make them more efficient within the cloud data access paradigm. These features include: overviews and internal tiling. Below we will demonstrate how to leverage these features.\n\n\nSpatioTemporal Asset Catalog (STAC) is a specification that provides a common language for interpreting geospatial information in order to standardize indexing and discovering data.\nThe STAC specification is made up of a collection of related, yet independent specifications that when used together provide search and discovery capabilities for remote assets.\n\n\nSTAC Catalog (aka DAAC Archive)\nSTAC Collection (aka Data Product)\nSTAC Item (aka Granule)\nSTAC API\nIn the following sections, we will explore each of STAC element using NASA’s Common Metadata Repository (CMR) STAC application programming interface (API), or CMR-STAC API for short.\n\n\n\n\nThe CMR-STAC API is NASA’s implementation of the STAC API specification for all NASA data holdings within EOSDIS. The current implementation does not allow for querries accross the entire NASA catalog. Users must execute searches within provider catalogs (e.g., LPCLOUD) to find the STAC Items they are searching for. All the providers can be found at the CMR-STAC endpoint here: https://cmr.earthdata.nasa.gov/stac/.\nIn this exercise, we will query the LPCLOUD provider to identify STAC Items from the Harmonized Landsat Sentinel-2 (HLS) collection that fall within our region of interest (ROI) and within our specified time range.\n\n\n\n\n\nhow to connect to NASA CMR-STAC API using Python’s pystac-client\n\nhow to navigate CMR-STAC records\n\nhow to read in a geojson file using geopandas to specify your region of interest\nhow to use the CMR-STAC API to search for data\nhow to perform post-search filtering of CMR-STAC API search result in Python\n\nhow to extract and save data access URLs for geospatial assets\n\nThis exercise can be found in the 2021 Cloud Hackathon Book\n\n\n\n\n\nfrom pystac_client import Client  \nfrom collections import defaultdict    \nimport json\nimport geopandas\nfrom shapely.geometry import Polygon\nimport geoviews as gv\nfrom cartopy import crs\nfrom pyproj import CRS\ngv.extension('bokeh', 'matplotlib')\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n  \n  \n\n\n\n\n\n\n\n\n\nSTAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n\n\n\n\nprovider_cat = Client.open(STAC_URL)\n\nWe’ll create a providers variable so we can take a deeper look into available data providers - subcategories are referred to as “children”. We can then print them as a for loop.\n\nproviders = [p for p in provider_cat.get_children()]\n\nfor count, provider in enumerate(providers):\n    print(f'{count} - {provider.title}')\n\n0 - LARC_ASDC\n1 - USGS_EROS\n2 - ESA\n3 - GHRC\n4 - LAADS\n5 - OBPG\n6 - OB_DAAC\n7 - ECHO\n8 - ISRO\n9 - LPCUMULUS\n10 - EDF_DEV04\n11 - GES_DISC\n12 - ASF\n13 - OMINRT\n14 - EUMETSAT\n15 - NCCS\n16 - NSIDCV0\n17 - PODAAC\n18 - LARC\n19 - USGS\n20 - SCIOPS\n21 - LANCEMODIS\n22 - CDDIS\n23 - JAXA\n24 - AU_AADC\n25 - ECHO10_OPS\n26 - LPDAAC_ECS\n27 - NSIDC_ECS\n28 - ORNL_DAAC\n29 - LM_FIRMS\n30 - SEDAC\n31 - LANCEAMSR2\n32 - NOAA_NCEI\n33 - USGS_LTA\n34 - GESDISCCLD\n35 - GHRSSTCWIC\n36 - ASIPS\n37 - ESDIS\n38 - POCLOUD\n39 - NSIDC_CPRD\n40 - ORNL_CLOUD\n41 - FEDEO\n42 - MLHUB\n43 - XYZ_PROV\n44 - GHRC_DAAC\n45 - CSDA\n46 - NRSCC\n47 - CEOS_EXTRA\n48 - AMD_KOPRI\n49 - MOPITT\n50 - GHRC_CLOUD\n51 - LPCLOUD\n52 - CCMEO\n\n\n\n\n\n\nFor this next step we need the provider title (e.g., LPCLOUD) from above. We will add the provider to the end of the CMR-STAC API URL (i.e., https://cmr.earthdata.nasa.gov/stac/) to connect to the LPCLOUD STAC Catalog.\n\ncatalog = Client.open(f'{STAC_URL}/LPCLOUD/')\n\nSince we are using a dedicated client (i.e., pystac-client.Client) to connect to our STAC Provider Catalog, we will have access to some useful internal methods and functions (e.g., get_children() or get_all_items()) we can use to get information from these objects.\n\n\n\nWe’ll create a products variable to view deeper in the STAC Catalog.\n\nproducts = [c for c in catalog.get_children()]\n\n\n\nTo view the products variable we just created, let’s look at one entry as a dictionary.\n\nproducts[1].to_dict()\n\n{'type': 'Collection',\n 'id': 'HLSL30.v2.0',\n 'stac_version': '1.0.0',\n 'description': 'The Harmonized Landsat and Sentinel-2 (HLS) project provides consistent surface reflectance (SR) and top of atmosphere (TOA) brightness data from the Operational Land Imager (OLI) aboard the joint NASA/USGS Landsat 8 satellite and the Multi-Spectral Instrument (MSI) aboard Europe’s Copernicus Sentinel-2A and Sentinel-2B satellites. The combined measurement enables global observations of the land every 2–3 days at 30-meter (m) spatial resolution. The HLS project uses a set of algorithms to obtain seamless products from OLI and MSI that include atmospheric correction, cloud and cloud-shadow masking, spatial co-registration and common gridding, illumination and view angle normalization, and spectral bandpass adjustment.\\r\\n\\r\\nThe HLSL30 product provides 30-m Nadir Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Landsat 8 OLI data products. The HLSS30 and HLSL30 products are gridded to the same resolution and Military Grid Reference System ([MGRS](https://hls.gsfc.nasa.gov/products-description/tiling-system/)) tiling system, and thus are “stackable” for time series analysis.\\r\\n\\r\\nThe HLSL30 product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate file. There are 11 bands included in the HLSL30 product along with one quality assessment (QA) band and four angle bands. See the User Guide for a more detailed description of the individual bands provided in the HLSL30 product.',\n 'links': [{'rel': <RelType.ROOT: 'root'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/',\n   'type': <MediaType.JSON: 'application/json'>,\n   'title': 'NASA CMR STAC Proxy'},\n  {'rel': 'items',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/items',\n   'type': 'application/json',\n   'title': 'Granules in this collection'},\n  {'rel': 'about',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2021957657-LPCLOUD.html',\n   'type': 'text/html',\n   'title': 'HTML metadata for collection'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C2021957657-LPCLOUD.json',\n   'type': 'application/json',\n   'title': 'CMR JSON metadata for collection'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2013',\n   'type': 'application/json',\n   'title': '2013 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2014',\n   'type': 'application/json',\n   'title': '2014 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2015',\n   'type': 'application/json',\n   'title': '2015 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2016',\n   'type': 'application/json',\n   'title': '2016 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2017',\n   'type': 'application/json',\n   'title': '2017 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2018',\n   'type': 'application/json',\n   'title': '2018 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2019',\n   'type': 'application/json',\n   'title': '2019 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2020',\n   'type': 'application/json',\n   'title': '2020 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2021',\n   'type': 'application/json',\n   'title': '2021 catalog'},\n  {'rel': 'child',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/2022',\n   'type': 'application/json',\n   'title': '2022 catalog'},\n  {'rel': <RelType.SELF: 'self'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0',\n   'type': <MediaType.JSON: 'application/json'>},\n  {'rel': <RelType.PARENT: 'parent'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/',\n   'type': <MediaType.JSON: 'application/json'>,\n   'title': 'LPCLOUD'}],\n 'stac_extensions': [],\n 'title': 'HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0',\n 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'temporal': {'interval': [['2013-04-11T00:00:00Z', None]]}},\n 'license': 'not-provided'}\n\n\n\n\n\nIn the above output, id and title are two elements of interest that we can print for all products using a for loop.\n\nfor p in products: \n    print(f\"{p.id}: {p.title}\")\n\nASTGTM.v003: ASTER Global Digital Elevation Model V003\nHLSL30.v2.0: HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0\nHLSS30.v2.0: HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0\n\n\n\n\n\n\nWe will define our ROI using a geojson file containing a small polygon feature in western Nebraska, USA. The geojson file is found in the ~/data directory. We’ll also specify the data collections and a time range for our example.\n\n\nReading in a geojson file with geopandas will return the geometry of our polygon (our ROI).\nNOTE: If you are running the notebook from the tutorials-templates directory, please use the following path to connect to the geojson file: “../tutorials/data/ne_w_agfields.geojson”\n\npoints = geopandas.read_file('../data/TNC_fall_2020.geojson')\npoints\n\n\n\n\n\n  \n    \n      \n      planting_date\n      health\n      geometry\n    \n  \n  \n    \n      0\n      2019-05-10\n      good\n      POINT Z (-120.41578 34.52600 191.30000)\n    \n    \n      1\n      2019-05-10\n      good\n      POINT Z (-120.41590 34.52611 194.80000)\n    \n    \n      2\n      2019-05-10\n      poor\n      POINT Z (-120.41589 34.52611 194.20000)\n    \n    \n      3\n      2019-05-10\n      good\n      POINT Z (-120.41591 34.52618 192.80000)\n    \n    \n      4\n      2019-05-10\n      good\n      POINT Z (-120.41600 34.52621 188.90000)\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      4058\n      2020-10-05\n      good\n      POINT Z (-120.41725 34.53041 0.00000)\n    \n    \n      4059\n      2020-10-05\n      good\n      POINT Z (-120.41718 34.53040 0.00000)\n    \n    \n      4060\n      2020-10-05\n      good\n      POINT Z (-120.41710 34.53039 0.00000)\n    \n    \n      4061\n      2020-10-06\n      good\n      POINT Z (-120.40952 34.52643 0.00000)\n    \n    \n      4062\n      2021-08-26\n      good\n      POINT Z (-120.41266 34.52768 207.56970)\n    \n  \n\n4063 rows × 3 columns\n\n\n\n\n\n\nWe can use that geometry to visualize the polygon: here, a square. But wait for it –\n\nminx, miny, maxx, maxy = points.geometry.total_bounds\nminx, miny, maxx, maxy\n\n(-120.45264628215773,\n 34.51050622261265,\n -120.40432447545712,\n 34.532398755692206)\n\n\nGeoJSON polygon boundary coordinates must be ordered counterclockwise\n\ncoords = [\n    (minx, miny),\n    (maxx, miny),\n    (maxx, maxy),\n    (minx, maxy)\n]\n\n\nfeature_shape = Polygon(coords)\nfeature_shape\n\n\n\n\nWe can plot the polygon using the geoviews package that we imported as gv with ‘bokeh’ and ‘matplotlib’ extensions. The following has reasonable width, height, color, and line widths to view our polygon when it is overlayed on a base tile map.\n\nbase = gv.tile_sources.EsriImagery.opts(width=650, height=500)\nfarmField = gv.Polygons(feature_shape).opts(line_color='yellow', line_width=10, color=None)\nbase * farmField\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe will now start to specify the search criteria we are interested in, i.e, the date range, the ROI, and the data collections, that we will pass to the STAC API.\n\n\n\n\ncrs = CRS('epsg:4326')\npolygon = geopandas.GeoDataFrame(index=[0], crs=crs, geometry=[feature_shape])\npolygon\n\n\n\n\n\n  \n    \n      \n      geometry\n    \n  \n  \n    \n      0\n      POLYGON ((-120.45265 34.51051, -120.40432 34.5...\n    \n  \n\n\n\n\n\nroi = json.loads(polygon.to_json())['features'][0]['geometry']\nroi\n\n{'type': 'Polygon',\n 'coordinates': [[[-120.45264628215773, 34.51050622261265],\n   [-120.40432447545712, 34.51050622261265],\n   [-120.40432447545712, 34.532398755692206],\n   [-120.45264628215773, 34.532398755692206],\n   [-120.45264628215773, 34.51050622261265]]]}\n\n\nSo, what just happen there? Let’s take a quick detour to break it down.\n\n\n\n\nNext up is to specify our date range using ISO_8601 date formatting.\n\n#date_range = \"2021-05-01T00:00:00Z/2021-08-30T23:59:59Z\"    # closed interval\n#date_range = \"2021-05-01T00:00:00Z/..\"                      # open interval - does not currently work with the CMR-STAC API\n#date_range = \"2021-05/2021-08\"\ndate_range = \"2020-09\"\n\n\n\n\nSTAC Collection is synonomous with what we usually consider a NASA data product. Desired STAC Collections are submitted to the search API as a list containing the collection id. We can use the ids that we printed from our products for loop above. Let’s focus on S30 and L30 collections.\n\ncollections = ['HLSL30.v2.0', 'HLSS30.v2.0']\ncollections\n\n['HLSL30.v2.0', 'HLSS30.v2.0']\n\n\n\n\n\n\nNow we can put all our search criteria together using catalog.search from the pystac_client package.\n\nsearch = catalog.search(\n    collections=collections,\n    intersects=roi,\n    datetime=date_range,\n    limit=100\n)\n\n\n\n\nsearch.matched()\n\n4\n\n\nWe now have a search object containing the STAC Items that matched our query. Now, let’s pull out all of the STAC Items (as a PySTAC ItemCollection object) and explore the contents (i.e., the STAC Items)\n\nitem_collection = search.get_all_items()\n\nLet’s list some of the Items from our pystac item_collection:\n\nlist(item_collection)[0:5]\n\n[<Item id=HLS.L30.T10SGD.2020247T184028.v2.0>,\n <Item id=HLS.L30.T10SGD.2020256T183444.v2.0>,\n <Item id=HLS.L30.T10SGD.2020263T184033.v2.0>,\n <Item id=HLS.L30.T10SGD.2020272T183449.v2.0>]\n\n\nWe can view a single Item as a dictionary, as we did above with STAC Collections/products.\n\nitem_collection[0].to_dict()\n\n{'type': 'Feature',\n 'stac_version': '1.0.0',\n 'id': 'HLS.L30.T10SGD.2020247T184028.v2.0',\n 'properties': {'datetime': '2020-09-03T18:40:28.257000Z',\n  'start_datetime': '2020-09-03T18:40:28.257Z',\n  'end_datetime': '2020-09-03T18:40:52.144Z',\n  'eo:cloud_cover': 48},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-120.464742, 34.2267025],\n    [-120.1745662, 35.2100913],\n    [-120.8028976, 35.2231329],\n    [-120.8289365, 34.2336951],\n    [-120.464742, 34.2267025]]]},\n 'links': [{'rel': 'self',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/items/HLS.L30.T10SGD.2020247T184028.v2.0'},\n  {'rel': 'parent',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0'},\n  {'rel': 'collection',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0'},\n  {'rel': <RelType.ROOT: 'root'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/',\n   'type': <MediaType.JSON: 'application/json'>,\n   'title': 'LPCLOUD'},\n  {'rel': 'provider', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2153816661-LPCLOUD.json'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2153816661-LPCLOUD.umm_json'}],\n 'assets': {'B06': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B06.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B06.tif'},\n  'VZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.VZA.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.VZA.tif'},\n  'SAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.SAA.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.SAA.tif'},\n  'B11': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B11.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B11.tif'},\n  'VAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.VAA.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.VAA.tif'},\n  'B02': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B02.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B02.tif'},\n  'SZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.SZA.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.SZA.tif'},\n  'B09': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B09.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B09.tif'},\n  'B03': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B03.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B03.tif'},\n  'B07': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B07.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B07.tif'},\n  'B10': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B10.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B10.tif'},\n  'B04': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B04.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B04.tif'},\n  'B05': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B05.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B05.tif'},\n  'Fmask': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.Fmask.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.Fmask.tif'},\n  'B01': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B01.tif',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.B01.tif'},\n  'browse': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.jpg',\n   'type': 'image/jpeg',\n   'title': 'Download HLS.L30.T10SGD.2020247T184028.v2.0.jpg'},\n  'metadata': {'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2153816661-LPCLOUD.xml',\n   'type': 'application/xml'}},\n 'bbox': [-120.828937, 34.226702, -120.174566, 35.223133],\n 'stac_extensions': ['https://stac-extensions.github.io/eo/v1.0.0/schema.json'],\n 'collection': 'HLSL30.v2.0'}\n\n\n\n\n\n\nWhile the CMR-STAC API is a powerful search and discovery utility, it is still maturing and currently does not have the full gamut of filtering capabilities that the STAC API specification allows for. Hence, additional filtering is required if we want to filter by a property, for example cloud cover. Below we will loop through and filter the item_collection by a specified cloud cover as well as extract the band we’d need to do an Enhanced Vegetation Index (EVI) calculation for a future analysis.\nWe’ll make a cloudcover variable where we will set the maximum allowable cloud cover and extract the band links for those Items that match or are less than the max cloud cover.\n\ncloudcover = 100\n\nWe will also specify the STAC Assets (i.e., bands/layers) of interest for both the S30 and L30 collections (also in our collections variable above).\nIn this hypothetical workflow, we’ll extract the bands needed to calculate an enhanced vegetation index (EVI). Thus, the band needed include red, near infrared (NIR), and blue. We’ll also extract a quality band (i.e., Fmask) that we’d eventually use to perform per-pixel quality filtering.\nNotice that the band ids are in some case not one-to-one between the S30 and the L30 product. This is evident in the NIR band for each product where S30’s NIR band id is B8A and L30’s is B05. Note, the S30 product has an additional NIR band with a band id of B08, but the spectral ranges between B8A and B05 are more closely aligned. Visit the HLS Overview page to learn more about HLS spectral bands.\n\ns30_bands = ['B8A', 'B04', 'B02', 'Fmask']    # S30 bands for EVI calculation and quality filtering -> NIR, RED, BLUE, Quality \nl30_bands = ['B05', 'B04', 'B02', 'Fmask']    # L30 bands for EVI calculation and quality filtering -> NIR, RED, BLUE, Quality \n\nAnd now to loop through and filter the item_collection by cloud cover and bands:\n\nevi_band_links = []\n\nfor i in item_collection:\n    if i.properties['eo:cloud_cover'] <= cloudcover:\n        if i.collection_id == 'HLSS30.v2.0':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = s30_bands\n        elif i.collection_id == 'HLSL30.v2.0':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = l30_bands\n\n        for a in i.assets:\n            if any(b==a for b in evi_bands):\n                evi_band_links.append(i.assets[a].href)\n\nThe filtering done in the previous steps produces a list of links to STAC Assets. Let’s print out the first ten links.\n\nevi_band_links[:10]\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B02.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B05.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.Fmask.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020256T183444.v2.0/HLS.L30.T10SGD.2020256T183444.v2.0.Fmask.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020256T183444.v2.0/HLS.L30.T10SGD.2020256T183444.v2.0.B02.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020256T183444.v2.0/HLS.L30.T10SGD.2020256T183444.v2.0.B05.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020256T183444.v2.0/HLS.L30.T10SGD.2020256T183444.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020263T184033.v2.0/HLS.L30.T10SGD.2020263T184033.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020263T184033.v2.0/HLS.L30.T10SGD.2020263T184033.v2.0.Fmask.tif']\n\n\nNOTE that HLS data is mapped to the Universal Transverse Mercator (UTM) projection and is tiled using the Sentinel-2 Military Grid Reference System (MGRS) UTM grid. Notice that in the list of links we have multiple tiles, i.e. T14TKL & T13TGF, that intersect with our region of interest. In this case, these two tiles represent neighboring UTM zones. The tiles can be discern from the file name, which is the last element in a link (far right) following the last forward slash (/) - e.g., HLS.L30.T14TKL.2021133T172406.v1.5.B04.tif. The figure below explains where to find the tile/UTM zone from the file name.\n\nWe will now split the list of links into separate logical sub-lists.\n\n\n\nWe have a list of links to data assets that meet our search and filtering criteria. Below we’ll split our list from above into lists first by tile/UTM zone and then further by individual bands bands. The commands that follow will do the splitting with python routines.\n\n\n\ntile_dicts = defaultdict(list)    # https://stackoverflow.com/questions/26367812/appending-to-list-in-python-dictionary\n\n\nfor l in evi_band_links:\n    tile = l.split('.')[-6]\n    tile_dicts[tile].append(l)\n\n\n\n\ntile_dicts.keys()\n\ndict_keys(['T10SGD'])\n\n\n\ntile_dicts['T10SGD'][:5]\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B02.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B05.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.Fmask.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020256T183444.v2.0/HLS.L30.T10SGD.2020256T183444.v2.0.Fmask.tif']\n\n\nNow we will create a separate list of data links for each tile\n\ntile_links_T10SGD = tile_dicts['T10SGD']\n\n\n\n\n\n# tile_links_T10SGD[:10]\n\n\n\n\n\n\nbands_dicts = defaultdict(list)\n\n\nfor b in tile_links_T10SGD:\n    band = b.split('.')[-2]\n    bands_dicts[band].append(b)\n\n\nbands_dicts.keys()\n\ndict_keys(['B02', 'B04', 'B05', 'Fmask'])\n\n\n\nbands_dicts['B04']\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020247T184028.v2.0/HLS.L30.T10SGD.2020247T184028.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020256T183444.v2.0/HLS.L30.T10SGD.2020256T183444.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020263T184033.v2.0/HLS.L30.T10SGD.2020263T184033.v2.0.B04.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020272T183449.v2.0/HLS.L30.T10SGD.2020272T183449.v2.0.B04.tif']\n\n\n\n\n\n\nTo complete this exercise, we will save the individual link lists as separate text files with descriptive names.\nNOTE: If you are running the notebook from the tutorials-templates directory, please use the following path to write to the data directory: “../tutorials/data/{name}”\n\n\n\nfor k, v in bands_dicts.items():\n    name = (f'HTTPS_T10SGD_{k}_Links.txt')\n    with open(f'../data/{name}', 'w') as f:    # use ../tutorials/data/{name} as your path if running the notebook from \"tutorials-template\"\n        for l in v:\n            f.write(f\"{l}\" + '\\n')\n\n\n\n\n\nfor k, v in bands_dicts.items():\n    name = (f'S3_T10SGD_{k}_Links.txt')\n    with open(f'../data/{name}', 'w') as f:    # use ../tutorials/data/{name} as your path if running the notebook from \"tutorials-template\"\n        for l in v:\n            s3l = l.replace('https://data.lpdaac.earthdatacloud.nasa.gov/', 's3://')\n            f.write(f\"{s3l}\" + '\\n')\n\n\n\n\n\n\n\nSTAC Specification Webpage\nSTAC API Documentation\nCMR-STAC API Github\nPySTAC Client Documentation\nhttps://stackoverflow.com/questions/26367812/appending-to-list-in-python-dictionary\nGeopandas\nHLS Overview"
  },
  {
    "objectID": "tutorials/additional_resources/On-prem_Cloud_example.html",
    "href": "tutorials/additional_resources/On-prem_Cloud_example.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "This tutorial will combine several workflow steps and components from the previous days, demonstrating the process of using the geolocation of data available outside of the Earthdata Cloud to then access coincident variables of cloud-accessible data. This may be a common use case as NASA Earthdata continues to migrate to the cloud, producing a “hybrid” data archive across Amazon Web Services (AWS) and original on-premise data storage systems. Additionally, you may also want to combine field measurements with remote sensing data available on the Earthdata Cloud.\nThis specific example explores the pairing of the ICESat-2 ATL07 Sea Ice Height data product, currently (as of November 2021) available publicly via direct download at the NSIDC DAAC, along with Sea Surface Temperature (SST) from the GHRSST MODIS L2 dataset (MODIS_A-JPL-L2P-v2019.0) available from PO.DAAC on the Earthdata Cloud.\nThe use case we’re looking at today centers over an area north of Greenland for a single day in June, where a melt pond was observed using the NASA OpenAltimetry application. Melt ponds are an important feature of Arctic sea ice dynamics, leading to an decrease in sea ice albedo and other changes in heat balance. Many NASA Earthdata datasets produce variables including sea ice albedo, sea surface temperature, air temperature, and sea ice height, which can be used to better understand these dynamics.\n\n\n\n\n\nAWS instance running in us-west 2\nEarthdata Login\n.netrc file\n\n\n\n\n\n\nSearch for data programmatically using the Common Metadata Repository (CMR), determining granule (file) coverage across two datasets over an area of interest.\nDownload data from an on-premise storage system to our cloud environment.\nRead in 1-dimensional trajectory data (ICESat-2 ATL07) into xarray and perform attribute conversions.\nSelect and read in sea surface temperature (SST) data (MODIS_A-JPL-L2P-v2019.0) from the Earthdata Cloud into xarray.\nExtract, resample, and plot coincident SST data based on ICESat-2 geolocation.\n\n\n\n\n\n\nimport os\nfrom pathlib import Path\nfrom pprint import pprint\n\n# Access via download\nimport requests\n\n# Access AWS S3\nimport s3fs\n\n# Read and work with datasets\nimport xarray as xr\nimport numpy as np\nimport h5py\n\n# For plotting\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nfrom shapely.geometry import box\n\n# For resampling\nimport pyresample\n\n\n\n\nWe are going to focus on getting data for an area north of Greenland for a single day in June.\nThese bounding_box and temporal variables will be used for data search, subset, and access below.\nThe same search and access steps for both datasets can be performed via Earthdata Search using the same spatial and temporal filtering options. See the Earthdata Search tutorial for more information on how to use Earthdata Search to discover and access data from the Earthdata Cloud.\n\n# Bounding Box spatial parameter in decimal degree 'W,S,E,N' format.\nbounding_box = '-62.8,81.7,-56.4,83'\n\n# Each date in yyyy-MM-ddTHH:mm:ssZ format; date range in start,end format\ntemporal = '2019-06-22T00:00:00Z,2019-06-22T23:59:59Z'\n\nSee the Data Discovery with CMR tutorial for more details on how to navigate the NASA Common Metadata Repository (CMR) Application Programming Interface, or API. For some background, the CMR catalogs all data for NASA’s Earth Observing System Data and Information System (EOSDIS). The CMR API allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results.\nFor this tutorial, we have already identified the unique identifier, or concept_id for each dataset:\n\nmodis_concept_id = 'C1940473819-POCLOUD'\nicesat2_concept_id = 'C2003771980-NSIDC_ECS'\n\nThis Earthdata Search Project also provides the same data access links that we will identify in the following steps for each dataset (note that you will need an Earthdata Login account to access this project).\n\n\n\nPerform a granule search over our time and area of interest. How many granules are returned?\n\ngranule_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n\n\nresponse = requests.get(granule_url,\n                       params={\n                           'concept_id': icesat2_concept_id,\n                           'temporal': temporal,\n                           'bounding_box': bounding_box,\n                           'page_size': 200,\n                       },\n                       headers={\n                           'Accept': 'application/json'\n                       }\n                      )\nprint(response.headers['CMR-Hits'])\n\n2\n\n\nPrint the file names, size, and links:\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"producer_granule_id\"]} {granule[\"granule_size\"]} {granule[\"links\"][0][\"href\"]}')\n\nATL07-01_20190622055317_12980301_004_01.h5 237.0905504227 https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL07.004/2019.06.22/ATL07-01_20190622055317_12980301_004_01.h5\nATL07-01_20190622200154_13070301_004_01.h5 230.9151573181 https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL07.004/2019.06.22/ATL07-01_20190622200154_13070301_004_01.h5\n\n\n\n\nAlthough several services are supported for ICESat-2 data, we are demonstrating direct access through the “on-prem” file system at NSIDC for simplicity.\nSome of these services include: - icepyx - From the icepyx documentation: “icepyx is both a software library and a community composed of ICESat-2 data users, developers, and the scientific community. We are working together to develop a shared library of resources - including existing resources, new code, tutorials, and use-cases/examples - that simplify the process of querying, obtaining, analyzing, and manipulating ICESat-2 datasets to enable scientific discovery.” - NSIDC DAAC Data Access and Service API - The API provided by the NSIDC DAAC allows you to access data programmatically using specific temporal and spatial filters. The same subsetting, reformatting, and reprojection services available on select data sets through NASA Earthdata Search can also be applied using this API. - IceFlow - The IceFlow python library simplifies accessing and combining data from several of NASA’s cryospheric altimetry missions, including ICESat/GLAS, Operation IceBridge, and ICESat-2. In particular, IceFlow harmonizes the various file formats and georeferencing parameters across several of the missions’ data sets, allowing you to analyze data across the multi-decadal time series.\nWe’ve found 2 granules. We’ll download the first one and write it to a file with the same name as the producer_granule_id.\nWe need the url for the granule as well. This is href links we printed out above.\n\nicesat_id = granules[0]['producer_granule_id']\nicesat_url = granules[0]['links'][0]['href']\n\nTo retrieve the granule data, we use the requests.get() method, which will utilize the .netrc file on the backend to authenticate the request against Earthdata Login.\n\nr = requests.get(icesat_url)\n\nThe response returned by requests has the same structure as all the other responses: a header and contents. The header information has information about the response, including the size of the data we downloaded in bytes.\n\nfor k, v in r.headers.items():\n    print(f'{k}: {v}')\n\nDate: Sun, 12 Dec 2021 01:52:31 GMT\nServer: Apache\nVary: User-Agent\nContent-Disposition: attachment\nContent-Length: 248607461\nKeep-Alive: timeout=15, max=100\nConnection: Keep-Alive\n\n\nThe contents needs to be saved to a file. To keep the directory clean, we will create a downloads directory to store the file. We can use a shell command to do this or use the makedirs method from the os package.\n\nos.makedirs(\"downloads\", exist_ok=True)\n\nYou should see a downloads directory in the file browser.\nTo write the data to a file, we use open to open a file. We need to specify that the file is open for writing by using the write-mode w. We also need to specify that we want to write bytes by setting the binary-mode b. This is important because the response contents are bytes. The default mode for open is text-mode. So make sure you use b.\nWe’ll use the with statement context-manager to open the file, write the contents of the response, and then close the file. Once the data in r.content is written sucessfully to the file, or if there is an error, the file is closed by the context-manager.\nWe also need to prepend the downloads path to the filename. We do this using Path from the pathlib package in the standard library.\n\noutfile = Path('downloads', icesat_id)\n\n\nif not outfile.exists():\n    with open(outfile, 'wb') as f:\n        f.write(r.content)\n\nATL07-01_20190622055317_12980301_004_01.h5 is an HDF5 file. xarray can open this but you need to tell it which group to read the data from. In this case we read the sea ice segment height data for ground-track 1 left-beam. You can explore the variable hierarchy in Earthdata Search, by selecting the Customize option under Download Data.\nThis code block performs the following operations: - Extracts the height_segment_height variable from the heights group, along with the dimension variables contained in the higher level sea_ice_segments group, - Convert attributes from bytestrings to strings, - Drops the HDF attribute DIMENSION_LIST, - Sets _FillValue to NaN\n\nvariable_names = [\n    '/gt1l/sea_ice_segments/latitude',\n    '/gt1l/sea_ice_segments/longitude',\n    '/gt1l/sea_ice_segments/delta_time',\n    '/gt1l/sea_ice_segments/heights/height_segment_height'\n    ]\nwith h5py.File(outfile, 'r') as h5:\n    data_vars = {}\n    for varname in variable_names:\n        var = h5[varname]\n        name = varname.split('/')[-1]\n        # Convert attributes\n        attrs = {}\n        for k, v in var.attrs.items():\n            if k != 'DIMENSION_LIST':\n                if isinstance(v, bytes):\n                    attrs[k] = v.decode('utf-8')\n                else:\n                    attrs[k] = v\n        data = var[:]\n        if '_FillValue' in attrs:\n            data = np.where(data < attrs['_FillValue'], data, np.nan)\n        data_vars[name] = (['segment'], data, attrs)\n    is2_ds = xr.Dataset(data_vars)\n    \nis2_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                (segment: 235584)\nDimensions without coordinates: segment\nData variables:\n    latitude               (segment) float64 82.38 82.38 82.38 ... 72.61 72.61\n    longitude              (segment) float64 -55.11 -55.11 ... 145.1 145.1\n    delta_time             (segment) float64 4.642e+07 4.642e+07 ... 4.642e+07\n    height_segment_height  (segment) float32 nan nan nan ... -0.4335 -0.4463xarray.DatasetDimensions:segment: 235584Coordinates: (0)Data variables: (4)latitude(segment)float6482.38 82.38 82.38 ... 72.61 72.61contentType :referenceInformationcoordinates :delta_time longitudedescription :Latitude, WGS84, North=+, Lat of segment centerlong_name :Latitudesource :ATBD, section 4.4standard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0array([82.38431982, 82.38431982, 82.38431982, ..., 72.60984638,\n       72.60977493, 72.60970985])longitude(segment)float64-55.11 -55.11 ... 145.1 145.1contentType :referenceInformationcoordinates :delta_time latitudedescription :Longitude, WGS84, East=+,Lon of segment centerlong_name :Longitudesource :ATBD, section 4.4standard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0array([-55.10896068, -55.10896068, -55.10896068, ..., 145.05396164,\n       145.05392851, 145.05389832])delta_time(segment)float644.642e+07 4.642e+07 ... 4.642e+07CLASS :DIMENSION_SCALENAME :gt1l/sea_ice_segments/delta_timeREFERENCE_LIST :[(<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)]contentType :physicalMeasurementcoordinates :latitude longitudedescription :Number of GPS seconds since the ATLAS SDP epoch. The ATLAS Standard Data Products (SDP) epoch offset is defined within /ancillary_data/atlas_sdp_gps_epoch as the number of GPS seconds between the GPS epoch (1980-01-06T00:00:00.000000Z UTC) and the ATLAS SDP epoch. By adding the offset contained within atlas_sdp_gps_epoch to delta time parameters, the time in gps_seconds relative to the GPS epoch can be computed.long_name :Elapsed GPS secondssource :telemetrystandard_name :timeunits :seconds since 2018-01-01array([46419293.64266939, 46419293.64266939, 46419293.64266939, ...,\n       46419681.87646231, 46419681.87759533, 46419681.87862704])height_segment_height(segment)float32nan nan nan ... -0.4335 -0.4463_FillValue :3.4028235e+38contentType :referenceInformationcoordinates :../delta_time ../latitude ../longitudedescription :Mean height from along-track segment fit detremined by the sea ice algorithm. The sea ice height is relative to the tide-free MSS.long_name :height of segment surfacesource :ATBD, section 4.2.2.4units :metersarray([        nan,         nan,         nan, ..., -0.46550068,\n       -0.43347716, -0.4462675 ], dtype=float32)Attributes: (0)\n\n\n\nis2_ds.height_segment_height.plot() ;\n\n\n\n\n\n\n\n\n\nresponse = requests.get(granule_url, \n                        params={\n                            'concept_id': modis_concept_id,\n                            'temporal': temporal,\n                            'bounding_box': bounding_box,\n                            'page_size': 200,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.headers['CMR-Hits'])\n\n14\n\n\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"title\"]} {granule[\"granule_size\"]} {granule[\"links\"][0][\"href\"]}')\n\n20190622000501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.71552562713623 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622000501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622014501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622014501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622032501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 21.307741165161133 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622032501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622050001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622050001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622050501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.065649032592773 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622050501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622064001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0 18.602201461791992 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-N-v02.0-fv01.0.nc\n20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 18.665077209472656 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622064501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622082001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.782299995422363 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622082001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.13440227508545 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622113501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 20.3239164352417 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622113501-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622114001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 9.34600830078125E-5 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622114001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622163001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.257243156433105 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622163001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n20190622181001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0 19.93498420715332 s3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622181001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc\n\n\n\n\n\nOur CMR granule search returned 14 files for our time and area of interest. However, not all granules will be suitable for analysis.\nI’ve identified the image with granule id G1956158784-POCLOUD as a good candidate, this is the 9th granule. In this image, our area of interest is close to nadir. This means that the instantaneous field of view over the area of interest cover a smaller area than at the edge of the image.\nWe are looking for the link for direct download access via s3. This is a url but with a prefix s3://. This happens to be the first href link in the metadata.\nFor a single granule we can cut and paste the s3 link. If we have several granules, the s3 links can be extracted with some simple code.\n\ngranule = granules[9]\n\nfor link in granule['links']:\n    if link['href'].startswith('s3://'):\n        s3_link = link['href']\n        \ns3_link\n\n's3://podaac-ops-cumulus-protected/MODIS_A-JPL-L2P-v2019.0/20190622100001-JPL-L2P_GHRSST-SSTskin-MODIS_A-D-v02.0-fv01.0.nc'\n\n\n\n\nAs with the previous S3 download tutorials we need credentials to access data from s3: access keys and tokens.\n\ns3_credentials = requests.get('https://archive.podaac.earthdata.nasa.gov/s3credentials').json()\n\nEssentially, what we are doing in this step is to “mount” the s3 bucket as a file system. This allows us to treat the S3 bucket in a similar way to a local file system.\n\ns3_fs = s3fs.S3FileSystem(\n    key=s3_credentials[\"accessKeyId\"],\n    secret=s3_credentials[\"secretAccessKey\"],\n    token=s3_credentials[\"sessionToken\"],\n)\n\n\n\n\nNow we have the S3FileSystem set up, we can access the granule. xarray cannot open a S3File directly, so we use the open method for the S3FileSystem to open the granule using the endpoint url we extracted from the metadata. We also have to set the mode='rb'. This opens the granule in read-only mode and in byte-mode. Byte-mode is important. By default, open opens a file as text - in this case it would just be a string of characters - and xarray doesn’t know what to do with that.\nWe then pass the S3File object f to xarray.open_dataset. For this dataset, we also have to set decode_cf=False. This switch tells xarray not to use information contained in variable attributes to generate human readable coordinate variables. Normally, this should work for netcdf files but for this particular cloud-hosted dataset, variable attribute data is not in the form expected by xarray. We’ll fix this.\n\nf = s3_fs.open(s3_link, mode='rb')\nmodis_ds = xr.open_dataset(f, decode_cf=False)\n\nIf you click on the Show/Hide Attributes icon (the first document-like icon to the right of coordinate variable metadata) you can see that attributes are one-element arrays containing bytestrings.\n\nmodis_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (nj: 2030, ni: 1354, time: 1)\nCoordinates:\n  * time                     (time) int32 1214042401\nDimensions without coordinates: nj, ni\nData variables:\n    lat                      (nj, ni) float32 ...\n    lon                      (nj, ni) float32 ...\n    sea_surface_temperature  (time, nj, ni) int16 ...\n    sst_dtime                (time, nj, ni) int16 ...\n    quality_level            (time, nj, ni) int8 ...\n    sses_bias                (time, nj, ni) int8 ...\n    sses_standard_deviation  (time, nj, ni) int8 ...\n    l2p_flags                (time, nj, ni) int16 ...\n    chlorophyll_a            (time, nj, ni) float32 ...\n    K_490                    (time, nj, ni) int16 ...\n    wind_speed               (time, nj, ni) int8 ...\n    dt_analysis              (time, nj, ni) int8 ...\nAttributes: (12/49)\n    Conventions:                [b'CF-1.7, ACDD-1.3']\n    title:                      [b'MODIS Aqua L2P SST']\n    summary:                    [b'Sea surface temperature retrievals produce...\n    references:                 [b'GHRSST Data Processing Specification v2r5']\n    institution:                [b'NASA/JPL/OBPG/RSMAS']\n    history:                    [b'MODIS L2P created at JPL PO.DAAC']\n    ...                         ...\n    publisher_email:            [b'ghrsst-po@nceo.ac.uk']\n    processing_level:           [b'L2P']\n    cdm_data_type:              [b'swath']\n    startDirection:             [b'Ascending']\n    endDirection:               [b'Descending']\n    day_night_flag:             [b'Day']xarray.DatasetDimensions:nj: 2030ni: 1354time: 1Coordinates: (1)time(time)int321214042401long_name :[b'reference time of sst file']standard_name :[b'time']units :[b'seconds since 1981-01-01 00:00:00']comment :[b'time of first sensor observation']coverage_content_type :[b'coordinate']array([1214042401], dtype=int32)Data variables: (12)lat(nj, ni)float32...long_name :[b'latitude']standard_name :[b'latitude']units :[b'degrees_north']_FillValue :[-999.]valid_min :[-90.]valid_max :[90.]comment :[b'geographical coordinates, WGS84 projection']coverage_content_type :[b'coordinate'][2748620 values with dtype=float32]lon(nj, ni)float32...long_name :[b'longitude']standard_name :[b'longitude']units :[b'degrees_east']_FillValue :[-999.]valid_min :[-180.]valid_max :[180.]comment :[b'geographical coordinates, WGS84 projection']coverage_content_type :[b'coordinate'][2748620 values with dtype=float32]sea_surface_temperature(time, nj, ni)int16...long_name :[b'sea surface temperature']standard_name :[b'sea_surface_skin_temperature']units :[b'kelvin']_FillValue :[-32767]valid_min :[-1000]valid_max :[10000]comment :[b'sea surface temperature from thermal IR (11 um) channels']scale_factor :[0.005]add_offset :[273.15]source :[b'NASA and University of Miami']coordinates :[b'lon lat']coverage_content_type :[b'physicalMeasurement'][2748620 values with dtype=int16]sst_dtime(time, nj, ni)int16...long_name :[b'time difference from reference time']units :[b'seconds']_FillValue :[-32768]valid_min :[-32767]valid_max :[32767]comment :[b'time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981']coordinates :[b'lon lat']coverage_content_type :[b'referenceInformation'][2748620 values with dtype=int16]quality_level(time, nj, ni)int8...long_name :[b'quality level of SST pixel']_FillValue :[-128]valid_min :[0]valid_max :[5]comment :[b'thermal IR SST proximity confidence value; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value']coordinates :[b'lon lat']flag_values :[0 1 2 3 4 5]flag_meanings :[b'no_data bad_data worst_quality low_quality acceptable_quality best_quality']coverage_content_type :[b'qualityInformation'][2748620 values with dtype=int8]sses_bias(time, nj, ni)int8...long_name :[b'SSES bias error based on proximity confidence flags']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'thermal IR SST bias error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value']scale_factor :[0.15748031]add_offset :[0.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]sses_standard_deviation(time, nj, ni)int8...long_name :[b'SSES standard deviation error based on proximity confidence flags']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'thermal IR SST standard deviation error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value']scale_factor :[0.07874016]add_offset :[10.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]l2p_flags(time, nj, ni)int16...long_name :[b'L2P flags']valid_min :[0]valid_max :[16]comment :[b'These flags can be used to further filter data variables']coordinates :[b'lon lat']flag_meanings :[b'microwave land ice lake river']flag_masks :[ 1  2  4  8 16]coverage_content_type :[b'qualityInformation'][2748620 values with dtype=int16]chlorophyll_a(time, nj, ni)float32...long_name :[b'Chlorophyll Concentration, OC3 Algorithm']units :[b'mg m^-3']_FillValue :[-32767.]valid_min :[0.001]valid_max :[100.]comment :[b'non L2P core field']coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=float32]K_490(time, nj, ni)int16...long_name :[b'Diffuse attenuation coefficient at 490 nm (OBPG)']units :[b'm^-1']_FillValue :[-32767]valid_min :[50]valid_max :[30000]comment :[b'non L2P core field']scale_factor :[0.0002]add_offset :[0.]coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int16]wind_speed(time, nj, ni)int8...long_name :[b'10m wind speed']standard_name :[b'wind_speed']units :[b'm s-1']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'Wind at 10 meters above the sea surface']scale_factor :[0.2]add_offset :[25.]source :[b'TBD.  Placeholder.  Currently empty']coordinates :[b'lon lat']grid_mapping :[b'TBD']time_offset :[2.]height :[b'10 m']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]dt_analysis(time, nj, ni)int8...long_name :[b'deviation from SST reference climatology']units :[b'kelvin']_FillValue :[-128]valid_min :[-127]valid_max :[127]comment :[b'TBD']scale_factor :[0.1]add_offset :[0.]source :[b'TBD. Placeholder.  Currently empty']coordinates :[b'lon lat']coverage_content_type :[b'auxiliaryInformation'][2748620 values with dtype=int8]Attributes: (49)Conventions :[b'CF-1.7, ACDD-1.3']title :[b'MODIS Aqua L2P SST']summary :[b'Sea surface temperature retrievals produced at the NASA OBPG for the MODIS Aqua sensor.  These have been reformatted to GHRSST GDS specifications by the JPL PO.DAAC']references :[b'GHRSST Data Processing Specification v2r5']institution :[b'NASA/JPL/OBPG/RSMAS']history :[b'MODIS L2P created at JPL PO.DAAC']comment :[b'L2P Core without DT analysis or other ancillary fields; Day, Start Node:Ascending, End Node:Descending; WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value; Refined']license :[b'GHRSST and PO.DAAC protocol allow data use as free and open.']id :[b'MODIS_A-JPL-L2P-v2019.0']naming_authority :[b'org.ghrsst']product_version :[b'2019.0']uuid :[b'f6e1f61d-c4a4-4c17-8354-0c15e12d688b']gds_version_id :[b'2.0']netcdf_version_id :[b'4.1']date_created :[b'20200221T085224Z']file_quality_level :[3]spatial_resolution :[b'1km']start_time :[b'20190622T100001Z']time_coverage_start :[b'20190622T100001Z']stop_time :[b'20190622T100459Z']time_coverage_end :[b'20190622T100459Z']northernmost_latitude :[89.9862]southernmost_latitude :[66.2723]easternmost_longitude :[-45.9467]westernmost_longitude :[152.489]source :[b'MODIS sea surface temperature observations for the OBPG']platform :[b'Aqua']sensor :[b'MODIS']metadata_link :[b'http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MODIS_A-JPL-L2P-v2019.0']keywords :[b'Oceans > Ocean Temperature > Sea Surface Temperature']keywords_vocabulary :[b'NASA Global Change Master Directory (GCMD) Science Keywords']standard_name_vocabulary :[b'NetCDF Climate and Forecast (CF) Metadata Convention']geospatial_lat_units :[b'degrees_north']geospatial_lat_resolution :[0.01]geospatial_lon_units :[b'degrees_east']geospatial_lon_resolution :[0.01]acknowledgment :[b'The MODIS L2P sea surface temperature data are sponsored by NASA']creator_name :[b'Ed Armstrong, JPL PO.DAAC']creator_email :[b'edward.m.armstrong@jpl.nasa.gov']creator_url :[b'http://podaac.jpl.nasa.gov']project :[b'Group for High Resolution Sea Surface Temperature']publisher_name :[b'The GHRSST Project Office']publisher_url :[b'http://www.ghrsst.org']publisher_email :[b'ghrsst-po@nceo.ac.uk']processing_level :[b'L2P']cdm_data_type :[b'swath']startDirection :[b'Ascending']endDirection :[b'Descending']day_night_flag :[b'Day']\n\n\nTo fix this, we need to extract array elements as scalars, and convert those scalars from bytestrings to strings. We use the decode method to do this. The bytestrings are encoded as utf-8, which is a unicode character format. This is the default encoding for decode but we’ve included it as an argument to be explicit.\nNot all attributes are bytestrings. Some are floats. Take a look at _FillValue, and valid_min and valid_max. To avoid an error, we use the isinstance function to check if the value of an attributes is type bytes - a bytestring. If it is, then we decode it. If not, we just extract the scalar and do nothing else.\nWe also fix the global attributes.\n\ndef fix_attributes(da):\n    '''Decodes bytestring attributes to strings'''\n    for attr, value in da.attrs.items():\n        if isinstance(value[0], bytes):\n            da.attrs[attr] = value[0].decode('utf-8')\n        else:\n            da.attrs[attr] = value[0]\n    return\n\n# Fix variable attributes\nfor var in modis_ds.variables:\n    da = modis_ds[var]\n    fix_attributes(da)\n            \n# Fix global attributes\nfix_attributes(modis_ds)\n\nWith this done, we can use the xarray function decode_cf to convert the attributes.\n\nmodis_ds = xr.decode_cf(modis_ds)\n\n\nmodis_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (nj: 2030, ni: 1354, time: 1)\nCoordinates:\n    lat                      (nj, ni) float32 ...\n    lon                      (nj, ni) float32 ...\n  * time                     (time) datetime64[ns] 2019-06-22T10:00:01\nDimensions without coordinates: nj, ni\nData variables:\n    sea_surface_temperature  (time, nj, ni) float32 ...\n    sst_dtime                (time, nj, ni) timedelta64[ns] ...\n    quality_level            (time, nj, ni) float32 ...\n    sses_bias                (time, nj, ni) float32 ...\n    sses_standard_deviation  (time, nj, ni) float32 ...\n    l2p_flags                (time, nj, ni) int16 ...\n    chlorophyll_a            (time, nj, ni) float32 ...\n    K_490                    (time, nj, ni) float32 ...\n    wind_speed               (time, nj, ni) float32 ...\n    dt_analysis              (time, nj, ni) float32 ...\nAttributes: (12/49)\n    Conventions:                CF-1.7, ACDD-1.3\n    title:                      MODIS Aqua L2P SST\n    summary:                    Sea surface temperature retrievals produced a...\n    references:                 GHRSST Data Processing Specification v2r5\n    institution:                NASA/JPL/OBPG/RSMAS\n    history:                    MODIS L2P created at JPL PO.DAAC\n    ...                         ...\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L2P\n    cdm_data_type:              swath\n    startDirection:             Ascending\n    endDirection:               Descending\n    day_night_flag:             Dayxarray.DatasetDimensions:nj: 2030ni: 1354time: 1Coordinates: (3)lat(nj, ni)float32...long_name :latitudestandard_name :latitudeunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[2748620 values with dtype=float32]lon(nj, ni)float32...long_name :longitudestandard_name :longitudeunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geographical coordinates, WGS84 projectioncoverage_content_type :coordinate[2748620 values with dtype=float32]time(time)datetime64[ns]2019-06-22T10:00:01long_name :reference time of sst filestandard_name :timecomment :time of first sensor observationcoverage_content_type :coordinatearray(['2019-06-22T10:00:01.000000000'], dtype='datetime64[ns]')Data variables: (10)sea_surface_temperature(time, nj, ni)float32...long_name :sea surface temperaturestandard_name :sea_surface_skin_temperatureunits :kelvinvalid_min :-1000valid_max :10000comment :sea surface temperature from thermal IR (11 um) channelssource :NASA and University of Miamicoverage_content_type :physicalMeasurement[2748620 values with dtype=float32]sst_dtime(time, nj, ni)timedelta64[ns]...long_name :time difference from reference timevalid_min :-32767valid_max :32767comment :time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981coverage_content_type :referenceInformation[2748620 values with dtype=timedelta64[ns]]quality_level(time, nj, ni)float32...long_name :quality level of SST pixelvalid_min :0valid_max :5comment :thermal IR SST proximity confidence value; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valueflag_values :0flag_meanings :no_data bad_data worst_quality low_quality acceptable_quality best_qualitycoverage_content_type :qualityInformation[2748620 values with dtype=float32]sses_bias(time, nj, ni)float32...long_name :SSES bias error based on proximity confidence flagsunits :kelvinvalid_min :-127valid_max :127comment :thermal IR SST bias error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]sses_standard_deviation(time, nj, ni)float32...long_name :SSES standard deviation error based on proximity confidence flagsunits :kelvinvalid_min :-127valid_max :127comment :thermal IR SST standard deviation error; signed byte array: WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported valuecoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]l2p_flags(time, nj, ni)int16...long_name :L2P flagsvalid_min :0valid_max :16comment :These flags can be used to further filter data variablesflag_meanings :microwave land ice lake riverflag_masks :1coverage_content_type :qualityInformation[2748620 values with dtype=int16]chlorophyll_a(time, nj, ni)float32...long_name :Chlorophyll Concentration, OC3 Algorithmunits :mg m^-3valid_min :0.001valid_max :100.0comment :non L2P core fieldcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]K_490(time, nj, ni)float32...long_name :Diffuse attenuation coefficient at 490 nm (OBPG)units :m^-1valid_min :50valid_max :30000comment :non L2P core fieldcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]wind_speed(time, nj, ni)float32...long_name :10m wind speedstandard_name :wind_speedunits :m s-1valid_min :-127valid_max :127comment :Wind at 10 meters above the sea surfacesource :TBD.  Placeholder.  Currently emptygrid_mapping :TBDtime_offset :2.0height :10 mcoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]dt_analysis(time, nj, ni)float32...long_name :deviation from SST reference climatologyunits :kelvinvalid_min :-127valid_max :127comment :TBDsource :TBD. Placeholder.  Currently emptycoverage_content_type :auxiliaryInformation[2748620 values with dtype=float32]Attributes: (49)Conventions :CF-1.7, ACDD-1.3title :MODIS Aqua L2P SSTsummary :Sea surface temperature retrievals produced at the NASA OBPG for the MODIS Aqua sensor.  These have been reformatted to GHRSST GDS specifications by the JPL PO.DAACreferences :GHRSST Data Processing Specification v2r5institution :NASA/JPL/OBPG/RSMAShistory :MODIS L2P created at JPL PO.DAACcomment :L2P Core without DT analysis or other ancillary fields; Day, Start Node:Ascending, End Node:Descending; WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value; Refinedlicense :GHRSST and PO.DAAC protocol allow data use as free and open.id :MODIS_A-JPL-L2P-v2019.0naming_authority :org.ghrsstproduct_version :2019.0uuid :f6e1f61d-c4a4-4c17-8354-0c15e12d688bgds_version_id :2.0netcdf_version_id :4.1date_created :20200221T085224Zfile_quality_level :3spatial_resolution :1kmstart_time :20190622T100001Ztime_coverage_start :20190622T100001Zstop_time :20190622T100459Ztime_coverage_end :20190622T100459Znorthernmost_latitude :89.9862southernmost_latitude :66.2723easternmost_longitude :-45.9467westernmost_longitude :152.489source :MODIS sea surface temperature observations for the OBPGplatform :Aquasensor :MODISmetadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MODIS_A-JPL-L2P-v2019.0keywords :Oceans > Ocean Temperature > Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventiongeospatial_lat_units :degrees_northgeospatial_lat_resolution :0.01geospatial_lon_units :degrees_eastgeospatial_lon_resolution :0.01acknowledgment :The MODIS L2P sea surface temperature data are sponsored by NASAcreator_name :Ed Armstrong, JPL PO.DAACcreator_email :edward.m.armstrong@jpl.nasa.govcreator_url :http://podaac.jpl.nasa.govproject :Group for High Resolution Sea Surface Temperaturepublisher_name :The GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L2Pcdm_data_type :swathstartDirection :AscendingendDirection :Descendingday_night_flag :Day\n\n\nLet’s make a quick plot to take a look at the sea_surface_temperature variable.\n\nmodis_ds.sea_surface_temperature.plot() ;\n\n\n\n\n\n\n\n\nmap_proj = ccrs.NorthPolarStereo()\n\nfig = plt.figure(figsize=(10,5))\nax = fig.add_subplot(projection=map_proj)\nax.coastlines()\n\n# Plot MODIS sst, save object as sst_img, so we can add colorbar\nsst_img = ax.pcolormesh(modis_ds.lon, modis_ds.lat, modis_ds.sea_surface_temperature[0,:,:], \n                        vmin=240, vmax=270,  # Set max and min values for plotting\n                        cmap='viridis', shading='auto',   # shading='auto' to avoid warning\n                        transform=ccrs.PlateCarree())  # coords are lat,lon but map if NPS \n\n# Plot IS2 surface height \nis2_img = ax.scatter(is2_ds.longitude, is2_ds.latitude,\n                     c=is2_ds.height_segment_height, \n                     vmax=1.5,  # Set max height to plot\n                     cmap='Reds', alpha=0.6, s=2,\n                     transform=ccrs.PlateCarree())\n\n# Add colorbars\nfig.colorbar(sst_img, label='MODIS SST (K)')\nfig.colorbar(is2_img, label='ATL07 Height (m)')\n\n\n<matplotlib.colorbar.Colorbar at 0x7fb3944adb50>\n\n\n\n\n\n\n\n\nThe MODIS SST is swath data, not a regularly-spaced grid of sea surface temperatures. ICESat-2 sea surface heights are irregularly spaced segments along one ground-track traced by the ATLAS instrument on-board ICESat-2. Fortunately, pyresample allows us to resample swath data.\npyresample has many resampling methods. We’re going to use the nearest neighbour resampling method, which is implemented using a k-dimensional tree algorithm or K-d tree. K-d trees are data structures that improve search efficiency for large data sets.\nThe first step is to define the geometry of the ICESat-2 and MODIS data. To do this we use the latitudes and longitudes of the datasets.\n\nis2_geometry = pyresample.SwathDefinition(lons=is2_ds.longitude,\n                                          lats=is2_ds.latitude)\n\n\nmodis_geometry = pyresample.SwathDefinition(lons=modis_ds.lon, lats=modis_ds.lat)\n\nWe then implement the resampling method, passing the two geometries we have defined, the data array we want to resample - in this case sea surface temperature, and a search radius. The resampling method expects a numpy.Array rather than an xarray.DataArray, so we use values to get the data as a numpy.Array.\nWe set the search radius to 1000 m. The MODIS data is nominally 1km spacing.\n\nsearch_radius=1000.\nfill_value = np.nan\nis2_sst = pyresample.kd_tree.resample_nearest(\n    modis_geometry,\n    modis_ds.sea_surface_temperature.values,\n    is2_geometry,\n    search_radius,\n    fill_value=fill_value\n)\n\n\nis2_sst\n\narray([263.375, 263.375, 263.375, ...,     nan,     nan,     nan],\n      dtype=float32)\n\n\n\nis2_ds['sea_surface_temperature'] = xr.DataArray(is2_sst, dims='segment')\nis2_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                  (segment: 235584)\nDimensions without coordinates: segment\nData variables:\n    latitude                 (segment) float64 82.38 82.38 82.38 ... 72.61 72.61\n    longitude                (segment) float64 -55.11 -55.11 ... 145.1 145.1\n    delta_time               (segment) float64 4.642e+07 4.642e+07 ... 4.642e+07\n    height_segment_height    (segment) float32 nan nan nan ... -0.4335 -0.4463\n    sea_surface_temperature  (segment) float32 263.4 263.4 263.4 ... nan nan nanxarray.DatasetDimensions:segment: 235584Coordinates: (0)Data variables: (5)latitude(segment)float6482.38 82.38 82.38 ... 72.61 72.61contentType :referenceInformationcoordinates :delta_time longitudedescription :Latitude, WGS84, North=+, Lat of segment centerlong_name :Latitudesource :ATBD, section 4.4standard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0array([82.38431982, 82.38431982, 82.38431982, ..., 72.60984638,\n       72.60977493, 72.60970985])longitude(segment)float64-55.11 -55.11 ... 145.1 145.1contentType :referenceInformationcoordinates :delta_time latitudedescription :Longitude, WGS84, East=+,Lon of segment centerlong_name :Longitudesource :ATBD, section 4.4standard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0array([-55.10896068, -55.10896068, -55.10896068, ..., 145.05396164,\n       145.05392851, 145.05389832])delta_time(segment)float644.642e+07 4.642e+07 ... 4.642e+07CLASS :DIMENSION_SCALENAME :gt1l/sea_ice_segments/delta_timeREFERENCE_LIST :[(<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)\n (<HDF5 object reference>, 0) (<HDF5 object reference>, 0)]contentType :physicalMeasurementcoordinates :latitude longitudedescription :Number of GPS seconds since the ATLAS SDP epoch. The ATLAS Standard Data Products (SDP) epoch offset is defined within /ancillary_data/atlas_sdp_gps_epoch as the number of GPS seconds between the GPS epoch (1980-01-06T00:00:00.000000Z UTC) and the ATLAS SDP epoch. By adding the offset contained within atlas_sdp_gps_epoch to delta time parameters, the time in gps_seconds relative to the GPS epoch can be computed.long_name :Elapsed GPS secondssource :telemetrystandard_name :timeunits :seconds since 2018-01-01array([46419293.64266939, 46419293.64266939, 46419293.64266939, ...,\n       46419681.87646231, 46419681.87759533, 46419681.87862704])height_segment_height(segment)float32nan nan nan ... -0.4335 -0.4463_FillValue :3.4028235e+38contentType :referenceInformationcoordinates :../delta_time ../latitude ../longitudedescription :Mean height from along-track segment fit detremined by the sea ice algorithm. The sea ice height is relative to the tide-free MSS.long_name :height of segment surfacesource :ATBD, section 4.2.2.4units :metersarray([        nan,         nan,         nan, ..., -0.46550068,\n       -0.43347716, -0.4462675 ], dtype=float32)sea_surface_temperature(segment)float32263.4 263.4 263.4 ... nan nan nanarray([263.375, 263.375, 263.375, ...,     nan,     nan,     nan],\n      dtype=float32)Attributes: (0)\n\n\n\n\n\nThis is a quick plot of the extracted data. We’re using matplotlib so we can use latitude as the x-value:\n\nis2_ds = is2_ds.set_coords(['latitude'])\n\nfig, ax1 = plt.subplots(figsize=(15, 7))\nax1.set_xlim(82.,88.)\nax1.plot(is2_ds.latitude, is2_ds.sea_surface_temperature, \n         color='orange', label='SST', zorder=3)\nax1.set_ylabel('SST (K)')\n\nax2 = ax1.twinx()\nax2.plot(is2_ds.latitude, is2_ds.height_segment_height, label='Height')\nax2.set_ylabel('Height (m)')\n\nfig.legend()\n\n<matplotlib.legend.Legend at 0x7fb39fcd8040>"
  },
  {
    "objectID": "tutorials/additional_resources/Data_Discovery_CMR_API.html",
    "href": "tutorials/additional_resources/Data_Discovery_CMR_API.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "In this notebook, we will walk through how to search for Earthdata data collections and granules. Along the way we will explore the available search parameters, information return, and specific contrains when using the CMR API. Our object is to identify assets to access that we would downloaded, or perform S3 direct access, within an analysis workflow\nWe will be querying CMR for Harmonized Landsat Sentinel-2 (HLS) collections/granules to identify assets we would downloaded, or perform S3 direct access, within an analysis workflow\n\n\n\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\n\n\nunderstand what CMR/CMR API is and what CMR/CMR API can be used for\nhow to use the requests package to search data collections and granules\n\nhow to parse the results of these searches.\n\n\n\n\nCMR is the Common Metadata Repository. It catalogs all data for NASA’s Earth Observing System Data and Information System (EOSDIS). It is the backend of Earthdata Search, the GUI search interface you are probably familiar with. More information about CMR can be found here.\nUnfortunately, the GUI for Earthdata Search is not accessible from a cloud instance - at least not without some work. Earthdata Search is also not immediately reproducible. What I mean by that is if you create a search using the GUI you would have to note the search criteria (date range, search area, collection name, etc), take a screenshot, copy the search url, or save the list of data granules returned by the search, in order to recreate the search. This information would have to be re-entered each time you or someone else wanted to do the search. You could make typos or other mistakes. A cleaner, reproducible solution is to search CMR programmatically using the CMR API.\n\n\n\nAPI stands for Application Programming Interface. It allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results.\n\n\n\n\nThe first step is to import python packages. We will use:\n- requests This package does most of the work for us accessing the CMR API using HTTP methods. - pprint to pretty print the results of the search.\nA more in-depth tutorial on requests is here\n\nimport requests\nfrom pprint import pprint\n\nTo conduct a search using the CMR API, requests needs the url for the root CMR search endpoint. We’ll assign this url to a python variable as a string.\n\nCMR_OPS = 'https://cmr.earthdata.nasa.gov/search'\n\nCMR allows search by collections, which are datasets, and granules, which are files that contain data. Many of the same search parameters can be used for collections and granules but the type of results returned differ. Search parameters can be found in the API Documentation.\nWhether we search collections or granules is distinguished by adding \"collections\" or \"granules\" to the end of the CMR endpoint URL.\nWe are going to search collections first, so we add \"collections\" to the URL. We are using a python format string in the examples below.\n\nurl = f'{CMR_OPS}/{\"collections\"}'\n\nIn this first example, I want to retrieve a list of collections available from the Earthdata Cloud using the cloud_hosted parameter in the request.\nWe want to get the content in json (pronounced “jason”) format, so I pass a dictionary to the header keyword argument to say that I want results returned as json.\nThe .get() method is used to send this information to the CMR API. get() calls the HTTP method GET.\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                        },\n                        headers={\n                            'Accept': 'application/json',\n                        }\n                       )\n\nThe request returns a Response object.\nTo check that our request was successful we can print the response variable we saved the request to.\n\nresponse\n\n<Response [200]>\n\n\nA 200 response is what we want. This means that the requests was successful. For more information on HTTP status codes see https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\nA more explict way to check the status code is to use the status_code attribute. Both methods return a HTTP status code.\n\nresponse.status_code\n\n200\n\n\nThe response from requests.get returns the results of the search and metadata about those results in the headers.\nMore information about the response object can be found by typing help(response).\nheaders contains useful information in a case-insensitive dictionary. We requested (above) that the information be return in json which means the object return is a dictionary in our Python environment. We’ll iterate through the returned dictionary, looping throught each field (k) and its associated value (v). For more on interating through dictionary object click here.\n\nfor k, v in response.headers.items():\n    print(f'{k}: {v}')\n\nContent-Type: application/json;charset=utf-8\nContent-Length: 4517\nConnection: keep-alive\nDate: Mon, 04 Apr 2022 21:04:20 GMT\nX-Frame-Options: SAMEORIGIN\nAccess-Control-Allow-Origin: *\nX-XSS-Protection: 1; mode=block\nCMR-Request-Id: 6b187d39-c5b8-40f7-9b06-9228ea3c33b2\nStrict-Transport-Security: max-age=31536000\nCMR-Search-After: [0.0,18400.0,\"SENTINEL-1A_OCN\",\"1\",1214472977,1250]\nCMR-Hits: 1169\nAccess-Control-Expose-Headers: CMR-Hits, CMR-Request-Id, X-Request-Id, CMR-Scroll-Id, CMR-Search-After, CMR-Timed-Out, CMR-Shapefile-Original-Point-Count, CMR-Shapefile-Simplified-Point-Count\nX-Content-Type-Options: nosniff\nCMR-Took: 303\nX-Request-Id: zbAtpLRlRe2kPJi8PNdomG7u6C22HPSRbAy5E2m23uYIKga7V8DEfg==\nVary: Accept-Encoding, User-Agent\nContent-Encoding: gzip\nServer: ServerTokens ProductOnly\nX-Cache: Miss from cloudfront\nVia: 1.1 e9c8cd6cad69627cb7c9d88123e6e2cc.cloudfront.net (CloudFront)\nX-Amz-Cf-Pop: HIO50-C2\nX-Amz-Cf-Id: zbAtpLRlRe2kPJi8PNdomG7u6C22HPSRbAy5E2m23uYIKga7V8DEfg==\n\n\nEach item in the dictionary can be accessed in the normal way you access a python dictionary but the keys uniquely case-insensitive. Let’s take a look at the commonly used CMR-Hits key.\n\nresponse.headers['CMR-Hits']\n\n'1169'\n\n\nNote that “cmr-hits” works as well!\n\nresponse.headers['cmr-hits']\n\n'1169'\n\n\nIn some situations the response to your query can return a very large number of result, some of which may not be relevant. We can add additional query parameters to restrict the information returned. We’re going to restrict the search by the provider parameter.\nYou can modify the code below to explore all Earthdata data products hosted by the various providers. When searching by provider, use Cloud Provider to search for cloud-hosted datasets and On-Premises Provider to search for datasets archived at the DAACs. A partial list of providers is given below.\n\n\n\n\n\n\n\n\n\nDAAC\nShort Name\nCloud Provider\nOn-Premises Provider\n\n\n\n\nNSIDC\nNational Snow and Ice Data Center\nNSIDC_CPRD\nNSIDC_ECS\n\n\nGHRC DAAC\nGlobal Hydrometeorology Resource Center\nGHRC_DAAC\nGHRC_DAAC\n\n\nPO DAAC\nPhysical Oceanography Distributed Active Archive Center\nPOCLOUD\nPODAAC\n\n\nASF\nAlaska Satellite Facility\nASF\nASF\n\n\nORNL DAAC\nOak Ridge National Laboratory\nORNL_CLOUD\nORNL_DAAC\n\n\nLP DAAC\nLand Processes Distributed Active Archive Center\nLPCLOUD\nLPDAAC_ECS\n\n\nGES DISC\nNASA Goddard Earth Sciences (GES) Data and Information Services Center (DISC)\nGES_DISC\nGES_DISC\n\n\nOB DAAC\nNASA’s Ocean Biology Distributed Active Archive Center\n\nOB_DAAC\n\n\nSEDAC\nNASA’s Socioeconomic Data and Applications Center\n\nSEDAC\n\n\n\nWe’ll assign the provider to a variable as a string and insert the variable into the parameter argument in the request.\n\nprovider = 'LPCLOUD'\n\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                            'provider': provider,\n                        },\n                        headers={\n                            'Accept': 'application/json'\n                        }\n                       )\nresponse\n\n<Response [200]>\n\n\n\nresponse.headers['cmr-hits']\n\n'3'\n\n\nSearch results are contained in the content part of the Response object. However, response.content returns information in bytes.\n\nresponse.content\n\nb'{\"feed\":{\"updated\":\"2022-04-04T21:04:21.524Z\",\"id\":\"https://cmr.earthdata.nasa.gov:443/search/collections.json?cloud_hosted=True&has_granules=True&provider=LPCLOUD\",\"title\":\"ECHO dataset metadata\",\"entry\":[{\"processing_level_id\":\"3\",\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2013-04-11T00:00:00.000Z\",\"version_id\":\"2.0\",\"updated\":\"2015-12-03T10:57:07.000Z\",\"dataset_id\":\"HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"HLSL30\",\"organizations\":[\"LP DAAC\",\"NASA/IMPACT\"],\"title\":\"HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The Harmonized Landsat and Sentinel-2 (HLS) project provides consistent surface reflectance (SR) and top of atmosphere (TOA) brightness data from the Operational Land Imager (OLI) aboard the joint NASA/USGS Landsat 8 satellite and the Multi-Spectral Instrument (MSI) aboard Europe\\xe2\\x80\\x99s Copernicus Sentinel-2A and Sentinel-2B satellites. The combined measurement enables global observations of the land every 2\\xe2\\x80\\x933 days at 30-meter (m) spatial resolution. The HLS project uses a set of algorithms to obtain seamless products from OLI and MSI that include atmospheric correction, cloud and cloud-shadow masking, spatial co-registration and common gridding, illumination and view angle normalization, and spectral bandpass adjustment.\\\\r\\\\n\\\\r\\\\nThe HLSL30 product provides 30-m Nadir Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Landsat 8 OLI data products. The HLSS30 and HLSL30 products are gridded to the same resolution and Military Grid Reference System ([MGRS](https://hls.gsfc.nasa.gov/products-description/tiling-system/)) tiling system, and thus are \\xe2\\x80\\x9cstackable\\xe2\\x80\\x9d for time series analysis.\\\\r\\\\n\\\\r\\\\nThe HLSL30 product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate file. There are 11 bands included in the HLSL30 product along with one quality assessment (QA) band and four angle bands. See the User Guide for a more detailed description of the individual bands provided in the HLSL30 product.\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2021957657-LPCLOUD\",\"has_formats\":false,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"OTHER\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":true,\"platforms\":[\"LANDSAT-8\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2021957657-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/HLS/HLSL30.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1118/HLS_User_Guide_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/769/HLS_ATBD_V15_provisional.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/768/HLS_Quick_Guide_v011.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-tutorial/browse\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-super-script/browse\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://cmr.earthdata.nasa.gov/browse-scaler/browse_images/granules/G2095313663-LPCLOUD?h=512&w=512\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-bulk-download/browse\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/resources/e-learning/getting-started-with-cloud-native-harmonized-landsat-sentinel-2-hls-data-in-r/\"}]},{\"processing_level_id\":\"3\",\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2015-11-28T00:00:00.000Z\",\"version_id\":\"2.0\",\"updated\":\"2020-03-04T07:19:53.396Z\",\"dataset_id\":\"HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"HLSS30\",\"organizations\":[\"LP DAAC\",\"NASA/IMPACT\"],\"title\":\"HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The Harmonized Landsat and Sentinel-2 (HLS) project provides consistent surface reflectance data from the Operational Land Imager (OLI) aboard the joint NASA/USGS Landsat 8 satellite and the Multi-Spectral Instrument (MSI) aboard Europe\\xe2\\x80\\x99s Copernicus Sentinel-2A and Sentinel-2B satellites. The combined measurement enables global observations of the land every 2\\xe2\\x80\\x933 days at 30-meter (m) spatial resolution. The HLS project uses a set of algorithms to obtain seamless products from OLI and MSI that include atmospheric correction, cloud and cloud-shadow masking, spatial co-registration and common gridding, illumination and view angle normalization, and spectral bandpass adjustment. \\\\r\\\\n\\\\r\\\\nThe HLSS30 product provides 30-m Nadir Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Sentinel-2A and Sentinel-2B MSI data products. The HLSS30 and HLSL30 products are gridded to the same resolution and Military Grid Reference System (MGRS) (https://hls.gsfc.nasa.gov/products-description/tiling-system/) tiling system, and thus are \\xe2\\x80\\x9cstackable\\xe2\\x80\\x9d for time series analysis.\\\\r\\\\n\\\\r\\\\nThe HLSS30 product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate COG. There are 13 bands included in the HLSS30 product along with four angle bands and a quality assessment (QA) band. See the User Guide for a more detailed description of the individual bands provided in the HLSS30 product.\\\\r\\\\n\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2021957295-LPCLOUD\",\"has_formats\":false,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"OTHER\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":true,\"platforms\":[\"Sentinel-2A\",\"Sentinel-2B\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2021957295-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/HLS/HLSS30.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1118/HLS_User_Guide_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/769/HLS_ATBD_V15_provisional.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/768/HLS_Quick_Guide_v011.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-tutorial/browse\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-super-script/browse\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://cmr.earthdata.nasa.gov/browse-scaler/browse_images/granules/G2095548655-LPCLOUD?h=512&w=512\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-bulk-download/browse\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/resources/e-learning/getting-started-with-cloud-native-harmonized-landsat-sentinel-2-hls-data-in-r/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://wiki.earthdata.nasa.gov/pages/viewpage.action?pageId=195432390\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://worldview.earthdata.nasa.gov/?v=-191.12397594071413,-86.52209371542455,182.39111268571813,89.5139950740695&t=2020-10-15-T16%3A46%3A06Z&l=Reference_Labels(hidden),Reference_Features(hidden),Coastlines,HLS_S30_Nadir_BRDF_Adjusted_Reflectance(hidden),VIIRS_NOAA20_CorrectedReflectance_TrueColor(hidden),VIIRS_SNPP_CorrectedReflectance_TrueColor(hidden),MODIS_Aqua_CorrectedReflectance_TrueColor(hidden),MODIS_Terra_CorrectedReflectance_TrueColor&tr=hls_intro \"}]},{\"processing_level_id\":\"3\",\"boxes\":[\"-83 -180 82 180\"],\"time_start\":\"2000-03-01T00:00:00.000Z\",\"version_id\":\"003\",\"updated\":\"2015-09-30T10:42:35.418Z\",\"dataset_id\":\"ASTER Global Digital Elevation Model V003\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"ASTGTM\",\"organizations\":[\"LP DAAC\",\"NASA/JPL/ASTER\"],\"title\":\"ASTER Global Digital Elevation Model V003\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The ASTER Global Digital Elevation Model (GDEM) Version 3 (ASTGTM) provides a global digital elevation model (DEM) of land areas on Earth at a spatial resolution of 1 arc second (approximately 30 meter horizontal posting at the equator).\\\\r\\\\n\\\\r\\\\nThe development of the ASTER GDEM data products is a collaborative effort between National Aeronautics and Space Administration (NASA) and Japan\\xe2\\x80\\x99s Ministry of Economy, Trade, and Industry (METI). The ASTER GDEM data products are created by the Sensor Information Laboratory Corporation (SILC) in Tokyo. \\\\r\\\\n\\\\r\\\\nThe ASTER GDEM Version 3 data product was created from the automated processing of the entire ASTER Level 1A (https://doi.org/10.5067/ASTER/AST_L1A.003) archive of scenes acquired between March 1, 2000, and November 30, 2013. Stereo correlation was used to produce over one million individual scene based ASTER DEMs, to which cloud masking was applied. All cloud screened DEMs and non-cloud screened DEMs were stacked. Residual bad values and outliers were removed. In areas with limited data stacking, several existing reference DEMs were used to supplement ASTER data to correct for residual anomalies. Selected data were averaged to create final pixel values before partitioning the data into 1 degree latitude by 1 degree longitude tiles with a one pixel overlap. To correct elevation values of water body surfaces, the ASTER Global Water Bodies Database (ASTWBD) (https://doi.org/10.5067/ASTER/ASTWBD.001) Version 1 data product was also generated. \\\\r\\\\n\\\\r\\\\nThe geographic coverage of the ASTER GDEM extends from 83\\xc2\\xb0 North to 83\\xc2\\xb0 South. Each tile is distributed in GeoTIFF format and projected on the 1984 World Geodetic System (WGS84)/1996 Earth Gravitational Model (EGM96) geoid. Each of the 22,912 tiles in the collection contain at least 0.01% land area. \\\\r\\\\n\\\\r\\\\nProvided in the ASTER GDEM product are layers for DEM and number of scenes (NUM). The NUM layer indicates the number of scenes that were processed for each pixel and the source of the data.\\\\r\\\\n\\\\r\\\\nWhile the ASTER GDEM Version 3 data products offer substantial improvements over Version 2, users are advised that the products still may contain anomalies and artifacts that will reduce its usability for certain applications. \\\\r\\\\n\\\\r\\\\nImprovements/Changes from Previous Versions \\\\r\\\\n\\xe2\\x80\\xa2 Expansion of acquisition coverage to increase the amount of cloud-free input scenes from about 1.5 million in Version 2 to about 1.88 million scenes in Version 3.\\\\r\\\\n\\xe2\\x80\\xa2 Separation of rivers from lakes in the water body processing. \\\\r\\\\n\\xe2\\x80\\xa2 Minimum water body detection size decreased from 1 km2 to 0.2 km2. \",\"time_end\":\"2013-11-30T23:59:59.999Z\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C1711961296-LPCLOUD\",\"has_formats\":false,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":true,\"platforms\":[\"Terra\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q= C1711961296-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/ASTER/ASTGTM.003\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://asterweb.jpl.nasa.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/434/ASTGTM_User_Guide_V3.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://cmr.earthdata.nasa.gov/browse-scaler/browse_images/granules/G1726374754-LPCLOUD?h=512&w=512\"}]}]}}'\n\n\nA more convenient way to work with this information is to use json formatted data. I’m using pretty print pprint to print the data in an easy to read way.\nNote - response.json() will format our response in json - ['feed']['entry'] returns all entries that CMR returned in the request (not the same as CMR-Hits) - [0] returns the first entry. Reminder that python starts indexing at 0, not 1!\n\npprint(response.json()['feed']['entry'][0])\n\n{'archive_center': 'LP DAAC',\n 'boxes': ['-90 -180 90 180'],\n 'browse_flag': True,\n 'collection_data_type': 'OTHER',\n 'consortiums': ['GEOSS', 'EOSDIS'],\n 'coordinate_system': 'CARTESIAN',\n 'data_center': 'LPCLOUD',\n 'dataset_id': 'HLS Landsat Operational Land Imager Surface Reflectance and '\n               'TOA Brightness Daily Global 30m v2.0',\n 'has_formats': False,\n 'has_spatial_subsetting': False,\n 'has_temporal_subsetting': False,\n 'has_transforms': False,\n 'has_variables': False,\n 'id': 'C2021957657-LPCLOUD',\n 'links': [{'href': 'https://search.earthdata.nasa.gov/search?q=C2021957657-LPCLOUD',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n           {'href': 'https://doi.org/10.5067/HLS/HLSL30.002',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/1118/HLS_User_Guide_V2.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/769/HLS_ATBD_V15_provisional.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/768/HLS_Quick_Guide_v011.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-tutorial/browse',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-super-script/browse',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://cmr.earthdata.nasa.gov/browse-scaler/browse_images/granules/G2095313663-LPCLOUD?h=512&w=512',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n           {'href': 'https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-bulk-download/browse',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/resources/e-learning/getting-started-with-cloud-native-harmonized-landsat-sentinel-2-hls-data-in-r/',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'}],\n 'online_access_flag': True,\n 'orbit_parameters': {},\n 'organizations': ['LP DAAC', 'NASA/IMPACT'],\n 'original_format': 'UMM_JSON',\n 'platforms': ['LANDSAT-8'],\n 'processing_level_id': '3',\n 'service_features': {'esi': {'has_formats': False,\n                              'has_spatial_subsetting': False,\n                              'has_temporal_subsetting': False,\n                              'has_transforms': False,\n                              'has_variables': False},\n                      'harmony': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False},\n                      'opendap': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False}},\n 'short_name': 'HLSL30',\n 'summary': 'The Harmonized Landsat and Sentinel-2 (HLS) project provides '\n            'consistent surface reflectance (SR) and top of atmosphere (TOA) '\n            'brightness data from the Operational Land Imager (OLI) aboard the '\n            'joint NASA/USGS Landsat 8 satellite and the Multi-Spectral '\n            'Instrument (MSI) aboard Europe’s Copernicus Sentinel-2A and '\n            'Sentinel-2B satellites. The combined measurement enables global '\n            'observations of the land every 2–3 days at 30-meter (m) spatial '\n            'resolution. The HLS project uses a set of algorithms to obtain '\n            'seamless products from OLI and MSI that include atmospheric '\n            'correction, cloud and cloud-shadow masking, spatial '\n            'co-registration and common gridding, illumination and view angle '\n            'normalization, and spectral bandpass adjustment.\\r\\n'\n            '\\r\\n'\n            'The HLSL30 product provides 30-m Nadir Bidirectional Reflectance '\n            'Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is '\n            'derived from Landsat 8 OLI data products. The HLSS30 and HLSL30 '\n            'products are gridded to the same resolution and Military Grid '\n            'Reference System '\n            '([MGRS](https://hls.gsfc.nasa.gov/products-description/tiling-system/)) '\n            'tiling system, and thus are “stackable” for time series '\n            'analysis.\\r\\n'\n            '\\r\\n'\n            'The HLSL30 product is provided in Cloud Optimized GeoTIFF (COG) '\n            'format, and each band is distributed as a separate file. There '\n            'are 11 bands included in the HLSL30 product along with one '\n            'quality assessment (QA) band and four angle bands. See the User '\n            'Guide for a more detailed description of the individual bands '\n            'provided in the HLSL30 product.',\n 'time_start': '2013-04-11T00:00:00.000Z',\n 'title': 'HLS Landsat Operational Land Imager Surface Reflectance and TOA '\n          'Brightness Daily Global 30m v2.0',\n 'updated': '2015-12-03T10:57:07.000Z',\n 'version_id': '2.0'}\n\n\nThe first response contains a lot more information than we need. We’ll narrow in on a few fields to get a feel for what we have. We’ll print the name of the dataset (dataset_id) and the concept id (id). We can build this variable and print statement like we did above with the url variable.\n\ncollections = response.json()['feed']['entry']\n\n\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} | {collection[\"dataset_id\"]} | {collection[\"id\"]}')\n\nLP DAAC | HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 | C2021957657-LPCLOUD\nLP DAAC | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | C2021957295-LPCLOUD\nLP DAAC | ASTER Global Digital Elevation Model V003 | C1711961296-LPCLOUD\n\n\nIn some situations we may be expecting a certain number of results. Only 10 datasets are return be default. This can be modified by setting the page_size parameter to a different any value less than or equal to 2000 (2000 is the maximum number of results return by CMR). Note, this is different that what we see from CMR-Hits in the header, which is the number of entries found that are available for request.\n\n\n\nIn NASA speak, Granules are files or groups of files. In this example, we will search for ECO2LSTE version 1 for a specified region of interest and datetime range.\nWe need to change the resource url to look for granules instead of collections\n\nurl = f'{CMR_OPS}/{\"granules\"}'\n\nWe will search by concept_id, temporal, and bounding_box. Details about these search parameters can be found in the CMR API Documentation.\nThe formatting of the values for each parameter is quite specific.\nconcept_id parameter is one to many collections/collection id(s) assigned to a list\ntemporal parameter are dates in ISO 8061 format yyyy-MM-ddTHH:mm:ssZ\nbounding_box parameter are coordinates in the order: lower left longitude, lower left latitude, upper right longitude, upper right latitude\n\ncollection_id = ['C2021957657-LPCLOUD', 'C2021957295-LPCLOUD']\ndate_range = '2020-10-17T00:00:00Z,2020-11-18T23:59:59Z'\nbbox = '-120.45264628,34.51050622,-120.40432448,34.53239876'\n\n\nresponse = requests.get(url, \n                        params={\n                            'concept_id': collection_id,\n                            'temporal': date_range,\n                            'bounding_box': bbox,\n                            'page_size': 200,\n                            },\n                        headers={\n                            'Accept': 'application/json'\n                            }\n                       )\nprint(response.status_code)\n\n200\n\n\n\nprint(response.headers['CMR-Hits'])\n\n10\n\n\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"data_center\"]} | {granule[\"dataset_id\"]} | {granule[\"id\"]}')\n\nLPCLOUD | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | G2167580011-LPCLOUD\nLPCLOUD | HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 | G2152756095-LPCLOUD\nLPCLOUD | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | G2167475882-LPCLOUD\nLPCLOUD | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | G2167295721-LPCLOUD\nLPCLOUD | HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 | G2152682346-LPCLOUD\nLPCLOUD | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | G2166867404-LPCLOUD\nLPCLOUD | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | G2166768089-LPCLOUD\nLPCLOUD | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | G2166693426-LPCLOUD\nLPCLOUD | HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 | G2152120260-LPCLOUD\nLPCLOUD | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | G2165191762-LPCLOUD\n\n\n\npprint(granules[0])\n\n{'browse_flag': True,\n 'collection_concept_id': 'C2021957295-LPCLOUD',\n 'coordinate_system': 'GEODETIC',\n 'data_center': 'LPCLOUD',\n 'dataset_id': 'HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance '\n               'Daily Global 30m v2.0',\n 'day_night_flag': 'DAY',\n 'id': 'G2167580011-LPCLOUD',\n 'links': [{'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B09.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B09.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B09.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B07.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B07.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B07.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B01.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B01.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B01.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B04.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B04.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B04.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B12.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B12.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B12.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.SZA.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.SZA.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.SZA.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B03.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B03.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B03.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B11.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B11.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B11.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B02.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B02.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B02.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B08.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B08.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B08.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B10.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B10.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B10.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.SAA.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.SAA.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.SAA.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.VZA.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.VZA.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.VZA.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B05.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B05.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B05.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B06.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B06.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B06.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B8A.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.B8A.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.B8A.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.Fmask.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.Fmask.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.Fmask.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.VAA.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.VAA.tif'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.VAA.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0_stac.json',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0_stac.json '\n                     '(EXTENDED METADATA)'},\n           {'href': 's3://lp-prod-public/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0_stac.json',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule (EXTENDED METADATA)'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.cmr.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.cmr.xml '\n                     '(EXTENDED METADATA)'},\n           {'href': 's3://lp-prod-protected/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.cmr.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule (EXTENDED METADATA)'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'api endpoint to retrieve temporary credentials valid for '\n                     'same-region direct s3 access (VIEW RELATED INFORMATION)'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.jpg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download HLS.S30.T10SGD.2020292T184411.v2.0.jpg'},\n           {'href': 's3://lp-prod-public/HLSS30.020/HLS.S30.T10SGD.2020292T184411.v2.0/HLS.S30.T10SGD.2020292T184411.v2.0.jpg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://search.earthdata.nasa.gov/search?q=C2021957295-LPCLOUD',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n           {'href': 'https://doi.org/10.5067/HLS/HLSS30.002',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/1118/HLS_User_Guide_V2.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/769/HLS_ATBD_V15_provisional.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/768/HLS_Quick_Guide_v011.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-tutorial/browse',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-super-script/browse',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://git.earthdata.nasa.gov/projects/LPDUR/repos/hls-bulk-download/browse',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/resources/e-learning/getting-started-with-cloud-native-harmonized-landsat-sentinel-2-hls-data-in-r/',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'}],\n 'online_access_flag': True,\n 'original_format': 'ECHO10',\n 'polygons': [['34.2068165 -119.6382697 35.1952508 -119.5980052 35.2231329 '\n               '-120.8028976 34.2336951 -120.8289365 34.2068165 -119.6382697']],\n 'producer_granule_id': 'HLS.S30.T10SGD.2020292T184411',\n 'time_end': '2020-10-18T18:54:56.060Z',\n 'time_start': '2020-10-18T18:54:56.060Z',\n 'title': 'HLS.S30.T10SGD.2020292T184411.v2.0',\n 'updated': '2021-11-17T08:22:16.691Z'}"
  },
  {
    "objectID": "tutorials/AppEEARS.html",
    "href": "tutorials/AppEEARS.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "Application for Extracting and Exploring Analysis Ready Samples (AρρEEARS)\nThe Application for Extracting and Exploring Analysis Ready Samples (AppEEARS) offers a simple and efficient way to access and transform geospatial data from a variety of federal data archives in an easy-to-use web application interface. AppEEARS enables users to subset geospatial data spatially, temporally, and by band/layer for point and area samples. AppEEARS returns not only the requested data, but also the associated quality values, and offers interactive visualizations with summary statistics in the web interface. This tutorial shows how to use AppEEARS to access ECOSTRESS version 2 data hosted in the cloud.\n\nStep 1. Sign in\nSign in using your Earthdata login credentials. If you do not have an Earthdata account, please see the Workshop Prerequisites for guidance.\n\n\n\nStep 2. Extract the Sample\nSelect the Point or Area sample using Extract dropdown. we will be directed to Extract Area or Point Sample page.\n1. Enter your sample name.\n2. Upload your area of ineterst or draw a polygon on the leaflet map for area sample. for point sample, provide the CSV file including the lattitude and longitude coordinates. You can also use the map to manually select your locations or type them directly.\n3. Select your time period of interest.\n4. Add datasets you are interested in to your Selected Layers. You can choose from various data collections available in AppEEARS. you can click on the (i) icon for the dataset to see more details. In this example we are interested in the ECOSTRESS LSTE which is managed by the LP DAAC and made available from the NASA Earthdata Cloud archive hosted in AWS cloud. At the time of this workshop, only ECOSTRESS Level 1 and Level 2 swath data products are available in AppEEARS but Gridded and Tiled data will be added to AppEEARS in future.\n5. For area sample, you can select your output file format. You also have an option to reproject all your layers to another coordinate reference system.\n6. Now you can submit.\n\n\n\nFigure caption: Extract area and point sample for ECOSTRESS data available in AWS cloud in AppEEARS\n\n\nOnce your request is complete, you can View and Download your results from the Explore Requets page.\n\n\n\nFigure caption: Refine search\n\n\n\n\nStep 3. Explore the outputs\nFrom the Explore Requests page, click the View icon in order to view and interact with your results. This will take you to the View Area Sample page. \n\n\nStep 4. Download the outputs\nFinally navigate to Download Sample page by clicking the Download icon on the Explore Requests page or from View Sample pag to download your results. Besides your actual outputs, you will have access to supporting files including a text file with URLs to source data, JSON file you can use to recreate the same sample, decoded quality information, and CSV file with the layer statistics.\n\n\n\nFigure caption: Download Sample Results\n\n\nCheck out AppEEARS help documentation for more details. If you wish to access AppEEARS programatically checkout AppEEARS API documenation."
  },
  {
    "objectID": "tutorials/Getting_started_setup.html",
    "href": "tutorials/Getting_started_setup.html",
    "title": "Setup for tutorials",
    "section": "",
    "text": "This tutorial will help you set up your JupyterHub (or Hub) with tutorials and other materials from our Workshop folder."
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#step-1.-login-to-the-hub",
    "href": "tutorials/Getting_started_setup.html#step-1.-login-to-the-hub",
    "title": "Setup for tutorials",
    "section": "Step 1. Login to the Hub",
    "text": "Step 1. Login to the Hub\nPlease go to Jupyter Hub and Log in with your GitHub Account, and select “Small”.\nAlternatively, you can also click this badge to launch the Hub:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: It takes a few minutes for the Hub to load. Please be patient!\n\nWhile the Hub loads, we’ll:\n\nDiscuss cloud environments\n\nSee how my Desktop is setup\n\nDiscuss python and conda environments\n\nThen, when the Hub is loaded, we’ll get oriented in the Hub."
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#discussion-cloud-environment",
    "href": "tutorials/Getting_started_setup.html#discussion-cloud-environment",
    "title": "Setup for tutorials",
    "section": "Discussion: Cloud environment",
    "text": "Discussion: Cloud environment\nA brief overview. See NASA Openscapes Cloud Environment in the 2021-Cloud-Hackathon book for more detail.\n\nCloud infrastructure\n\nCloud: AWS us-west-2\n\nData: AWS S3 (cloud) and NASA DAAC data centers (on-prem).\n\nCloud compute environment: 2i2c Jupyterhub deployment\n\nIDE: JupyterLab"
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#discussion-my-desktop-setup",
    "href": "tutorials/Getting_started_setup.html#discussion-my-desktop-setup",
    "title": "Setup for tutorials",
    "section": "Discussion: My desktop setup",
    "text": "Discussion: My desktop setup\nI’ll screenshare to show and/or talk through how I have oriented the following software we’re using:\n\nWorkshop Book\nSlack"
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#discussion-python-and-conda-environments",
    "href": "tutorials/Getting_started_setup.html#discussion-python-and-conda-environments",
    "title": "Setup for tutorials",
    "section": "Discussion: Python and Conda environments",
    "text": "Discussion: Python and Conda environments\nWhy Python?\n\n\n\nPython Data Stack. Source: Jake VanderPlas, “The State of the Stack,” SciPy Keynote (SciPy 2015).\n\n\nDefault Python Environment:\nWe’ve set up the Python environment with conda.\n\n\n\n\n\n\nConda environment\n\n\n\n\n\nname: openscapes\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.9\n  - pangeo-notebook\n  - awscli~=1.20\n  - boto3~=1.19\n  - gdal~=3.3\n  - rioxarray~=0.8\n  - xarray~=0.19\n  - h5netcdf~=0.11\n  - netcdf4~=1.5\n  - h5py~=2.10\n  - geoviews~=1.9\n  - matplotlib-base~=3.4\n  - hvplot~=0.7\n  - pyproj~=3.2\n  - bqplot~=0.12\n  - geopandas~=0.10\n  - zarr~=2.10\n  - cartopy~=0.20\n  - shapely==1.7.1\n  - pyresample~=1.22\n  - joblib~=1.1\n  - pystac-client~=0.3\n  - s3fs~=2021.7\n  - ipyleaflet~=0.14\n  - sidecar~=0.5\n  - jupyterlab-geojson~=3.1\n  - jupyterlab-git\n  - jupyter-resource-usage\n  - ipympl~=0.6\n  - conda-lock~=0.12\n  - pooch~=1.5\n  - pip\n  - pip:\n    - tqdm\n    - harmony-py\n    - earthdata\n    - zarr-eosdis-store\n\n\n\n\nBash terminal and installed software\nLibraries that are available from the terminal\n\ngdal 3.3 commands ( gdalinfo, gdaltransform…)\nhdf5 commands ( h5dump, h5ls..)\nnetcdf4 commands (ncdump, ncinfo …)\njq (parsing json files or streams from curl)\ncurl (fetch resources from the web)\nawscli (AWS API client, to interact with AWS cloud services)\nvim (editor)\ntree ( directory tree)\nmore …\n\n\n\nUpdating the environment\nScientific Python is a vast space and we only included libraries that are needed in our tutorials. Our default environment can be updated to include any Python library that’s available on pip or conda.\nThe project used to create our default environment is called corn (as it can include many Python kernels).\nIf we want to update a library or install a whole new environment we need to open an issue on this repository.\n\n\ncorn 🌽"
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#step-2.-jupyterhub-orientation",
    "href": "tutorials/Getting_started_setup.html#step-2.-jupyterhub-orientation",
    "title": "Setup for tutorials",
    "section": "Step 2. JupyterHub orientation",
    "text": "Step 2. JupyterHub orientation\nNow that the Hub is loaded, let’s get oriented.\n\n\n\n\n\n\nFirst impressions\n\nLauncher & the big blue button\n“home directory”"
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#step-3.-navigate-to-the-workshop-folder",
    "href": "tutorials/Getting_started_setup.html#step-3.-navigate-to-the-workshop-folder",
    "title": "Setup for tutorials",
    "section": "Step 3. Navigate to the Workshop folder",
    "text": "Step 3. Navigate to the Workshop folder\nThe workshop folder 2022-ECOSTRESS-Cloud-Workshop is in the shared folder on JupyterHub."
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#jupyter-notebooks",
    "href": "tutorials/Getting_started_setup.html#jupyter-notebooks",
    "title": "Setup for tutorials",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nLet’s get oriented to Jupyter notebooks, which we’ll use in all the tutorials."
  },
  {
    "objectID": "tutorials/Getting_started_setup.html#how-do-i-end-my-session",
    "href": "tutorials/Getting_started_setup.html#how-do-i-end-my-session",
    "title": "Setup for tutorials",
    "section": "How do I end my session?",
    "text": "How do I end my session?\n(Also see How do I end my Openscapes session? Will I lose all of my work?) When you are finished working for the day it is important to explicitly log out of your Openscapes session. The reason for this is it will save money and is a good habit to be in. When you keep a session active it uses up AWS resources and keeps a series of virtual machines deployed.\nStopping the server happens automatically when you log out, so navigate to “File -> Log Out” and click “Log Out”!\n\n\n\nhub-control-panel-button (credit: UW Hackweek)\n\n\n!!! NOTE “logging out” - Logging out will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day."
  },
  {
    "objectID": "tutorials/Carpinteria-ECOSTRESS-Analysis.html",
    "href": "tutorials/Carpinteria-ECOSTRESS-Analysis.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "Gregory Halverson Jet Propulsion Laboratory, California Institute of Technology\nThis research was carried out at the Jet Propulsion Laboratory, California Institute of Technology, and was sponsored by ECOSTRESS and the National Aeronautics and Space Administration (80NM0018D0004).\n© 2022. All rights reserved.\n\n\nThese are some built-in Python functions we need for this notebook, including functions for handling filenames and dates.\n\nfrom os.path import join, abspath, basename, splitext\nfrom glob import glob\nfrom datetime import datetime, date, timedelta\nfrom zipfile import ZipFile\nimport warnings\n\nWe’re using the rioxarray package for loading raster data from GeoTIFF files, and we’re importing it as rxr. We’re using the numpy library to handle arrays, and we’re importing it as np. We’re using the rasterstats package for zonal statistics.\n\n!mamba install -q -y rasterstats\n\n\nimport rioxarray as rxr\nimport numpy as np\nfrom rasterstats import zonal_stats\n\nWe’re using the geopandas library to load vector data from GeoJSON files, and we’re importing it as gpd. We’re using the shapely library to handle vector data and the pyproj library to handle projections.\n\nimport geopandas as gpd\nfrom shapely.geometry import Point, box\nfrom shapely.ops import transform\nfrom pyproj import Transformer\n\nWe’re using the pandas library to handle tables, and we’re importing it as pd.\n\nimport pandas as pd\n\nWe’re using the seaborn library to produce our graphs, and we’re importing it as sns. We’re using the hvplot library to produce our maps. We’re using the matplotlib library to handle plotting figures, and we’re importing it as plt.\n\nimport seaborn as sns\nimport hvplot.xarray\nimport hvplot.pandas\nimport matplotlib.pyplot as plt\n\n\n\n\nThese constants define the dimensions of our figures. Feel free to adjust these to fit your display.\n\nFIG_WIDTH_PX = 1080\nFIG_HEIGHT_PX = 720\nFIG_WIDTH_IN = 16\nFIG_HEIGHT_IN = 9\nFIG_ALPHA = 0.7\nBASEMAP = \"ESRI\"\nSEABORN_STYLE = \"whitegrid\"\nsns.set_style(SEABORN_STYLE)\n\nThis is the location of the example ECOSTRESS Collection 2 product files.\n\nDATA_DIRECTORY = \"/home/jovyan/shared/2022-fall-ecostress-workshop/Carpinteria_ECOSTRESS_Collection_2\"\nprint(f\"data directory: {DATA_DIRECTORY}\")\n\n\n\n\nFirst, let’s trying opening a data layer from a product file.\n\nfilename = join(DATA_DIRECTORY, \"ECOv002_L2T_LSTE_12139_005_11SKU_20200826T191453_0700_01.zip\")\nprint(f\"example L2T LSTE file: {filename}\")\n\nThe granule ID for this granule can be parsed from the filename by dropping the .zip extension.\n\ngranule_ID = splitext(basename(filename))[0]\nprint(f\"granule ID: {granule_ID}\")\n\nThis product bundle, stored in zip format, contains a number of files, including raster data layers in GeoTIFF format as .tif files, and GeoJPEG browse images as .jpeg files. The GeoTIFF files can be loaded into GIS software, such as QGIS and ArcGIS. The GeoJPEG files can be loaded into Google Earth.\n\nwith ZipFile(filename) as zip_file:\n    for internal_file in zip_file.filelist:\n        print(internal_file.filename)\n\nThe ECOSTRESS Collection 2 tiled products include metadata in JSON format as a .json text file.\n\nwith ZipFile(filename) as zip_file:\n    metadata = zip_file.read(f\"{granule_ID}/{granule_ID}.json\").decode()\n\nprint(metadata)\n\nTo open the temperature layer of this file, we’ll form a Universion Resource Identifier (URI) with the pattern: zip://{filename}!/{granule_ID}/{granule_ID}_{variable}.tif\n\nURI = f\"zip://{abspath(filename)}!/{granule_ID}/{granule_ID}_LST.tif\"\nprint(f\"URI: {URI}\")\n\nWe’re using rioxarray to open the surface temperature layer from the L2T_LSTE product on the 11SKU tile covering the Carpinteria Salt Marsh. We’re passing the URI pointing to the GeoTIFF file contained within this zip file. If you unzip this product bundle or download the GeoTIFF file on its own, you can pass the filename of the GeoTIFF file directly into rioxarray.\n\nST_K_raster = rxr.open_rasterio(URI).squeeze('band', drop=True)\nST_K_raster\n\nThis xarray.DataArray object contains both an array of image values and spatial metadata. The rioxarray package extends xarray with a .rio attribute containing the metadata. Here we’re examining the coordinate reference system (CRS) of this image in the rioxarray metadata.\n\nCRS = ST_K_raster.rio.crs\nCRS.to_wkt()\n\nThis image is projected in UTM zone 11 north. Distances in this projection system are in meters. All tiled ECOSTRESS products are projected in UTM, but tiles are projected into different UTM zones, depending on where they are in the world.\nWe can also check the spatial resolution of the grid cells in this image in the .rio metadata.\n\ncell_width, cell_height = ST_K_raster.rio.resolution()\nprint(f\"cell width: {cell_width} cell height: {cell_height}\")\n\nThis image is projected with 70m square pixels, as are all tiled ECOSTRESS products.\nTo know the observation time for this granule, we’re parsing the timestamp from the filename. This timestamp is given as Coordinated Universal Time (UTC).\n\ndatetime_UTC = datetime.strptime(basename(filename).split(\"_\")[-3], \"%Y%m%dT%H%M%S\")\nprint(f\"date/time UTC: {datetime_UTC:%Y-%m-%d %H:%M:%S}\")\n\nWe want to know the centroid coordinate of this tile so that we can adjust the UTC time given to solar apparent time. We’re calculating the centroid as the average of x coordinate values and average of y coordinate values.\n\ncentroid_UTM = Point(np.nanmean(ST_K_raster.x), np.nanmean(ST_K_raster.y))\nprint(f\"centroid UTM: {centroid_UTM}\")\n\nThis centroid coordinate is in meters. We want to convert these projected x and y values in meters to geographic latitude and longitude in degress.\n\ncentroid_latlon = transform(Transformer.from_crs(CRS, \"EPSG:4326\", always_xy=True).transform, centroid_UTM)\nprint(f\"centroid lat/lon: {centroid_latlon.wkt}\")\n\nWe’re shifting the UTC time to local time according to longitude.\n\ndatetime_solar = datetime_UTC + timedelta(hours=(np.radians(centroid_latlon.x) / np.pi * 12))\nprint(f\"date/time solar: {datetime_solar:%Y-%m-%d %H:%M:%S}\")\n\nThe hvplot package extends xarray to allow us to plot maps. We’re reprojecting the raster geographic projection EPSG 4326 to overlay on the basemap with a latitude and longitude graticule. We’re using the jet color scheme to render temperature with a rainbow of colors with red meaning hot and blue meaning cool. We’re setting the alpha to make the raster semi-transparent on top of the basemap. We’re filtering out values lower than the 2% percentile and higher than the 98% percentile to make the variation in the image more visible.\n\nST_CMAP = \"jet\"\n\nST_K_map = ST_K_raster.rio.reproject(\"EPSG:4326\").hvplot.image(\n    geo=True,\n    cmap=ST_CMAP,\n    tiles=BASEMAP,\n    alpha=FIG_ALPHA,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(ST_K_raster.quantile(0.02), ST_K_raster.quantile(0.98)),\n    title=f\"{granule_ID} Surface Temperature (Kelvin)\"\n)\n\nST_K_map = ST_K_map.options(xlabel=\"Longitude\", ylabel=\"Latitude\")\nST_K_map\n\nThe temperatures in the L2T_LSTE product are given in Kelvin. To convert them to Celsius, we subtract 273.15.\n\nST_C_raster = ST_K_raster.copy()\nST_C_raster.data = ST_K_raster.data - 273.15\n\nST_C_map = ST_C_raster.rio.reproject(\"EPSG:4326\").hvplot.image(\n    geo=True,\n    cmap=ST_CMAP,\n    tiles=BASEMAP,\n    alpha=FIG_ALPHA,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(ST_C_raster.quantile(0.02), ST_C_raster.quantile(0.98)),\n    title=f\"{granule_ID} Surface Temperature (Celsius)\"\n)\n\nST_C_map = ST_C_map.options(xlabel=\"Longitude\", ylabel=\"Latitude\")\nST_C_map\n\n\n\n\nWe want to analyze the ECOSTRESS images using the Carpinteria Salt Marsh Habitat Polygons provided by the USGS. This dataset is included here in GeoJSON format, which we’ll load using the geopandas package.\n\nlandcover_latlon = gpd.read_file(\"landcover.geojson\")\nlandcover_latlon\n\nTo align this vector dataset with the raster datasets, we need to project it to the UTM projection used for the rasters.\n\nlandcover_UTM = landcover_latlon.to_crs(ST_C_raster.rio.crs)\nlandcover_UTM\n\nThis vector dataset contains polygons classifying the surface of the Carpinteria Salt Marsh into channel, salt flat, upland, pan, and marsh.\n\nlandcover_colors = {\n    \"channel\": \"blue\",\n    \"marsh\": \"yellow\",\n    \"pan\": \"green\",\n    \"salt flat\": \"white\",\n    \"upland\": \"brown\"\n}\n\nlandcover_map = landcover_latlon.to_crs(\"EPSG:4326\").hvplot.polygons(\n    geo=True,\n    color=landcover_UTM[\"type\"].apply(lambda type: landcover_colors[type]),\n    tiles=BASEMAP,\n    alpha=FIG_ALPHA,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    title=\"Carpinteria Salt Marsh Habitat Polygons\"\n)\n\nlandcover_map = landcover_map.options(xlabel=\"Longitude\", ylabel=\"Latitude\")\nlandcover_map\n\nTo compare the raster image to the vector dataset, we want to subset the raster to the bounds of the vector dataset. We’re getting the bounds of our study area in meters from the convex hull of our land-cover polygons with a 100 meter buffer.\n\nxmin, ymin, xmax, ymax = landcover_UTM.unary_union.convex_hull.buffer(100).bounds\nxmin, ymin, xmax, ymax\n\nWe’re using this bounding box obtained from the extent of the vector dataset to clip the extent of the raster dataset. Now we can look at the ECOSTRESS surface temperature over the Carpinteria Salt Marsh.\n\nST_C_subset = ST_C_raster.rio.clip([box(xmin, ymin, xmax, ymax)])\n\nST_C_subset_map = ST_C_subset.rio.reproject(\"EPSG:4326\").hvplot.image(\n    geo=True,\n    cmap=ST_CMAP,\n    tiles=BASEMAP,\n    alpha=FIG_ALPHA,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(ST_C_subset.quantile(0.02), ST_C_subset.quantile(0.98)),\n    title = \"Carpinteria Salt Marsh Surface Temperature (Celsius)\"\n)\n\nST_C_subset_map = ST_C_subset_map.options(xlabel=\"Longitude\", ylabel=\"Latitude\")\nST_C_subset_map\n\nWe’re comparing the raster dataset to the polygon dataset by calculating zonal statistics with the rasterstats package.\n\nlandcover_stats = pd.DataFrame(zonal_stats(\n    landcover_UTM,\n    ST_C_subset.data,\n    affine=ST_C_subset.rio.transform(),\n    nodata=np.nan,\n    stats=[\"min\", \"median\", \"max\"]\n))\n\nlandcover_stats[\"type\"] = landcover_UTM[\"type\"]\nlandcover_stats[\"variable\"] = \"ST_C\"\nlandcover_stats[\"datetime_solar\"] = datetime_solar\nlandcover_stats = landcover_stats[[\"type\", \"variable\", \"datetime_solar\", \"min\", \"median\", \"max\"]]\nlandcover_stats\n\n\n\n\nWe’re using the built-in glob function to search for L2T_LSTE filenames in our downloaded collection of ECOSTRESS files.\n\nL2T_LSTE_filenames = sorted(glob(join(DATA_DIRECTORY, \"*_L2T_LSTE_*.zip\")))\nL2T_LSTE_filenames\n\nLet’s run our zonal statistics analysis on each of the ECOSTRESS granules we have available, and produce a table of these statistics.\n\nsalt_marsh_ST = pd.DataFrame({}, columns=[\"datetime_solar\", \"type\", \"ST\"])\n\nfor i, filename in enumerate(L2T_LSTE_filenames):\n    granule_ID = splitext(basename(filename))[0]\n    print(f\"({i+1}/{len(L2T_LSTE_filenames)}) {granule_ID}\")\n    URI = f\"zip://{filename}!/{granule_ID}/{granule_ID}_LST.tif\"\n    ST_K_raster = rxr.open_rasterio(URI).squeeze('band', drop=True)\n    ST_C_raster = ST_K_raster.copy()\n    ST_C_raster.data = ST_K_raster.data - 273.15\n    datetime_UTC = datetime.strptime(basename(filename).split(\"_\")[-3], \"%Y%m%dT%H%M%S\")\n    centroid_UTM = Point(np.nanmean(ST_K_raster.x), np.nanmean(ST_K_raster.y))\n    centroid_latlon = transform(Transformer.from_crs(CRS, \"EPSG:4326\", always_xy=True).transform, centroid_UTM)\n    datetime_solar = datetime_UTC + timedelta(hours=(np.radians(centroid_latlon.x) / np.pi * 12))\n    ST_C_subset = ST_C_raster.rio.clip([box(xmin, ymin, xmax, ymax)])\n\n    landcover_stats = pd.DataFrame(zonal_stats(\n        landcover_UTM,\n        ST_C_subset.data,\n        affine=ST_C_subset.rio.transform(),\n        nodata=np.nan,\n        stats=[\"median\"]\n    ))\n\n    landcover_stats[\"type\"] = landcover_UTM[\"type\"]\n    landcover_stats[\"datetime_solar\"] = datetime_solar\n    landcover_stats[\"ST\"] = landcover_stats[\"median\"].apply(lambda value: np.nan if value is None else value)\n    landcover_stats = landcover_stats[[\"datetime_solar\", \"type\", \"ST\"]]\n    salt_marsh_ST = pd.concat([salt_marsh_ST, landcover_stats])\n\nsalt_marsh_ST = salt_marsh_ST.dropna()\nsalt_marsh_ST\n\n\n\n\nLet’s examine the distribution of ECOSTRESS surface temperatures seen in the different land-cover types within the Carpinteria Salt Marsh. To visualize these distributions, we’re plotting them as boxplots. We’re using the seaborn library to generate these boxplots.\nWe tend to see cooler temperatures where water settles in the channel and warmer temperatures in the upland and salt flat areas, which are above the water and lack vegetation. Moderate temperatures are seen in the vegetated marsh and pan areas in between.\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    salt_marsh_ST_overall_median = salt_marsh_ST.groupby(\"type\").median().reset_index()[[\"type\", \"ST\"]].sort_values(\"ST\")\n\nfig, ax = plt.subplots(figsize=(FIG_WIDTH_IN, FIG_HEIGHT_IN))\n\nsns.boxplot(\n    data=salt_marsh_ST,\n    x=\"type\",\n    y=\"ST\",\n    order=salt_marsh_ST_overall_median.type,\n    palette=landcover_colors,\n    ax=ax\n)\n\nax.set(xlabel=\"Land Cover Type\", ylabel=\"Median Surface Temperature (Celsius)\")\nyticks = range(int(np.nanmin(salt_marsh_ST.ST)), int(np.nanmax(salt_marsh_ST.ST)) + 2)\nax.set_yticks(yticks)\nyticklabels = [f\"{tick}°C\" for tick in yticks]\nax.set_yticklabels(yticklabels)\nplt.title(\"Carpinteria Salt Marsh Median ECOSTRESS Surface Temperature Boxplots by Land Cover\")\nplt.show()\nplt.close(fig)\n\nprint(salt_marsh_ST_overall_median)\n\n\n\n\nLet’s examine a time-series of ECOSTRESS surface temperature. To make this time-series smooth, we’ll aggregate our zonal statistics by season.\n\nsalt_marsh_ST_seasonal = salt_marsh_ST.copy()\nsalt_marsh_ST_seasonal[\"season\"] = salt_marsh_ST_seasonal.apply(\n    lambda row: date(int(row.datetime_solar.year), int(((row.datetime_solar.month - 1) // 3) * 3 + 1), 1), axis=1)\nsalt_marsh_ST_seasonal = salt_marsh_ST_seasonal[[\"season\", \"type\", \"ST\"]]\nsalt_marsh_ST_seasonal = salt_marsh_ST_seasonal.dropna().groupby(\n    by=[\"season\", \"type\"]).median().reset_index()\nsalt_marsh_ST_seasonal\n\nWe’re using the seaborn package again to visualize this timeseries as a line-plot. In this line-plot, we can see that temperatures in the Carptineria Salt Marsh tend to drop in the Winter and rise in the Summer. Channel and marsh temperatures tend to be the coolest for much of the year, and upland and salt flat temperatures tend to be the warmest for much of the year.\n\nfig, ax = plt.subplots(figsize=(FIG_WIDTH_IN, FIG_HEIGHT_IN))\n\nsns.lineplot(\n    data=salt_marsh_ST_seasonal,\n    x=\"season\",\n    y=\"ST\",\n    hue=\"type\",\n    ax=ax\n)\n\nax.set(xlabel=\"Season\", ylabel=\"Seasonal Median Surface Temperature (Celsius)\")\nxticks = sorted(np.unique(salt_marsh_ST_seasonal.season))\nax.set_xticks(xticks)\nseasons = {1: \"Winter\", 4: \"Spring\", 7: \"Summer\", 10: \"Fall\"}\nax.set_xticklabels([f\"{seasons[xtick.month]}\\n{xtick.year}\" for xtick in xticks])\nyticks = range(int(min(salt_marsh_ST_seasonal.ST)), int(max(salt_marsh_ST_seasonal.ST)) + 1)\nax.set_yticks(yticks)\nax.set_yticklabels([f\"{ytick:0.0f}°C\" for ytick in yticks])\n\nplt.legend(title=\"Marsh Land Type\")\nplt.title(\"Carpinteria Salt Marsh Seasonal Median ECOSTRESS Surface Temperature Timeline by Land Cover\")\nplt.show()\nplt.close(fig)\n\n\n\n\nNow let’s repeat our time-series analysis using the ECOSTRESS evapotranspiration product. ECOSTRESS Collection 2 includes the JPL Evapotranspiration Ensemble (JET) product, which runs several models to create a balanced ET estimate. We’re searching through our set of downloaded ECOSTRESS files for L3T_JET granules using glob.\n\nL3T_JET_filenames = sorted(glob(join(DATA_DIRECTORY, \"*_L3T_JET_*.zip\")))\nL3T_JET_filenames\n\nWe’re running our zonal statistics time-series analysis again, but now loading daily evapotranspiration (ET) in millimeters per day from the L3T JET product.\n\nsalt_marsh_ET = pd.DataFrame({}, columns=[\"datetime_solar\", \"type\", \"ET\"])\n\nfor i, filename in enumerate(L3T_JET_filenames):\n    granule_ID = splitext(basename(filename))[0]\n    print(f\"({i+1}/{len(L3T_JET_filenames)}) {granule_ID}\")\n    URI = f\"zip://{filename}!/{granule_ID}/{granule_ID}_ETdaily.tif\"\n    ET_raster = rxr.open_rasterio(URI).squeeze('band', drop=True)\n    datetime_UTC = datetime.strptime(\n        basename(filename).split(\"_\")[-3], \"%Y%m%dT%H%M%S\")\n    centroid_UTM = Point(np.nanmean(ST_K_raster.x), np.nanmean(ST_K_raster.y))\n    centroid_latlon = transform(Transformer.from_crs(\n        CRS, \"EPSG:4326\", always_xy=True).transform, centroid_UTM)\n    datetime_solar = datetime_UTC + \\\n        timedelta(hours=(np.radians(centroid_latlon.x) / np.pi * 12))\n    ET_subset = ET_raster.rio.clip([box(xmin, ymin, xmax, ymax)])\n\n    landcover_stats = pd.DataFrame(zonal_stats(\n        landcover_UTM,\n        ET_subset.data,\n        affine=ET_subset.rio.transform(),\n        nodata=np.nan,\n        stats=[\"median\"]\n    ))\n\n    landcover_stats[\"type\"] = landcover_UTM[\"type\"]\n    landcover_stats[\"datetime_solar\"] = datetime_solar\n    landcover_stats[\"ET\"] = landcover_stats[\"median\"].apply(lambda value: np.nan if value is None or value == 0 else value)\n    landcover_stats = landcover_stats[[\"datetime_solar\", \"type\", \"ET\"]]\n    salt_marsh_ET = pd.concat([salt_marsh_ET, landcover_stats])\n\nsalt_marsh_ET = salt_marsh_ET.dropna()\nsalt_marsh_ET\n\nNow let’s produce seaborn box-plots again to visualize the distribution of ECOSTRESS evapotranspiration. The non-vegetated upland and salt flat areas tend to be the driest, while the channel and marsh areas tend to be the wettest.\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    salt_marsh_ET_overall_median = salt_marsh_ET.groupby(\"type\").median().reset_index()[[\"type\", \"ET\"]].sort_values(\"ET\")\n\nfig, ax = plt.subplots(figsize=(FIG_WIDTH_IN, FIG_HEIGHT_IN))\n\nsns.boxplot(\n    data=salt_marsh_ET,\n    x=\"type\",\n    y=\"ET\",\n    order=salt_marsh_ET_overall_median.type,\n    palette=landcover_colors,\n    ax=ax\n)\n\nax.set(xlabel=\"Land Cover Type\", ylabel=\"Median Daily Evapotranspiration (mm)\")\nyticks = np.arange(np.round(np.nanmin(salt_marsh_ET.ET)), np.round(np.nanmax(salt_marsh_ET.ET)) + 0.2, 0.1)\nax.set_yticks(yticks)\nyticklabels = [f\"{tick:.1f} mm\" for tick in yticks]\nax.set_yticklabels(yticklabels)\nplt.title(\"Carpinteria Salt Marsh Median ECOSTRESS Daily Evapotranspiration Boxplots by Land Cover\")\nplt.show()\nplt.close(fig)\nprint(salt_marsh_ET_overall_median)\n\nLet’s create another seasonal aggregate, now for ECOSTRESS evapotranspiration.\n\nsalt_marsh_ET_seasonal = salt_marsh_ET.copy()\nsalt_marsh_ET_seasonal[\"season\"] = salt_marsh_ET_seasonal.apply(\n    lambda row: date(int(row.datetime_solar.year), int(((row.datetime_solar.month - 1) // 3) * 3 + 1), 1), axis=1)\nsalt_marsh_ET_seasonal = salt_marsh_ET_seasonal[[\"season\", \"type\", \"ET\"]]\nsalt_marsh_ET_seasonal = salt_marsh_ET_seasonal.dropna().groupby(\n    by=[\"season\", \"type\"]).median().reset_index()\nsalt_marsh_ET_seasonal\n\nWe’re using a seaborn line-plot again to visualize the seasonal aggregate of ECOSTRESS evapotranspiration by land-cover type. The channel and pan areas become dry in the heat of the Summer but then become wet again in the Autumn.\n\nfig, ax = plt.subplots(figsize=(FIG_WIDTH_IN, FIG_HEIGHT_IN))\n\nsns.lineplot(\n    data=salt_marsh_ET_seasonal,\n    x=\"season\",\n    y=\"ET\",\n    hue=\"type\",\n    ax=ax\n)\n\nax.set(xlabel=\"Season\", ylabel=\"Seasonal Median Daily Evapotranspiration (mm)\")\nxticks = sorted(np.unique(salt_marsh_ET_seasonal.season))\nax.set_xticks(xticks)\nseasons = {1: \"Winter\", 4: \"Spring\", 7: \"Summer\", 10: \"Fall\"}\nax.set_xticklabels([f\"{seasons[xtick.month]}\\n{xtick.year}\" for xtick in xticks])\nyticks = np.arange(np.round(np.nanmin(salt_marsh_ET_seasonal.ET), 1), np.round(np.nanmax(salt_marsh_ET_seasonal.ET), 1) + 0.1, 0.1)\nax.set_yticks(yticks)\nyticklabels = [f\"{tick:.1f} mm\" for tick in yticks]\nax.set_yticklabels(yticklabels)\nplt.legend(title=\"Marsh Land Type\")\nplt.title(\"Carpinteria Salt Marsh Seasonal Median ECOSTRESS Evapotranspiration Timeline by Land Cover\")\nplt.show()\nplt.close(fig)\n\n\n\n\nNow let’s compare the seasonal time-series of surface temperature and evapotranspiration. First we’re merging the surface temperature and evapotranspiration tables we’re produced into a single table using pandas.\n\nsalt_marsh_seasonal = pd.merge(salt_marsh_ST_seasonal, salt_marsh_ET_seasonal, how=\"inner\").dropna().groupby([\"season\", \"type\"]).median().reset_index()\nsalt_marsh_seasonal\n\nLet’s compare the seasonal timelines of ECOSTRESS surface temperature and evapotranspiration with one last seaborn line-plot. We’re plotting these lines together with a shared x axis for the season and separate y axes to compare temperature in Celsius to evapotranspiration in millimeters per day. We see a consistent and complementary seasonal cycle between these two variables, with cool temperatures and high evapotranspiration in the Winter and warm temperatures and low evapotranspiration in the Summer.\n\nfig, ax = plt.subplots(figsize=(FIG_WIDTH_IN, FIG_HEIGHT_IN))\nax.set(xlabel=\"Season\", ylabel=\"Seasonal Median Surface Temperature (Celsius)\")\nax.grid(True)\nyticks = np.arange(np.round(np.nanmin(salt_marsh_seasonal.ST)), np.round(np.nanmax(salt_marsh_seasonal.ST)))\nxticks = sorted(np.unique(salt_marsh_seasonal.season))\nax.set_xticks(xticks)\nseasons = {1: \"Winter\", 4: \"Spring\", 7: \"Summer\", 10: \"Fall\"}\nax.set_xticklabels([f\"{seasons[xtick.month]}\\n{xtick.year}\" for xtick in xticks])\nax.set_yticks(yticks)\nax.set_yticklabels([f\"{int(tick)} °C\" for tick in yticks])\n\nsns.lineplot(\n    data=salt_marsh_seasonal,\n    x=\"season\",\n    y=\"ST\",\n    color=\"red\",\n    ax=ax,\n    label=\"Surface Temperature\"\n)\n\nplt.legend(loc=\"upper left\")\nax2 = ax.twinx()\nax2.set(ylabel=\"Seasonal Median Daily Evapotranspiration (mm)\")\nax2.grid(False)\nyticks = np.arange(np.round(np.nanmin(salt_marsh_seasonal.ET), 1), np.round(np.nanmax(salt_marsh_seasonal.ET), 1), 0.1)\nax2.set_yticks(yticks)\nax2.set_yticklabels([f\"{tick:0.1f} mm\" for tick in yticks])\n\nsns.lineplot(\n    data=salt_marsh_seasonal,\n    x=\"season\",\n    y=\"ET\",\n    color=\"blue\",\n    ax=ax2,\n    label=\"Evapotranspiration\"\n)\n\nplt.legend(loc=\"upper right\")\nplt.title(\"Carpinteria Salt Marsh Seasonal Median ECOSTRESS Surface Temperature & Evapotranspiration Timeline\")\nplt.show()\nplt.close(fig)\n\nFinally, let’s visualize this inverse relationship between surface temperature and evapotranspiration with a scatter-plot, again using seaborn. The surface temperature is the x-axis and the evapotranspiration is the y-axis. In this scatter of points, we see a roughly linear relationship from cool and wet points in the upper left to warm and dry points in the lower right. We’re using regplot to plot this trend-line.\n\nfig, ax = plt.subplots(figsize=(FIG_WIDTH_IN, FIG_HEIGHT_IN))\nxticks = np.arange(np.round(np.nanmin(salt_marsh_seasonal.ST)), np.round(np.nanmax(salt_marsh_seasonal.ST)) + 1)\nax.set_xticks(xticks)\nax.set_xticklabels([f\"{int(tick)} °C\" for tick in xticks])\nyticks = np.arange(np.round(np.nanmin(salt_marsh_seasonal.ET), 1), np.round(np.nanmax(salt_marsh_seasonal.ET), 1) + 0.1, 0.1)\nax.set_yticks(yticks)\nax.set_yticklabels([f\"{tick:0.1f} mm\" for tick in yticks])\n\nsns.scatterplot(\n    data=salt_marsh_seasonal,\n    x=\"ST\",\n    y=\"ET\",\n    hue=\"type\",\n    palette=landcover_colors,\n    edgecolor=\"black\",\n    linewidth=1,\n    ax=ax\n)\n\nsns.regplot(\n    data=salt_marsh_seasonal,\n    x=\"ST\",\n    y=\"ET\",\n    scatter=False,\n    color=\"gray\",\n    ax=ax\n)\n\nax.set(xlabel=\"Seasonal Median Surface Temperature (Celsius)\", ylabel=\"Seasonal Median Daily Evapotranspiration (mm)\")\nplt.legend(title=\"Marsh Land Type\")\nplt.title(\"Carpinteria Salt Marsh Seasonal Surface Temperature and Evapotranspiration Scatterplot\")\nplt.show()\nplt.close(fig)\n\nTo quantify this inverse relationship, first we’ll calculate the correlation between these two variables using numpy. We see an inverse relationship between seasonally aggregated ECOSTRESS surface temperature and evapotranspiration in the Carpinteria Salt Marsh with a correlation coefficient of -0.49.\n\nnp.round(np.corrcoef(x=salt_marsh_seasonal.ST, y=salt_marsh_seasonal.ET)[0][1], 2)"
  },
  {
    "objectID": "tutorials/Intro_xarray_hvplot.html",
    "href": "tutorials/Intro_xarray_hvplot.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "As Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g. zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors.\n\n\n\nxarray is an open-source project and python package to work with labelled multi-dimensional arrays. It is leverages numpy, pandas, matplotlib and dask to build Dataset and DataArray objects with built-in methods to subset, analyze, interpolate, and plot multi-dimensional data. It makes working with multi-dimensional data cubes efficient and fun. It will change your life for the better. You’ll be more attractive, more interesting, and better equiped to take on lifes challenges.\n\n\n\nIn this tutorial you will learn how to:\n\nload a netcdf file into xarray\ninterrogate the Dataset and understand the difference between DataArray and Dataset\nsubset a Dataset\ncalculate annual and monthly mean fields\ncalculate a time series of zonal means\nplot these results\n\nAs always, we’ll start by importing xarray. We’ll follow convention by giving the module the shortname xr\n\nimport xarray as xr\nxr.set_options(keep_attrs=True)\nimport hvplot.xarray\n\n\n\n\n\n\n\n\n\n\n\nI’m going to use one of xarray’s tutorial datasets. In this case, air temperature from the NCEP reanalysis. I’ll assign the result of the open_dataset to ds. I may change this to access a dataset directly\n\nds = xr.tutorial.open_dataset(\"air_temperature\")\n\nAs we are in an interactive environment, we can just type ds to see what we have.\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air      (time, lat, lon) float32 ...\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25time: 2920lon: 53Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (1)air(time, lat, lon)float32...long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ][3869000 values with dtype=float32]Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\nFirst thing to notice is that ds is an xarray.Dataset object. It has dimensions, lat, lon, and time. It also has coordinate variables with the same names as these dimensions. These coordinate variables are 1-dimensional. This is a NetCDF convention. The Dataset contains one data variable, air. This has dimensions (time, lat, lon).\nClicking on the document icon reveals attributes for each variable. Clicking on the disk icon reveals a representation of the data.\nEach of the data and coordinate variables can be accessed and examined using the variable name as a key.\n\nds.air\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 2920, lat: 25, lon: 53)>\n[3869000 values with dtype=float32]\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 25lon: 53...[3869000 values with dtype=float32]Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\n\nds['air']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 2920, lat: 25, lon: 53)>\n[3869000 values with dtype=float32]\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 25lon: 53...[3869000 values with dtype=float32]Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nThese are xarray.DataArray objects. This is the basic building block for xarray.\nVariables can also be accessed as attributes of ds.\n\nds.time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'time' (time: 2920)>\narray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    standard_name:  time\n    long_name:      Timexarray.DataArray'time'time: 29202013-01-01 2013-01-01T06:00:00 ... 2014-12-31T18:00:00array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Coordinates: (1)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (2)standard_name :timelong_name :Time\n\n\nA major difference between accessing a variable as an attribute versus using a key is that the attribute is read-only but the key method can be used to update the variable. For example, if I want to convert the units of air from Kelvin to degrees Celsius.\n\nds['air'] = ds.air - 273.15\n\nThis approach can also be used to add new variables\n\nds['air_kelvin'] = ds.air + 273.15\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air         (time, lat, lon) float32 -31.95 -30.65 -29.65 ... 23.04 22.54\n    air_kelvin  (time, lat, lon) float32 241.2 242.5 243.5 ... 296.5 296.2 295.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25time: 2920lon: 53Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)air(time, lat, lon)float32-31.95 -30.65 ... 23.04 22.54long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 20.540009,  20.73999 ,  22.23999 , ...,  21.940002,\n          21.540009,  21.140015],\n        [ 23.140015,  24.040009,  24.440002, ...,  22.140015,\n          21.940002,  21.23999 ],\n        [ 24.640015,  25.23999 ,  25.339996, ...,  22.540009,\n          22.339996,  22.040009]],\n\n       [[-28.059998, -28.86    , -29.86    , ..., -31.460007,\n         -31.660004, -31.36    ],\n        [-23.259995, -23.86    , -24.759995, ..., -33.559998,\n         -32.86    , -31.460007],\n        [-10.160004, -10.959991, -11.76001 , ..., -33.259995,\n         -30.559998, -26.86    ],\n        ...,\n        [ 20.640015,  20.540009,  21.940002, ...,  22.140015,\n          21.940002,  21.540009],\n        [ 22.940002,  23.73999 ,  24.040009, ...,  22.540009,\n          22.540009,  22.040009],\n        [ 24.540009,  24.940002,  24.940002, ...,  23.339996,\n          23.040009,  22.540009]]], dtype=float32)air_kelvin(time, lat, lon)float32241.2 242.5 243.5 ... 296.2 295.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\nIt is helpful to update attributes such as units, this saves time, confusion and mistakes, especially when you save the dataset.\n\nds['air'].attrs['units'] = 'degC'\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air         (time, lat, lon) float32 -31.95 -30.65 -29.65 ... 23.04 22.54\n    air_kelvin  (time, lat, lon) float32 241.2 242.5 243.5 ... 296.5 296.2 295.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25time: 2920lon: 53Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)air(time, lat, lon)float32-31.95 -30.65 ... 23.04 22.54long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 20.540009,  20.73999 ,  22.23999 , ...,  21.940002,\n          21.540009,  21.140015],\n        [ 23.140015,  24.040009,  24.440002, ...,  22.140015,\n          21.940002,  21.23999 ],\n        [ 24.640015,  25.23999 ,  25.339996, ...,  22.540009,\n          22.339996,  22.040009]],\n\n       [[-28.059998, -28.86    , -29.86    , ..., -31.460007,\n         -31.660004, -31.36    ],\n        [-23.259995, -23.86    , -24.759995, ..., -33.559998,\n         -32.86    , -31.460007],\n        [-10.160004, -10.959991, -11.76001 , ..., -33.259995,\n         -30.559998, -26.86    ],\n        ...,\n        [ 20.640015,  20.540009,  21.940002, ...,  22.140015,\n          21.940002,  21.540009],\n        [ 22.940002,  23.73999 ,  24.040009, ...,  22.540009,\n          22.540009,  22.040009],\n        [ 24.540009,  24.940002,  24.940002, ...,  23.339996,\n          23.040009,  22.540009]]], dtype=float32)air_kelvin(time, lat, lon)float32241.2 242.5 243.5 ... 296.2 295.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\n\n\n\nSubsetting and indexing methods depend on whether you are working with a Dataset or DataArray. A DataArray can be accessed using positional indexing just like a numpy array. To access the temperature field for the first time step, you do the following.\n\nds['air'][0,:,:]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (lat: 25, lon: 53)>\narray([[-31.949997, -30.649994, -29.649994, ..., -40.350006, -37.649994,\n        -34.550003],\n       [-29.350006, -28.649994, -28.449997, ..., -40.350006, -37.850006,\n        -33.850006],\n       [-23.149994, -23.350006, -24.259995, ..., -39.949997, -36.759995,\n        -31.449997],\n       ...,\n       [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,  21.950012,\n         21.549988],\n       [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,  22.75    ,\n         22.049988],\n       [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,  23.640015,\n         23.450012]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n    time     datetime64[ns] 2013-01-01\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'lat: 25lon: 53-31.95 -30.65 -29.65 -29.15 -29.05 ... 24.64 24.45 23.75 23.64 23.45array([[-31.949997, -30.649994, -29.649994, ..., -40.350006, -37.649994,\n        -34.550003],\n       [-29.350006, -28.649994, -28.449997, ..., -40.350006, -37.850006,\n        -33.850006],\n       [-23.149994, -23.350006, -24.259995, ..., -39.949997, -36.759995,\n        -31.449997],\n       ...,\n       [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,  21.950012,\n         21.549988],\n       [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,  22.75    ,\n         22.049988],\n       [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,  23.640015,\n         23.450012]], dtype=float32)Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time()datetime64[ns]2013-01-01standard_name :timelong_name :Timearray('2013-01-01T00:00:00.000000000', dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nNote this returns a DataArray with coordinates but not attributes.\nHowever, the real power is being able to access variables using coordinate variables. I can get the same subset using the following. (It’s also more explicit about what is being selected and robust in case I modify the DataArray and expect the same output.)\n\nds['air'].sel(time='2013-01-01').time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'time' (time: 4)>\narray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2013-01-01T18:00:00\nAttributes:\n    standard_name:  time\n    long_name:      Timexarray.DataArray'time'time: 42013-01-01 2013-01-01T06:00:00 2013-01-01T12:00:00 2013-01-01T18:00:00array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Coordinates: (1)time(time)datetime64[ns]2013-01-01 ... 2013-01-01T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (2)standard_name :timelong_name :Time\n\n\n\nds.air.sel(time='2013-01-01')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 4, lat: 25, lon: 53)>\narray([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 22.450012,  22.25    ,  22.25    , ...,  23.140015,\n          22.140015,  21.850006],\n        [ 23.049988,  23.350006,  23.140015, ...,  23.25    ,\n          22.850006,  22.450012],\n        [ 23.25    ,  23.140015,  23.25    , ...,  23.850006,\n          23.850006,  23.640015]],\n\n       [[-31.259995, -31.350006, -31.350006, ..., -38.759995,\n         -37.649994, -35.550003],\n        [-26.850006, -27.850006, -28.949997, ..., -42.259995,\n         -41.649994, -38.649994],\n        [-16.549988, -18.449997, -21.050003, ..., -42.449997,\n         -41.350006, -37.050003],\n        ...,\n        [ 23.450012,  23.25    ,  22.850006, ...,  23.350006,\n          22.640015,  22.140015],\n        [ 23.850006,  24.350006,  23.950012, ...,  23.640015,\n          23.450012,  23.140015],\n        [ 24.350006,  24.549988,  24.350006, ...,  24.640015,\n          24.850006,  24.75    ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2013-01-01T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 4lat: 25lon: 53-31.95 -30.65 -29.65 -29.15 -29.05 ... 25.45 25.05 24.64 24.85 24.75array([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 22.450012,  22.25    ,  22.25    , ...,  23.140015,\n          22.140015,  21.850006],\n        [ 23.049988,  23.350006,  23.140015, ...,  23.25    ,\n          22.850006,  22.450012],\n        [ 23.25    ,  23.140015,  23.25    , ...,  23.850006,\n          23.850006,  23.640015]],\n\n       [[-31.259995, -31.350006, -31.350006, ..., -38.759995,\n         -37.649994, -35.550003],\n        [-26.850006, -27.850006, -28.949997, ..., -42.259995,\n         -41.649994, -38.649994],\n        [-16.549988, -18.449997, -21.050003, ..., -42.449997,\n         -41.350006, -37.050003],\n        ...,\n        [ 23.450012,  23.25    ,  22.850006, ...,  23.350006,\n          22.640015,  22.140015],\n        [ 23.850006,  24.350006,  23.950012, ...,  23.640015,\n          23.450012,  23.140015],\n        [ 24.350006,  24.549988,  24.350006, ...,  24.640015,\n          24.850006,  24.75    ]]], dtype=float32)Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2013-01-01T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nI can also do slices. I’ll extract temperatures for the state of Colorado. The bounding box for the state is [-109 E, -102 E, 37 N, 41 N].\nIn the code below, pay attention to both the order of the coordinates and the range of values. The first value of the lat coordinate variable is 41 N, the second value is 37 N. Unfortunately, xarray expects slices of coordinates to be in the same order as the coordinates. Note lon is 0 to 360 not -180 to 180, and I let python calculate it for me within the slice.\n\nds.air.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 2920, lat: 2, lon: 3)>\narray([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 40.0 37.5\n  * lon      (lon) float32 252.5 255.0 257.5\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 2lon: 3-10.05 -9.25 -8.75 -6.25 -6.55 ... -15.36 -13.66 -13.76 -15.96 -14.46array([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)Coordinates: (3)lat(lat)float3240.0 37.5standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([40. , 37.5], dtype=float32)lon(lon)float32252.5 255.0 257.5standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([252.5, 255. , 257.5], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nWhat if we want temperature for a point, for example Denver, CO (39.72510678889283 N, -104.98785545855408 E). xarray can handle this! If we just want data from the nearest grid point, we can use sel and specify the method as “nearest”.\n\ndenver_lat, denver_lon = 39.72510678889283, -104.98785545855408\n\n\nds.air.sel(lat=denver_lat, lon=360+denver_lon, method='nearest').hvplot()\n\n\n\n\n\n  \n\n\n\n\nIf we want to interpolate, we can use interp(). In this case I use linear or bilinear interpolation.\ninterp() can also be used to resample data to a new grid and even reproject data\n\nds.air.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 2920)>\narray([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\n    lat      float64 39.73\n    lon      float64 255.0\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920-8.951 -14.5 -18.44 -11.33 -8.942 ... -22.4 -27.79 -25.79 -15.42array([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])Coordinates: (3)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat()float6439.73standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray(39.72510679)lon()float64255.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray(255.01214454)Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nsel() and interp() can also be used on Dataset objects.\n\nds.sel(lat=slice(41,37), lon=slice(360-109,360-102))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (lat: 2, time: 2920, lon: 3)\nCoordinates:\n  * lat         (lat) float32 40.0 37.5\n  * lon         (lon) float32 252.5 255.0 257.5\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air         (time, lat, lon) float32 -10.05 -9.25 -8.75 ... -15.96 -14.46\n    air_kelvin  (time, lat, lon) float32 263.1 263.9 264.4 ... 259.4 257.2 258.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 2time: 2920lon: 3Coordinates: (3)lat(lat)float3240.0 37.5standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([40. , 37.5], dtype=float32)lon(lon)float32252.5 255.0 257.5standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([252.5, 255. , 257.5], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)air(time, lat, lon)float32-10.05 -9.25 ... -15.96 -14.46long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)air_kelvin(time, lat, lon)float32263.1 263.9 264.4 ... 257.2 258.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[263.1    , 263.9    , 264.4    ],\n        [266.9    , 266.6    , 266.79   ]],\n\n       [[255.     , 258.19998, 263.19998],\n        [259.5    , 262.1    , 265.9    ]],\n\n       [[252.7    , 254.5    , 259.79   ],\n        [253.79999, 256.19998, 261.9    ]],\n\n       ...,\n\n       [[248.68999, 244.89   , 247.39   ],\n        [256.19   , 249.09   , 249.09   ]],\n\n       [[248.79   , 246.98999, 249.68999],\n        [257.19   , 250.29   , 250.18999]],\n\n       [[255.59   , 257.79   , 259.49   ],\n        [259.38998, 257.19   , 258.69   ]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\n\nds.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (time: 2920)\nCoordinates:\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\n    lat         float64 39.73\n    lon         float64 255.0\nData variables:\n    air         (time) float64 -8.951 -14.5 -18.44 ... -27.79 -25.79 -15.42\n    air_kelvin  (time) float64 264.2 258.7 254.7 261.8 ... 245.4 247.4 257.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:time: 2920Coordinates: (3)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat()float6439.73standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray(39.72510679)lon()float64255.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray(255.01214454)Data variables: (2)air(time)float64-8.951 -14.5 ... -25.79 -15.42long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])air_kelvin(time)float64264.2 258.7 254.7 ... 247.4 257.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([264.19914312, 258.65246598, 254.71284227, ..., 245.36262886,\n       247.36447002, 257.73218487])Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\n\n\n\nAs a simple example, let’s try to calculate a mean field for the whole time range.\n\nds.mean(dim='time').hvplot()\n\n\n\n\n\n  \n\n\n\n\nWe can also calculate a zonal mean (averaging over longitude)\n\nds.mean(dim='lon').hvplot()\n\n\n\n\n\n  \n\n\n\n\nOther aggregation methods include min(), max(), std(), along with others.\n\nds.std(dim='time').hvplot()\n\n\n\n\n\n  \n\n\n\n\nThe data we have are in 6h timesteps. This can be resampled to daily or monthly. If you are familiar with pandas, xarray uses the same methods.\n\nds_mon = ds.resample(time='M').mean()\nds_mon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (time: 24, lat: 25, lon: 53)\nCoordinates:\n  * time        (time) datetime64[ns] 2013-01-31 2013-02-28 ... 2014-12-31\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\nData variables:\n    air         (time, lat, lon) float32 -28.68 -28.49 -28.48 ... 24.57 24.56\n    air_kelvin  (time, lat, lon) float32 244.5 244.7 244.7 ... 297.7 297.7 297.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:time: 24lat: 25lon: 53Coordinates: (3)time(time)datetime64[ns]2013-01-31 ... 2014-12-31array(['2013-01-31T00:00:00.000000000', '2013-02-28T00:00:00.000000000',\n       '2013-03-31T00:00:00.000000000', '2013-04-30T00:00:00.000000000',\n       '2013-05-31T00:00:00.000000000', '2013-06-30T00:00:00.000000000',\n       '2013-07-31T00:00:00.000000000', '2013-08-31T00:00:00.000000000',\n       '2013-09-30T00:00:00.000000000', '2013-10-31T00:00:00.000000000',\n       '2013-11-30T00:00:00.000000000', '2013-12-31T00:00:00.000000000',\n       '2014-01-31T00:00:00.000000000', '2014-02-28T00:00:00.000000000',\n       '2014-03-31T00:00:00.000000000', '2014-04-30T00:00:00.000000000',\n       '2014-05-31T00:00:00.000000000', '2014-06-30T00:00:00.000000000',\n       '2014-07-31T00:00:00.000000000', '2014-08-31T00:00:00.000000000',\n       '2014-09-30T00:00:00.000000000', '2014-10-31T00:00:00.000000000',\n       '2014-11-30T00:00:00.000000000', '2014-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)Data variables: (2)air(time, lat, lon)float32-28.68 -28.49 ... 24.57 24.56long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-28.68323  , -28.486452 , -28.479755 , ..., -30.658554 ,\n         -29.743628 , -28.474194 ],\n        [-26.076784 , -26.127504 , -26.4225   , ..., -32.5679   ,\n         -31.105167 , -28.442825 ],\n        [-22.770565 , -23.31516  , -24.042498 , ..., -31.165657 ,\n         -28.38291  , -24.144924 ],\n        ...,\n        [ 22.688152 ,  22.00097  ,  21.773153 , ...,  22.218397 ,\n          21.734531 ,  21.118395 ],\n        [ 23.31952  ,  23.16702  ,  22.698233 , ...,  22.43775  ,\n          22.190727 ,  21.715578 ],\n        [ 23.903486 ,  23.89203  ,  23.585333 , ...,  23.154608 ,\n          22.947426 ,  22.889124 ]],\n\n       [[-32.41607  , -32.44866  , -32.738483 , ..., -31.54482  ,\n         -30.430185 , -29.205448 ],\n        [-31.216885 , -31.08063  , -31.236965 , ..., -32.135708 ,\n         -30.825186 , -28.42241  ],\n        [-27.826433 , -28.123934 , -28.78045  , ..., -29.734114 ,\n         -27.383936 , -23.491434 ],\n...\n        [ 24.899088 ,  24.200085 ,  24.072004 , ...,  24.861843 ,\n          24.510258 ,  23.995668 ],\n        [ 25.815008 ,  25.661922 ,  25.121607 , ...,  24.954088 ,\n          25.071083 ,  24.735588 ],\n        [ 26.023424 ,  26.06767  ,  25.74576  , ...,  25.566338 ,\n          25.591848 ,  25.630259 ]],\n\n       [[-26.348473 , -26.260897 , -26.380894 , ..., -33.07903  ,\n         -32.067986 , -30.868315 ],\n        [-25.419994 , -24.849277 , -24.405483 , ..., -34.531376 ,\n         -32.82783  , -30.179682 ],\n        [-23.181051 , -23.56476  , -23.574757 , ..., -35.446938 ,\n         -31.91259  , -26.923311 ],\n        ...,\n        [ 23.299198 ,  22.541454 ,  22.60839  , ...,  23.378307 ,\n          23.067505 ,  22.662996 ],\n        [ 24.295895 ,  24.286139 ,  24.031782 , ...,  23.80259  ,\n          23.908312 ,  23.579037 ],\n        [ 24.897346 ,  25.076134 ,  24.909689 , ...,  24.547583 ,\n          24.573233 ,  24.560413 ]]], dtype=float32)air_kelvin(time, lat, lon)float32244.5 244.7 244.7 ... 297.7 297.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[244.4667 , 244.66354, 244.67027, ..., 242.49142, 243.40633,\n         244.67577],\n        [247.07323, 247.02248, 246.7275 , ..., 240.58205, 242.04489,\n         244.70726],\n        [250.37941, 249.83484, 249.10748, ..., 241.98434, 244.76712,\n         249.00505],\n        ...,\n        [295.83795, 295.15085, 294.9229 , ..., 295.36826, 294.88437,\n         294.26828],\n        [296.46942, 296.31686, 295.84802, ..., 295.5876 , 295.34058,\n         294.86536],\n        [297.05316, 297.0418 , 296.73517, ..., 296.30438, 296.09732,\n         296.0389 ]],\n\n       [[240.73384, 240.7013 , 240.4115 , ..., 241.60518, 242.71988,\n         243.94455],\n        [241.93309, 242.06935, 241.913  , ..., 241.01428, 242.32481,\n         244.72758],\n        [245.32361, 245.0261 , 244.36955, ..., 243.41588, 245.7661 ,\n         249.65858],\n...\n        [298.04895, 297.35007, 297.22195, ..., 298.01172, 297.66013,\n         297.14554],\n        [298.96484, 298.81186, 298.27136, ..., 298.10403, 298.22104,\n         297.88547],\n        [299.17334, 299.2175 , 298.89566, ..., 298.71625, 298.74167,\n         298.7802 ]],\n\n       [[246.80156, 246.88907, 246.76907, ..., 240.07089, 241.08206,\n         242.2817 ],\n        [247.72998, 248.30064, 248.74443, ..., 238.61859, 240.3222 ,\n         242.97026],\n        [249.96893, 249.58516, 249.57521, ..., 237.70308, 241.23743,\n         246.22667],\n        ...,\n        [296.4491 , 295.6914 , 295.75824, ..., 296.52817, 296.21747,\n         295.8128 ],\n        [297.44586, 297.43613, 297.1817 , ..., 296.95242, 297.05823,\n         296.72897],\n        [298.0472 , 298.22598, 298.0595 , ..., 297.6975 , 297.72318,\n         297.71024]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\nThis is a really short time series but as an example, let’s calculate a monthly climatology (at least for 2 months). For this we can use groupby()\n\nds_clim = ds_mon.groupby(ds_mon.time.dt.month).mean()\nds_clim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (lat: 25, month: 12, lon: 53)\nCoordinates:\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\n  * month       (month) int64 1 2 3 4 5 6 7 8 9 10 11 12\nData variables:\n    air         (month, lat, lon) float32 -26.8 -26.76 -26.94 ... 24.42 24.39\n    air_kelvin  (month, lat, lon) float32 246.3 246.4 246.2 ... 297.6 297.5\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25month: 12lon: 53Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)month(month)int641 2 3 4 5 6 7 8 9 10 11 12array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])Data variables: (2)air(month, lat, lon)float32-26.8 -26.76 -26.94 ... 24.42 24.39long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-26.800243 , -26.764072 , -26.935038 , ..., -30.089035 ,\n         -29.062258 , -27.503464 ],\n        [-24.292501 , -24.242697 , -24.439713 , ..., -31.621532 ,\n         -29.641533 , -26.395487 ],\n        [-21.572863 , -21.953503 , -22.43548  , ..., -29.751295 ,\n         -26.365608 , -21.584438 ],\n        ...,\n        [ 22.70041  ,  22.094238 ,  22.077103 , ...,  22.036419 ,\n          21.507267 ,  20.898678 ],\n        [ 23.394682 ,  23.320007 ,  23.010254 , ...,  22.206137 ,\n          21.931412 ,  21.380169 ],\n        [ 24.004492 ,  24.08844  ,  23.899166 , ...,  22.868156 ,\n          22.625813 ,  22.486786 ]],\n\n       [[-26.4729   , -26.744377 , -27.201698 , ..., -31.29174  ,\n         -30.148129 , -28.706345 ],\n        [-25.35005  , -25.390135 , -25.672592 , ..., -32.503124 ,\n         -30.883888 , -28.083572 ],\n        [-24.079243 , -24.57772  , -25.207546 , ..., -30.72148  ,\n         -27.816654 , -23.427551 ],\n...\n        [ 24.692593 ,  23.990627 ,  23.837757 , ...,  24.818966 ,\n          24.418842 ,  24.011253 ],\n        [ 25.437843 ,  25.270302 ,  24.819141 , ...,  25.014214 ,\n          25.044006 ,  24.75834  ],\n        [ 25.661469 ,  25.706635 ,  25.471094 , ...,  25.579634 ,\n          25.601887 ,  25.668968 ]],\n\n       [[-25.179115 , -25.12904  , -25.23718  , ..., -33.37831  ,\n         -32.12641  , -30.52194  ],\n        [-23.41661  , -22.98972  , -22.664356 , ..., -34.360527 ,\n         -32.18549  , -29.033997 ],\n        [-21.12061  , -21.618912 , -21.78383  , ..., -35.07468  ,\n         -31.237263 , -26.080326 ],\n        ...,\n        [ 23.61517  ,  22.826698 ,  22.739243 , ...,  23.306011 ,\n          22.94138  ,  22.507713 ],\n        [ 24.318195 ,  24.23037  ,  23.894321 , ...,  23.70565  ,\n          23.696701 ,  23.371418 ],\n        [ 24.730938 ,  24.83678  ,  24.625538 , ...,  24.450327 ,\n          24.415411 ,  24.387712 ]]], dtype=float32)air_kelvin(month, lat, lon)float32246.3 246.4 246.2 ... 297.6 297.5long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[246.3497 , 246.38591, 246.21494, ..., 243.06091, 244.08772,\n         245.64651],\n        [248.85742, 248.90721, 248.71027, ..., 241.52846, 243.50848,\n         246.75456],\n        [251.57706, 251.19641, 250.71442, ..., 243.39868, 246.78441,\n         251.56552],\n        ...,\n        [295.85022, 295.24414, 295.22687, ..., 295.18628, 294.6571 ,\n         294.04852],\n        [296.5446 , 296.46985, 296.16003, ..., 295.35596, 295.08124,\n         294.52997],\n        [297.15424, 297.23822, 297.049  , ..., 296.018  , 295.7757 ,\n         295.6366 ]],\n\n       [[246.67706, 246.4056 , 245.94827, ..., 241.85826, 243.00189,\n         244.44366],\n        [247.79994, 247.75981, 247.47739, ..., 240.64688, 242.2661 ,\n         245.06639],\n        [249.07072, 248.57225, 247.94246, ..., 242.42856, 245.33334,\n         249.72246],\n...\n        [297.8424 , 297.1405 , 296.9876 , ..., 297.9688 , 297.5687 ,\n         297.16113],\n        [298.58765, 298.4202 , 297.96893, ..., 298.1641 , 298.19388,\n         297.9082 ],\n        [298.81134, 298.85648, 298.62097, ..., 298.72946, 298.75168,\n         298.81885]],\n\n       [[247.97087, 248.02097, 247.91278, ..., 239.77167, 241.02362,\n         242.62805],\n        [249.73335, 250.1602 , 250.4856 , ..., 238.78946, 240.96454,\n         244.116  ],\n        [252.0294 , 251.53104, 251.36612, ..., 238.0753 , 241.91273,\n         247.06966],\n        ...,\n        [296.765  , 295.9766 , 295.88907, ..., 296.45587, 296.09125,\n         295.65753],\n        [297.4681 , 297.38025, 297.0442 , ..., 296.85553, 296.84656,\n         296.5213 ],\n        [297.88074, 297.9866 , 297.77533, ..., 297.60022, 297.5653 ,\n         297.53754]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\n\n\n\nFinally, let’s plot the results! This will plot the lat/lon axes of the original ds DataArray.\n\nds_clim.air.sel(month=10).hvplot()"
  },
  {
    "objectID": "tutorials/Earthdata_search.html",
    "href": "tutorials/Earthdata_search.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "Earthdata Search\nThis tutorial guides you through how to use Earthdata Search for NASA Earth observations search and discovery, and how to connect the search output (e.g. download or access links) to a programmatic workflow (locally or from within the cloud).\n\nStep 1. Go to Earthdata Search and Login\nGo to Earthdata Search https://search.earthdata.nasa.gov and use your Earthdata login credentials to log in. If you do not have an Earthdata account, please see the Workshop Prerequisites for guidance.\n\n\nStep 2. Search for dataset of interest\nUse the search box in the upper left to type key words. In this example we are interested in the ECOSTRESS LSTE which is managed by the LP DAAC and made available from the NASA Earthdata Cloud archive hosted in AWS cloud.\nType ECOSTRESS in the search bar Click on the “Available from AWS Cloud” filter option on the left.\n\n\n\nFigure caption: Search for ECOSTRESS data available in AWS cloud in Earthdata Search portal\n\n\nLet’s refine our search further. Let’s search for ECOSTRESS ECO_L2T_LSTE in the search box. A single Earthdata Seach Collection is returned.\nWe can click on the (i) icon for the dataset to read more details, including the dataset shortname (helpful for programmatic workflows) just below the dataset name; here ECO_L2T_LSTE.\n\n\n\nFigure caption: Refine search\n\n\n\n\nStep 3. Explore the dataset details, including Cloud Access information\nOnce we clicked the (i), scrolling down the info page for the dataset we will see Cloud Access information, such as:\n\nwhether the dataset is available in the cloud\n\nthe cloud Region (all NASA Earthdata Cloud data is/will be in us-west-2 region)\n\nthe S3 storage bucket and object prefix where this data is located\n\nlink that generates AWS S3 Credentials for in-cloud data access (we will cover this in the Direct Data Access Tutorials)\n\nlink to documentation describing the In-region Direct S3 Access to Buckets. Note: these will be unique depending on the DAAC where the data is archived. (We will show examples of direct in-region access in Tutorial 3.)\n\n\n\n\nFigure caption: Cloud access info in EDS\n\n\n\n\n\nFigure caption: Documentation describing the In-region Direct S3 Access to Buckets\n\n\nNote: Clicking on “For Developers” to exapnd will provide programmatic endpoints such as those for the CMR API, and more.\nFor now, let’s say we are intersted in getting download link(s) or access link(s) for specific data files (granules) within this collection.\nAt the top of the dataset info section, click on Search Results, which will take us back to the list of datasets matching our search parameters. Clicking on the dataset (ECOSTRESS ECO_L2T_LSTE) we now see a list of files (granules) that are part of the dataset (collection).\n\n\nStep 4a. Download or data access for a single granule\nTo download files for a granule click the download arrow on the card (or list row)\n\n\n\nFigure caption: Download granules\n\n\nYou can also get the S3 information (e.g., AWS region, bucket, temperary credentials for S3 access, and file names) by selecting the AWS S3 Access tab.\n\n\n\nFigure caption: S3 access for granules\n\n\n\nStep 4b. Download or data access for multiple granule\nTo download multiple granules, click on the green + symbol to add files to our project. Click on the green button towards the bottom that says “Download”. This will take us to another page with options to customize our download or access link(s).\n\n\n\nFigure caption: Select granules and click download\n\n\nOn the next page click the Direct Download option and click the green Download Data on the bottom left side of the page.\n\n\n\nFigure caption: Direct download multiple granules\n\n\nWe’re now taked to the final page for instructions to download and links for data access in the cloud. You should see three tabs: Download Files, AWS S3 Access, Download Script:\n\n\n\nFigure caption: Download to local\n\n\n\n\n\nFigure caption: Direct S3 access\n\n\nThe Download Files tab provides the https:// links for downloading the files locally\nThe AWS S3 Access tab provides the S3:// links, which is what we would use to access the data directly in-region (us-west-2) within the AWS cloud."
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Login_Token.html",
    "href": "how-tos/authentication/NASA_Earthdata_Login_Token.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "import requests\nimport netrc\nfrom datetime import datetime\nimport json\nimport os\n\n\n\n\ndef get_edl_creds():\n    nc = netrc.netrc()\n    remoteHostName = \"urs.earthdata.nasa.gov\"\n    edl_creds = nc.authenticators(remoteHostName)\n    return {'username':edl_creds[0], 'password':edl_creds[2]}\n\n\n\n\n\nedl_token_urls = {\n    'generate_token':'https://urs.earthdata.nasa.gov/api/users/token',\n    'list_token':'https://urs.earthdata.nasa.gov/api/users/tokens',\n    'revoke_token': 'https://urs.earthdata.nasa.gov/api/users/revoke_token'\n}\n\n\n\n\n\nif not os.path.isdir('../../../.hidden_dir'):\n    os.mkdir('../../../.hidden_dir')    \n\n\n\n\n\nif len(list_tokens := requests.get(edl_token_urls['list_token'], auth=(get_edl_creds()['username'], get_edl_creds()['password'])).json()) < 1:\n    #print('No tokens available. Generating new Earthdata Login Token ...')\n    generate_token_url = \"https://urs.earthdata.nasa.gov/api/users/token\"\n    generate_token_req = requests.post(edl_token_urls['generate_token'], auth=(get_edl_creds()['username'], get_edl_creds()['password']))\n    token = generate_token_req.json()\n    with open(\"../../../.hidden_dir/edl_token.json\", \"w\") as outfile:\n        json.dump(token, outfile)\n    print(f'Your EDL token information can be found here: {os.path.abspath(\"../../../.hidden_dir/edl_token.json\")}')\nelif datetime.strptime(list_tokens[0]['expiration_date'], \"%m/%d/%Y\") < datetime.now():\n    #print('Available token is expired. Generating a new Earthdata Login Token ...')\n    revoke_token = requests.post(f\"{edl_token_urls['revoke_token']}?token={list_tokens[0]}\", auth=(get_edl_creds()['username'], get_edl_creds()['password']))\n    generate_token_req = requests.post(edl_token_urls['generate_token'], auth=(get_edl_creds()['username'], get_edl_creds()['password']))\n    token = generate_token_req.json()\n    with open(\"../../../.hidden_dir/edl_token.json\", \"w\") as outfile:\n        json.dump(token, outfile)\n    print(f'Your EDL token information can be found here: {os.path.abspath(\"../../../.hidden_dir/edl_token.json\")}')\nelse:\n    #print('Earthdata Login Token Found ...')\n    with open(\"../../../.hidden_dir/edl_token.json\", \"w\") as outfile:\n        json.dump(list_tokens[0], outfile)\n    print(f'Your EDL token information can be found here: {os.path.abspath(\"../../../.hidden_dir/edl_token.json\")}')\n\nYour EDL token information can be found here: /home/jovyan/.hidden_dir/edl_token.json\n\n\n\n\n\n\n\nhttps://wiki.earthdata.nasa.gov/display/EL/How+to+Generate+a+User+Token\nhttps://urs.earthdata.nasa.gov/documentation/for_users/user_token"
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Authentication.html",
    "href": "how-tos/authentication/NASA_Earthdata_Authentication.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "This notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin <USERNAME>\npassword <PASSWORD>\n<USERNAME> and <PASSWORD> would be replaced by your actual Earthdata Login username and password respectively.\n\n\n\n\n\nfrom netrc import netrc\nfrom subprocess import Popen\nfrom platform import system\nfrom getpass import getpass\nimport os\n\nThe code below will:\n\ncheck what operating system (OS) is being used to determine which netrc file to check for/create (.netrc or _netrc)\ncheck if you have an netrc file, and if so, varify if those credentials are for the Earthdata endpoint\ncreate a netrc file if a netrc file is not present.\n\n\nurs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\nprompts = ['Enter NASA Earthdata Login Username: ',\n           'Enter NASA Earthdata Login Password: ']\n\n# Determine the OS (Windows machines usually use an '_netrc' file)\nnetrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n\n# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\ntry:\n    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n    netrc(netrcDir).authenticators(urs)[0]\n\n# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\nexcept FileNotFoundError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('touch {0}{2} | echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n    # Set restrictive permissions\n    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)\n\n    # Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\nexcept TypeError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n\n\n\nIf the file was created, we’ll see a .netrc file (_netrc for Window OS) in the list printed below. To view the contents from a Jupyter environment, click File on the top toolbar, select Open from Path…, type .netrc, and click Open. The .netrc file will open within the text editor.\n\n!!! Beware, your password will be visible if the .netrc file is opened in the text editor.\n\n\n!ls -al ~/\n\ntotal 128\ndrwxr-xr-x 25 jovyan jovyan  6144 Nov 11 18:59  .\ndrwxr-xr-x  1 root   root      20 Mar 11  2022  ..\n-rw-------  1 jovyan jovyan 12347 Nov 11 20:21  .bash_history\ndrwxr-xr-x  8 jovyan jovyan  6144 Nov  8 20:16  .cache\ndrwxrwsr-x  2 jovyan jovyan  6144 Apr 12  2022  .conda\ndrwxr-xr-x  4 jovyan jovyan  6144 Feb 28  2022  .config\ndrwx------  2 jovyan jovyan  6144 Nov 11 17:36  .git-credential-cache\n-rw-r--r--  1 jovyan jovyan    56 Nov  4 15:14  .gitconfig\ndrwxr-xr-x  3 jovyan jovyan  6144 Apr 13  2022  .hidden_dir\ndrwxr-xr-x  2 jovyan jovyan  6144 Nov 11 02:45  .ipynb_checkpoints\ndrwxr-xr-x  5 jovyan jovyan  6144 Jul 20  2021  .ipython\ndrwxr-xr-x  3 jovyan jovyan  6144 Jul 20  2021  .jupyter\n-rw-r--r--  1 jovyan jovyan     0 Jul 20  2021  .jupyter-server-log.txt\ndrwxr-xr-x  3 jovyan jovyan  6144 Jul 20  2021  .local\n-rw-------  1 jovyan jovyan    73 Nov 11 18:24  .netrc\ndrwx------  2 jovyan jovyan  6144 Aug  4 15:42  .ssh\ndrwxr-xr-x 12 jovyan jovyan  6144 Nov 11 05:57  2022-ECOSTRESS-Cloud-Workshop\ndrwxr-xr-x 13 jovyan jovyan  6144 Nov 14 05:11  2022-Fall-ECOSTRESS-Cloud-Workshop\ndrwxr-xr-x 15 jovyan jovyan  6144 Nov  9 19:18  2022-Fall-ECOSTRESS-Cloud-Workshop_MJ\ndrwxr-xr-x 13 jovyan jovyan  6144 Nov 11 16:43  2022-Fall-ECOSTRESS-Cloud-Workshop_mmm\ndrwxr-xr-x  5 jovyan jovyan  6144 Nov 10 14:37 'Untitled Folder'\ndrwxr-xr-x  5 jovyan jovyan  6144 Aug 29 15:17  appeears-cloud-optimized-format-prototype\n-rw-r--r--  1 jovyan jovyan   131 Nov 11 18:59  cookies.txt\ndrwxr-xr-x 15 jovyan jovyan  6144 Mar 10  2022  earthdata-cloud-cookbook\ndrwxr-xr-x  6 jovyan jovyan  6144 Apr 28  2022  lpdaac_cloud_data_access\ndrwxr-xr-x  6 jovyan jovyan  6144 Oct 20  2021  lpdaac_cloud_data_access1\ndrwxr-xr-x  4 jovyan jovyan  6144 Jul 20  2021  lpdaac_hls_tutorial\ndrwxr-xr-x  4 jovyan jovyan  6144 Sep 19 21:29  mentors-2022\ndrwxr-xr-x 27 jovyan jovyan  6144 Oct 28 01:20  shared\ndrwxr-xr-x 27 jovyan jovyan  6144 Oct 28 01:20  shared-readwrite"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "In this notebook, we will access data for the Harmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002) data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each spectral band.\nWe will access a single COG file, L30 red band (0.64 – 0.67 μm), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data.\n\n\n\n\n\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata.\n\n\n\n\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to perform in-region direct access of HLS Cloud Optimized geoTIFF (COG) files in S3\nhow to plot the data\n\n\n\n\n\nimport os\nimport requests \nimport boto3\nfrom osgeo import gdal\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv\n\n\n\n\n\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('lpdaac')\n#temp_creds_req\n\n\n\n\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nGDAL environment variables must be configured to access COGs in Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\nIn this example we’re interested in the HLS L30 data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\ns3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'\n\n\n\n\nRead in the HLS s3 URL for the L30 red band (0.64 – 0.67 μm) into our workspace using rioxarray, an extension of xarray used to read geospatial data.\n\nda = rioxarray.open_rasterio(s3_url)\nda\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda_red = da.squeeze('band', drop=True)\nda_red\n\nPlot the dataarray, representing the L30 red band, using hvplot.\n\nda_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')\n\nExit the context manager.\n\nrio_env.__exit__()\n\n\n\n\n\nDirect S3 Data Access with rioxarray\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nGetting Started with Cloud-Native Harmonized Landsat Sentinel-2 (HLS) Data in R"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "In this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access a single netCDF file from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataset. This approach leverages S3 native protocols for efficient access to the data.\n\n\n\n\n\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata.\n\n\n\n\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to perform in-region direct access of ECCO_L4_SSH_05DEG_MONTHLY_V4R4 data in S3\nhow to plot the data\n\n\n\n\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport os\nimport requests\nimport s3fs\nfrom osgeo import gdal\nimport xarray as xr\nimport hvplot.xarray\nimport holoviews as hv\n\n\n\n\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('podaac')\n#temp_creds_req\n\n\n\n\ns3fs sessions are used for authenticated access to s3 bucket and allows for typical file-system style operations. Below we create session by passing in the temporary credentials we recieved from our temporary credentials endpoint.\n\nfs_s3 = s3fs.S3FileSystem(anon=False, \n                          key=temp_creds_req['accessKeyId'], \n                          secret=temp_creds_req['secretAccessKey'], \n                          token=temp_creds_req['sessionToken'])\n\nIn this example we’re interested in the ECCO data collection from NASA’s PO.DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\ns3_url = 's3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-01_ECCO_V4r4_latlon_0p50deg.nc'\n\n\n\n\nOpen with the netCDF file using the s3fs package, then load the cloud asset into an xarray dataset.\n\ns3_file_obj = fs_s3.open(s3_url, mode='rb')\n\n\nssh_ds = xr.open_dataset(s3_file_obj, engine='h5netcdf')\nssh_ds\n\nGet the SSH variable as an xarray dataarray\n\nssh_da = ssh_ds.SSH\nssh_da\n\nPlot the SSH dataarray for time 2015-01-16T12:00:00 using hvplot.\n\nssh_da.hvplot.image(x='longitude', y='latitude', cmap='Spectral_r', aspect='equal').opts(clim=(ssh_da.attrs['valid_min'][0],ssh_da.attrs['valid_max'][0]))\n\n\n\n\n\nDirect access to ECCO data in S3 (from us-west-2)\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links"
  },
  {
    "objectID": "how-tos/additional_resources/Direct_Access_netCDF_simple.html",
    "href": "how-tos/additional_resources/Direct_Access_netCDF_simple.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "Direct Access - ECCO netCDF example\n\nGetting Started\nIn this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF datasets into a single xarray dataset. This approach leverages S3 native protocols for efficient access to the data.\n\n\nRequirements\n\nAWS\nThis notebook should be running in an EC2 instance in AWS region us-west-2, as previously mentioned. We recommend using an EC2 with at least 8GB of memory available.\nThe notebook was developed and tested using a t2.small instance (_ CPUs; 8GB memory). Python 3\nMost of these imports are from the Python standard library. However, you will need to install these packages into your Python 3 environment if you have not already done so:\n\ns3fs\nrequests\npandas\nxarray\nmatplotlib\ncartopy\n\n\n\n\nLearning Objectives\n\nimport needed libraries\ndefine dataset of interest\nauthenticate for NASA Earthdata archive (Earthdata Login)\nobtain AWS credentials for Earthdata DAAC archive in AWS S3\naccess DAAC data directly from the in-region S3 bucket without moving or downloading any files to your local (cloud) workspace\nplot the first time step in the data\n\n\nimport os\nimport subprocess\nfrom os.path import dirname, join\n\n# Access EDS\nimport requests\n\n# Access AWS S3\nimport boto3\nimport s3fs\n\n# Read and work with datasets\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport cartopy\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeat\n\n\nDefine dataset of interest\nIn this case it’s the following string that unique identifies the collection of monthly, 0.5-degree sea surface height data.\n\nShortName = \"ECCO_L4_SSH_05DEG_MONTHLY_V4R4\"\n\n\n\n\nEarthdata login\nYou should have a .netrc file set up like:\nmachine urs.earthdata.nasa.gov login <username> password <password>\nSee the following (Authentication for NASA Earthdata tutorial)[https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/04_NASA_Earthdata_Authentication.html]\n\n\nAWS credentials to Access Data from S3\nPass credentials and configuration to AWS so we can interact with S3 objects from applicable buckets. For now, each DAAC has different AWS credentials endpoints. LP DAAC and PO.DAAC are listed here:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\n}\n\nIn this example we’re interested in the ECCO data collection from PO.DAAC in Earthdata Cloud in AWS S3, so we specify the podaac endpoint in the next code block.\nSet up an s3fs session for authneticated access to ECCO netCDF files in s3:\n\ndef begin_s3_direct_access(url: str=s3_cred_endpoint['podaac']):\n    response = requests.get(url).json()\n    return s3fs.S3FileSystem(key=response['accessKeyId'],\n                             secret=response['secretAccessKey'],\n                             token=response['sessionToken'],\n                             client_kwargs={'region_name':'us-west-2'})\n\nfs = begin_s3_direct_access()\n\nGet a list of netCDF files located at the S3 path corresponding to the ECCO V4r4 monthly sea surface height dataset on the 0.5-degree latitude/longitude grid, for year 2015.\n\nssh_Files = fs.glob(join(\"podaac-ops-cumulus-protected/\", ShortName, \"*2015*.nc\"))\n\nlen(ssh_Files)\n\n12\n\n\n\nAccess in-region S3 cloud data without moving files\nNow that we have authenticated in AWS, this next code block accesses data directly from the NASA Earthdata archive in an S3 bucket in us-west-2 region, without downloading or moving any files into your user cloud workspace (instnace).\nOpen with the netCDF files using the s3fs package, then load them all at once into a concatenated xarray dataset.\n\nssh_Dataset = xr.open_mfdataset(\n    paths=[fs.open(f) for f in ssh_Files],\n    combine='by_coords',\n    mask_and_scale=True,\n    decode_cf=True,\n    chunks={'latitude': 60,   # These were chosen arbitrarily. You must specify \n            'longitude': 120, # chunking that is suitable to the data and target\n            'time': 100}      # analysis.\n)\n\nssh = ssh_Dataset.SSH\n\nprint(ssh)\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              [-1.88057721]\n    valid_max:              [1.42077196]\n\n\n\n\n\nPlot the gridded sea surface height time series\nBut only the timesteps beginning in 2015:\n\nssh_after_201x = ssh[ssh['time.year']>=2015,:,:]\n\nprint(ssh_after_201x)\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              [-1.88057721]\n    valid_max:              [1.42077196]\n\n\nPlot the grid for the first time step using a Robinson projection. Define a helper function for consistency throughout the notebook:\n\ndef make_figure(proj):\n    fig = plt.figure(figsize=(16,6))\n    ax = fig.add_subplot(1, 1, 1, projection=proj)\n    ax.add_feature(cfeat.LAND)\n    ax.add_feature(cfeat.OCEAN)\n    ax.add_feature(cfeat.COASTLINE)\n    ax.add_feature(cfeat.BORDERS, linestyle='dotted')\n    return fig, ax\n\nfig, ax = make_figure(proj=ccrs.Robinson())\n\nssh_after_201x.isel(time=0).plot(ax=ax, transform=ccrs.PlateCarree(), cmap='Spectral_r')\n\n<cartopy.mpl.geocollection.GeoQuadMesh at 0x7fd040602b20>\n\n\n\n\n\n\n\nAdditional Resources\n\nFull example with additional plots and use cases here: https://github.com/podaac/ECCO/blob/main/Data_Access/cloud_direct_access_s3.ipynb"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Open-Science-Tutorial.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "Each of the following co-authors contributed to the following materials and code examples, as well as collaboration infrastructure for the NASA Earthdata Openscapes Project: * Julia S. Stewart Lowndes; Openscapes, NCEAS * Erin Robinson; Openscapes, Metadata Game Changers * Catalina M Oaida; NASA PO.DAAC, NASA Jet Propulsion Laboratory * Luis Alberto Lopez; NASA National Snow and Ice Data Center DAAC * Aaron Friesz; NASA Land Processes DAAC * Andrew P Barrett; NASA National Snow and Ice Data Center DAAC * Makhan Virdi; NASA ASDC DAAC * Jack McNelis; NASA PO.DAAC, NASA Jet Propulsion Laboratory\nAdditional credit to the entire NASA Earthdata Openscapes Project community, Patrick Quinn at Element84, and to2i2c for our Cloud computing infrastructure\n\n\n\n\nIntroduction to NASA Earthdata’s move to the cloud\n\nBackground and motivation\nEnabling Open Science via “Analysis-in-Place”\nResources for cloud adopters: NASA Earthdata Openscapes\n\nNASA Earthdata discovery and access in the cloud\n\nPart 1: Explore Earthdata cloud data availablity\nPart 2: Working with Cloud-Optimized GeoTIFFs using NASA’s Common Metadata Repository Spatio-Temporal Assett Catalog (CMR-STAC)\nPart 3: Working with Zarr-formatted data using NASA’s Harmony cloud transformation service\n\n\n\n\n\nThis notebook source code: update https://github.com/NASA-Openscapes/2021-Cloud-Workshop-AGU/tree/main/how-tos\nAlso available via online Quarto book: update https://nasa-openscapes.github.io/2021-Cloud-Workshop-AGU/\n\n\n\n\n\n\n\n\nEOSDIS Data Archive\n\n\n\n\n\n \n\nNASA Distributed Active Archive Centers (DAACs) are continuing to migrate data to the Earthdata Cloud\n\nSupporting increased data volume as new, high-resolution remote sensing missions launch in the coming years\nData hosted via Amazon Web Services, or AWS\nDAACs continuing to support tools, services, and tutorial resources for our user communities\n\n\n\n\n\n\nReducing barriers to large-scale scientific research in the era of “big data”\nIncreasing community contributions with hands-on engagement\nPromoting reproducible and shareable workflows without relying on local storage systems\n\n\n\n\nOpen Data\n\n\n\n\n\n\n\n\nEarthdata Cloud Paradigm\n\n\n\n\n\nShow slide with 3 panels of user resources\nEmphasize that the following tutorials are short examples that were taken from the tutorial resources we have been building for our users\n\n\n\nThe following tutorial demonstrates several basic end-to-end workflows to interact with data “in-place” from the NASA Earthdata Cloud, accessing Amazon Web Services (AWS) Single Storage Solution (S3) data locations without the need to download data. While the data can be downloaded locally, the cloud offers the ability to scale compute resources to perform analyses over large areas and time spans, which is critical as data volumes continue to grow.\nAlthough the examples we’re working with in this notebook only focuses on a small time and area for demonstration purposes, this workflow can be modified and scaled up to suit a larger time range and region of interest.\n\n\n\nHarmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002)\n\nSurface reflectance (SR) and top of atmosphere (TOA) brightness data\nGlobal observations of the land every 2–3 days at 30-meter (m)\nCloud Optimized GeoTIFF (COG) format\n\nECCO Sea Surface Height - Daily Mean 0.5 Degree (Version 4 Release 4)(10.5067/ECG5D-SSH44).\n\nDaily-averaged dynamic sea surface height\nTime series of monthly NetCDFs on a 0.5-degree latitude/longitude grid.\n\n\n\n\n\n\n\n\nFrom Earthdata Search https://search.earthdata.nasa.gov, use your Earthdata login credentials to log in. You can create an Earthdata Login account at https://urs.earthdata.nasa.gov.\nIn this example we are interested in the ECCO dataset, hosted by the PO.DAAC. This dataset is available from the NASA Earthdata Cloud archive hosted in AWS cloud.\nClick on the “Available from AWS Cloud” filter option on the left. Here, 39 matching collections were found with the ECCO monthly SSH search, and for the time period for year 2015. The latter can be done using the calendar icon on the left under the search box. Scroll down the list of returned matches until we see the dataset of interest, in this case ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4).\n\n\n\nClicking on the ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4) dataset, we now see a list of files (granules) that are part of the dataset (collection). We can click on the green + symbol to add a few files to our project. Here we added the first 3 listed for 2015. Then click on the green button towards the bottom that says “Download”. This will take us to another page with options to customize our download or access link(s).\n\n\n\nFigure caption: Select granules and click download\n\n\n\n\n\n\nSelect the “Direct Download” option to view Access options via Direct Download and from the AWS Cloud. Additional options to customize the data are also available for this dataset.\n\n\n\nFigure caption: Customize your download or access\n\n\n\n\n\nClicking the green Download Data button again, will take us to the final page for instructions to download and links for data access in the cloud. The AWS S3 Access tab provides the S3:// links, which is what we would use to access the data directly in-region (us-west-2) within the AWS cloud.\nE.g.: s3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-09_ECCO_V4r4_latlon_0p50deg.nc where s3 indicates data is stored in AWS S3 storage, podaac-ops-cumulus-protected is the bucket, and ECCO_L4_SSH_05DEG_MONTHLY_V4R4 is the object prefix (the latter two are also listed in the dataset collection information under Cloud Access (step 3 above)).\n\n\nIn the next two examples we will work programmatically in the cloud to access datasets of interest, to get us set up for further scientific analysis of choice. There are several ways to do this. One way to connect the search part of the workflow we just did in Earthdata Search to our next steps working in the cloud is to simply copy/paste the s3:// links provides in Step 4 above into a JupyterHub notebook or script in our cloud workspace, and continue the data analysis from there.\nOne could also copy/paste the s3:// links and save them in a text file, then open and read the text file in the notebook or script in the JupyterHub in the cloud.\n\n\n\nFigure caption: Direct S3 access\n\n\n\n\n\n\nIn this example we will access the NASA’s Harmonized Landsat Sentinel-2 (HLS) version 2 assets, which are archived in cloud optimized geoTIFF (COG) format archived by the Land Processes (LP) DAAC. The COGs can be used like any other GeoTIFF file, but have some added features that make them more efficient within the cloud data access paradigm. These features include: overviews and internal tiling.\n\n\nSpatioTemporal Asset Catalog (STAC) is a specification that provides a common language for interpreting geospatial information in order to standardize indexing and discovering data.\nThe STAC specification is made up of a collection of related, yet independent specifications that when used together provide search and discovery capabilities for remote assets.\n\n\nSTAC Catalog (aka DAAC Archive)\nSTAC Collection (aka Data Product)\nSTAC Item (aka Granule)\nSTAC API\n\n\n\n\nThe CMR-STAC API is NASA’s implementation of the STAC API specification for all NASA data holdings within EOSDIS. The current implementation does not allow for querries accross the entire NASA catalog. Users must execute searches within provider catalogs (e.g., LPCLOUD) to find the STAC Items they are searching for. All the providers can be found at the CMR-STAC endpoint here: https://cmr.earthdata.nasa.gov/stac/.\nIn this example, we will query the LPCLOUD provider to identify STAC Items from the Harmonized Landsat Sentinel-2 (HLS) collection that fall within our region of interest (ROI) and within our specified time range.\n\n\n\n\n\nimport os\nimport requests \nimport boto3\nfrom osgeo import gdal\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv\n\nfrom pystac_client import Client  \nfrom collections import defaultdict    \nimport json\nimport geopandas\nimport geoviews as gv\nfrom cartopy import crs\ngv.extension('bokeh', 'matplotlib')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n  \n  \n\n\n\n\n\n\n\n\n\nSTAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n\n\nprovider_cat = Client.open(STAC_URL)\n\n\n\nFor this next step we need the provider title (e.g., LPCLOUD). We will add the provider to the end of the CMR-STAC API URL (i.e., https://cmr.earthdata.nasa.gov/stac/) to connect to the LPCLOUD STAC Catalog.\n\ncatalog = Client.open(f'{STAC_URL}/LPCLOUD/')\n\nSince we are using a dedicated client (i.e., pystac-client.Client) to connect to our STAC Provider Catalog, we will have access to some useful internal methods and functions (e.g., get_children() or get_all_items()) we can use to get information from these objects.\n\n\n\n\nWe will define our ROI using a geojson file containing a small polygon feature in western Nebraska, USA. We’ll also specify the data collections and a time range for our example.\n\n\nReading in a geojson file with geopandas and extract coodinates for our ROI.\n\nfield = geopandas.read_file('../data/ne_w_agfields.geojson')\nfieldShape = field['geometry'][0]\nroi = json.loads(field.to_json())['features'][0]['geometry']\n\nWe can plot the polygon using the geoviews package that we imported as gv with ‘bokeh’ and ‘matplotlib’ extensions. The following has reasonable width, height, color, and line widths to view our polygon when it is overlayed on a base tile map.\n\nbase = gv.tile_sources.EsriImagery.opts(width=650, height=500)\nfarmField = gv.Polygons(fieldShape).opts(line_color='yellow', line_width=10, color=None)\nbase * farmField\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe will now start to specify the search criteria we are interested in, i.e, the date range, the ROI, and the data collections, that we will pass to the STAC API.\n\n\n\n\nNow we can put all our search criteria together using catalog.search from the pystac_client package. STAC Collection is synonomous with what we usually consider a NASA data product. Desired STAC Collections are submitted to the search API as a list containing the collection id. Let’s focus on S30 and L30 collections.\n\ncollections = ['HLSL30.v2.0', 'HLSS30.v2.0']\n\ndate_range = \"2021-05/2021-08\"\n\nsearch = catalog.search(\n    collections=collections,\n    intersects=roi,\n    datetime=date_range,\n    limit=100\n)\n\n\n\n\nprint('Matching STAC Items:', search.matched())\nitem_collection = search.get_all_items()\nitem_collection[0].to_dict()\n\nMatching STAC Items: 113\n\n\n{'type': 'Feature',\n 'stac_version': '1.0.0',\n 'id': 'HLS.L30.T13TGF.2021124T173013.v2.0',\n 'properties': {'datetime': '2021-05-04T17:30:13.428000Z',\n  'start_datetime': '2021-05-04T17:30:13.428Z',\n  'end_datetime': '2021-05-04T17:30:37.319Z',\n  'eo:cloud_cover': 36},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-101.5423534, 40.5109845],\n    [-101.3056118, 41.2066375],\n    [-101.2894253, 41.4919436],\n    [-102.6032964, 41.5268623],\n    [-102.638891, 40.5386175],\n    [-101.5423534, 40.5109845]]]},\n 'links': [{'rel': 'self',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0/items/HLS.L30.T13TGF.2021124T173013.v2.0'},\n  {'rel': 'parent',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0'},\n  {'rel': 'collection',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v2.0'},\n  {'rel': <RelType.ROOT: 'root'>,\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/',\n   'type': <MediaType.JSON: 'application/json'>,\n   'title': 'LPCLOUD'},\n  {'rel': 'provider', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.json'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.umm_json'}],\n 'assets': {'B11': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B11.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B11.tif'},\n  'B07': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B07.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B07.tif'},\n  'SAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.SAA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.SAA.tif'},\n  'B06': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B06.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B06.tif'},\n  'B09': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B09.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B09.tif'},\n  'B10': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B10.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B10.tif'},\n  'VZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.VZA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.VZA.tif'},\n  'SZA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.SZA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.SZA.tif'},\n  'B01': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B01.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B01.tif'},\n  'VAA': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.VAA.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.VAA.tif'},\n  'B05': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B05.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B05.tif'},\n  'B02': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B02.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B02.tif'},\n  'Fmask': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.Fmask.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.Fmask.tif'},\n  'B03': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B03.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B03.tif'},\n  'B04': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.B04.tif',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.B04.tif'},\n  'browse': {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/HLSL30.020/HLS.L30.T13TGF.2021124T173013.v2.0/HLS.L30.T13TGF.2021124T173013.v2.0.jpg',\n   'type': 'image/jpeg',\n   'title': 'Download HLS.L30.T13TGF.2021124T173013.v2.0.jpg'},\n  'metadata': {'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G2144020713-LPCLOUD.xml',\n   'type': 'application/xml'}},\n 'bbox': [-102.638891, 40.510984, -101.289425, 41.526862],\n 'stac_extensions': ['https://stac-extensions.github.io/eo/v1.0.0/schema.json'],\n 'collection': 'HLSL30.v2.0'}\n\n\n\n\n\n\nBelow we will loop through and filter the item_collection by a specified cloud cover as well as extract the band we’d need to do an Enhanced Vegetation Index (EVI) calculation for a future analysis. We will also specify the STAC Assets (i.e., bands/layers) of interest for both the S30 and L30 collections (also in our collections variable above) and print out the first ten links, converted to s3 locations:\n\ncloudcover = 25\n\ns30_bands = ['B8A', 'B04', 'B02', 'Fmask']    # S30 bands for EVI calculation and quality filtering -> NIR, RED, BLUE, Quality \nl30_bands = ['B05', 'B04', 'B02', 'Fmask']    # L30 bands for EVI calculation and quality filtering -> NIR, RED, BLUE, Quality \n\nevi_band_links = []\n\nfor i in item_collection:\n    if i.properties['eo:cloud_cover'] <= cloudcover:\n        if i.collection_id == 'HLSS30.v2.0':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = s30_bands\n        elif i.collection_id == 'HLSL30.v2.0':\n            #print(i.properties['eo:cloud_cover'])\n            evi_bands = l30_bands\n\n        for a in i.assets:\n            if any(b==a for b in evi_bands):\n                evi_band_links.append(i.assets[a].href)\n                \ns3_links = [l.replace('https://data.lpdaac.earthdatacloud.nasa.gov/', 's3://') for l in evi_band_links]\ns3_links[:10]\n\n['s3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B05.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.Fmask.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B02.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B02.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.B05.tif',\n 's3://lp-prod-protected/HLSL30.020/HLS.L30.T14TKL.2021133T172406.v2.0/HLS.L30.T14TKL.2021133T172406.v2.0.Fmask.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T14TKL.2021133T173859.v2.0/HLS.S30.T14TKL.2021133T173859.v2.0.B04.tif',\n 's3://lp-prod-protected/HLSS30.020/HLS.S30.T14TKL.2021133T173859.v2.0/HLS.S30.T14TKL.2021133T173859.v2.0.B8A.tif']\n\n\n\n\n\nAccess s3 credentials from LP.DAAC and create a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\ns3_cred_endpoint = 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials'\ntemp_creds_req = requests.get(s3_cred_endpoint).json()\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nGDAL Configurations\nGDAL is a foundational piece of geospatial software that is leveraged by several popular open-source, and closed, geospatial software. The rasterio package is no exception. Rasterio leverages GDAL to, among other things, read and write raster data files, e.g., GeoTIFFs/Cloud Optimized GeoTIFFs. To read remote files, i.e., files/objects stored in the cloud, GDAL uses its Virtual File System API. In a perfect world, one would be able to point a Virtual File System (there are several) at a remote data asset and have the asset retrieved, but that is not always the case. GDAL has a host of configurations/environmental variables that adjust its behavior to, for example, make a request more performant or to pass AWS credentials to the distribution system. Below, we’ll identify the evironmental variables that will help us get our data from cloud\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\n<rasterio.env.Env at 0x7f64510812e0>\n\n\n\ns3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'\n# s3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T13TGF.2021133T172406.v2.0/HLS.L30.T13TGF.2021133T172406.v2.0.B04.tif'\n\n\n\n\n\nda = rioxarray.open_rasterio(s3_url)\nda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 3660, x: 3660)>\n[13395600 values with dtype=int16]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.1e+06 4.1e+06 4.1e+06 ... 3.99e+06 3.99e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayband: 1y: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (4)band(band)int641array([1])x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.1e+06 4.1e+06 ... 3.99e+06array([4100025., 4099995., 4099965., ..., 3990315., 3990285., 3990255.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 11, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4100040.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\nWhen GeoTIFFS/Cloud Optimized GeoTIFFS are read in, a band coordinate variable is automatically created (see the print out above). In this exercise we will not use that coordinate variable, so we will remove it using the squeeze() function to avoid confusion.\n\nda_red = da.squeeze('band', drop=True)\nda_red\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 3660, x: 3660)>\n[13395600 values with dtype=int16]\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.1e+06 4.1e+06 4.1e+06 ... 3.99e+06 3.99e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayy: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (3)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.1e+06 4.1e+06 ... 3.99e+06array([4100025., 4099995., 4099965., ..., 3990315., 3990285., 3990255.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 11, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4100040.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\n\n\n\n\nda_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nrio_env.__exit__()\n\n\n\n\nWe have already explored direct access to the NASA EOSDIS archive in the cloud via the Amazon Simple Storage Service (S3). In addition to directly accessing the files archived and distributed by each of the NASA DAACs, many datasets also support services that allow us to customize the data via subsetting, reformatting, reprojection, and other transformations.\nThis example demonstrates “analysis in place” using customized ECCO Level 4 monthly sea surface height data, in this case reformatted to Zarr, from a new ecosystem of services operating within the NASA Earthdata Cloud: NASA Harmony:\n\nConsistent access patterns to EOSDIS holdings make cross-data center data access easier\nData reduction services allow us to request only the data we want, in the format and projection we want\nAnalysis Ready Data and cloud access will help reduce time-to-science\nCommunity Development helps reduce the barriers for re-use of code and sharing of domain knowledge\n\n\n\n\n\nfrom harmony import BBox, Client, Collection, Request, LinkType\nfrom harmony.config import Environment\nfrom pprint import pprint\nimport datetime as dt\nimport s3fs\nfrom pqdm.threads import pqdm\nimport xarray as xr\n\n\n\n\nHarmony-Py provides a pip installable Python alternative to directly using Harmony’s RESTful API to make it easier to request data and service options, especially when interacting within a Python Jupyter Notebook environment.\n\n\nFirst, we need to create a Harmony Client, which is what we will interact with to submit and inspect a data request to Harmony, as well as to retrieve results.\n\nharmony_client = Client()\n\n\n\n\n\nSpecify a temporal range over 2015, and Zarr as an output format.\nZarr is an open source library for storing N-dimensional array data. It supports multidimensional arrays with attributes and dimensions similar to NetCDF4, and it can be read by XArray. Zarr is often used for data held in cloud object storage (like Amazon S3), because it is better optimized for these situations than NetCDF4.\n\nshort_name = 'ECCO_L4_SSH_05DEG_MONTHLY_V4R4'\n\nrequest = Request(\n    collection=Collection(id=short_name),\n    temporal={\n        'start': dt.datetime(2015, 1, 2),\n        'stop': dt.datetime(2015, 12, 31),\n    },\n    format='application/x-zarr'\n)\n\njob_id = harmony_client.submit(request)\n\n\n\n\nHarmony data outputs can be accessed within the cloud using the s3 URLs and AWS credentials provided in the Harmony job response:\n\nharmony_client.wait_for_processing(job_id, show_progress=True)\n\nresults = harmony_client.result_urls(job_id, link_type=LinkType.s3)\ns3_urls = list(results)\ns3_urls\n\n [ Processing:  83% ] |##########################################         | [/]\n\n\n\n\nUsing aws_credentials you can retrieve the credentials needed to access the Harmony s3 staging bucket and its contents.\n\ncreds = harmony_client.aws_credentials()\n\n\n\n\n\nAccess AWS credentials for the Harmony bucket, and use the AWS s3fs package to create a file system that can then be read by xarray. Below we create session by passing in the temporary credentials we recieved from our temporary credentials endpoint.\n\ncreds = harmony_client.aws_credentials()\n\ns3_fs = s3fs.S3FileSystem(\n    key=creds['aws_access_key_id'],\n    secret=creds['aws_secret_access_key'],\n    token=creds['aws_session_token'],\n    client_kwargs={'region_name':'us-west-2'},\n)\n\nOpen the Zarr stores using the s3fs package, then load them all at once into a concatenated xarray dataset:\n\nstores = [s3fs.S3Map(root=url, s3=s3_fs, check=False) for url in s3_urls]\ndef open_zarr_xarray(store):\n    return xr.open_zarr(store=store, consolidated=True)\n\ndatasets = pqdm(stores, open_zarr_xarray, n_jobs=12)\n\nds = xr.concat(datasets, 'time', coords='minimal', )\nds = xr.decode_cf(ds, mask_and_scale=True, decode_coords=True)\nds\n\n\nssh_da = ds.SSH\n\nssh_da.to_masked_array(copy=False)\n\nssh_da\n\n\n\n\nNow we can start looking at aggregations across the time dimension. In this case, plot the standard deviation of the temperature at each point to get a visual sense of how much temperatures fluctuate over the course of the month.\n\nssh_da = ds.SSH\n\nstdev_ssh = ssh_da.std('time')\nstdev_ssh.name = 'stdev of analysed_sst [Kelvin]'\nstdev_ssh.plot();\n\nssh_da.hvplot.image(x='longitude', y='latitude', cmap='Spectral_r', aspect='equal').opts(clim=(ssh_da.attrs['valid_min'],ssh_da.attrs['valid_max']))\n\n\n\n\n\nReference Hackathon/workshop tutorials that go into more detail!\nEarthdata Cloud Cookbook\nEarthdata Cloud Primer\n\nGetting started with Amazon Web Services outside of the Workshop to access and work with data with a cloud environment."
  },
  {
    "objectID": "how-tos/additional_resources/Multi-File_Direct_S3_Access_NetCDF_Example.html",
    "href": "how-tos/additional_resources/Multi-File_Direct_S3_Access_NetCDF_Example.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "In this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF datasets into an xarray dataset. This approach leverages S3 native protocols for efficient access to the data.\n\n\n\n\n\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata.\n\n\n\n\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to define a dataset of interest and find netCDF files in S3 bucket\nhow to perform in-region direct access of ECCO_L4_SSH_05DEG_MONTHLY_V4R4 data in S3\nhow to plot the data\n\n\n\n\n\n\nimport os\nimport requests\nimport s3fs\nimport xarray as xr\nimport hvplot.xarray\n\n\n\n\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('podaac')\n#temp_creds_req\n\n\n\n\ns3fs sessions are used for authenticated access to s3 bucket and allows for typical file-system style operations. Below we create session by passing in the temporary credentials we recieved from our temporary credentials endpoint.\n\nfs_s3 = s3fs.S3FileSystem(anon=False, \n                          key=temp_creds_req['accessKeyId'], \n                          secret=temp_creds_req['secretAccessKey'], \n                          token=temp_creds_req['sessionToken'],\n                          client_kwargs={'region_name':'us-west-2'})\n\nIn this example we’re interested in the ECCO data collection from NASA’s PO.DAAC in Earthdata Cloud. In this case it’s the following string that unique identifies the collection of monthly, 0.5-degree sea surface height data (ECCO_L4_SSH_05DEG_MONTHLY_V4R4).\n\nshort_name = 'ECCO_L4_SSH_05DEG_MONTHLY_V4R4'\n\n\nbucket = os.path.join('podaac-ops-cumulus-protected/', short_name, '*2015*.nc')\nbucket\n\nGet a list of netCDF files located at the S3 path corresponding to the ECCO V4r4 monthly sea surface height dataset on the 0.5-degree latitude/longitude grid, for year 2015.\n\nssh_files = fs_s3.glob(bucket)\nssh_files\n\n\n\n\nOpen with the netCDF files using the s3fs package, then load them all at once into a concatenated xarray dataset.\n\nfileset = [fs_s3.open(file) for file in ssh_files]\n\nCreate an xarray dataset using the open_mfdataset() function to “read in” all of the netCDF4 files in one call.\n\nssh_ds = xr.open_mfdataset(fileset,\n                           combine='by_coords',\n                           mask_and_scale=True,\n                           decode_cf=True,\n                           chunks='auto')\nssh_ds\n\nGet the SSH variable as an xarray dataarray\n\nssh_da = ssh_ds.SSH\nssh_da\n\nPlot the SSH time series using hvplot\n\nssh_da.hvplot.image(y='latitude', x='longitude', cmap='Viridis',).opts(clim=(ssh_da.attrs['valid_min'][0],ssh_da.attrs['valid_max'][0]))\n\n\n\n\n\nDirect access to ECCO data in S3 (from us-west-2)\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links"
  },
  {
    "objectID": "how-tos/additional_resources/Multi-File_Direct_S3_Access_COG_Example.html",
    "href": "how-tos/additional_resources/Multi-File_Direct_S3_Access_COG_Example.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "from pystac_client import Client\nimport stackstac\n\n\nSTAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n\n\ncatalog = Client.open(f\"{STAC_URL}/LPCLOUD\")\n\n\nsearch = catalog.search(\n    collections = ['HLSL30.v2.0', 'HLSS30.v2.0'],\n    intersects = {'type': 'Polygon',\n                  'coordinates': [[[-101.67271614074707, 41.04754380304359],\n                                   [-101.65344715118408, 41.04754380304359],\n                                   [-101.65344715118408, 41.06213891056728],\n                                   [-101.67271614074707, 41.06213891056728],\n                                   [-101.67271614074707, 41.04754380304359]]]},\n    datetime = '2021-05/2021-08'\n)               \n\n\nsearch.matched()\n\n\nic = search.get_all_items()\n\n\nil = list(search.get_items())\n\n\ntic = [x for x in ic if 'T13TGF' in x.id]\n\n\nimport pystac\n\n\nitem_collection = pystac.ItemCollection(items=tic)\n\n\nitem_collection\n\n\nil\n\n\ndata = stackstac.stack(item_collection, assets=['B04', 'B02'], epsg=32613, resolution=30)\n\n\ndata.sel(band='B04').isel(time=[0])\n\n\nimport stackstac\nimport pystac_client\n\nURL = \"https://earth-search.aws.element84.com/v0\"\ncatalog = pystac_client.Client.open(URL)\n\n\ncatalog\n\n\nstac_items = catalog.search(\n    intersects=dict(type=\"Point\", coordinates=[-105.78, 35.79]),\n    collections=[\"sentinel-s2-l2a-cogs\"],\n    datetime=\"2020-04-01/2020-05-01\"\n).get_all_items()\n\n\nstac_items\n\n\nstack = stackstac.stack(stac_items)\n\n\nstack"
  },
  {
    "objectID": "how-tos/additional_resources/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html",
    "href": "how-tos/additional_resources/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "Requirements Earthdata Login\n\nOPeNDAP (Hyrax Server)\n\nOn-prem Endpoint - Open\nOn-prem Endpoint - Earthdata Login Authentication\nEarthdata Cloud Endpoint - Earthdata Login Authentication\n\n\n\n\n\n\nimport xarray as xr\nimport dask\nimport hvplot.xarray\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nopd_sst_url = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/NCEI/AVHRR_OI/v2/1981/244/19810901120000-NCEI-L4_GHRSST-SSTblend-AVHRR_OI-GLOB-v02.0-fv02.0.nc'\n\n\nopd_sst_ds = xr.open_dataset(opd_sst_url)\nopd_sst_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:           (lat: 720, lon: 1440, time: 1, nv: 2)\nCoordinates:\n  * lat               (lat) float32 -89.88 -89.62 -89.38 ... 89.38 89.62 89.88\n  * lon               (lon) float32 -179.9 -179.6 -179.4 ... 179.4 179.6 179.9\n  * time              (time) datetime64[ns] 1981-09-01\nDimensions without coordinates: nv\nData variables:\n    lat_bnds          (lat, nv) float32 -90.0 -89.75 -89.75 ... 89.75 89.75 90.0\n    lon_bnds          (lon, nv) float32 -180.0 -179.8 -179.8 ... 179.8 180.0\n    time_bnds         (time, nv) datetime64[ns] 1981-09-01 1981-09-02\n    analysed_sst      (time, lat, lon) float32 ...\n    analysis_error    (time, lat, lon) float32 ...\n    mask              (time, lat, lon) float32 ...\n    sea_ice_fraction  (time, lat, lon) float32 ...\nAttributes: (12/48)\n    product_version:                 Version 2.0\n    spatial_resolution:              0.25 degree\n    Conventions:                     CF-1.6,ACDD-1.3\n    title:                           NCEI global 0.25 deg daily sea surface t...\n    references:                      Reynolds, et al.(2009) What is New in Ve...\n    institution:                     NCEI\n    ...                              ...\n    source:                          AVHRR_Pathfinder-NODC-L3C-v5.1,ICOADS_SH...\n    summary:                         NOAA's 1/4-degree Daily Optimum Interpol...\n    time_coverage_start:             19810901T000000Z\n    time_coverage_end:               19810902T000000Z\n    uuid:                            39832cc3-d409-438a-820e-2bb1b38ebca8\n    DODS_EXTRA.Unlimited_Dimension:  timexarray.DatasetDimensions:lat: 720lon: 1440time: 1nv: 2Coordinates: (3)lat(lat)float32-89.88 -89.62 ... 89.62 89.88long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northcomment :Uniform grid with centers from -89.875 to 89.875 by 0.25 degrees.bounds :lat_bndsvalid_max :90.0valid_min :-90.0array([-89.875, -89.625, -89.375, ...,  89.375,  89.625,  89.875],\n      dtype=float32)lon(lon)float32-179.9 -179.6 ... 179.6 179.9long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastcomment :Uniform grid with centers from -179.875 to 179.875 by 0.25 degrees.bounds :lon_bndsvalid_max :180.0valid_min :-180.0array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875],\n      dtype=float32)time(time)datetime64[ns]1981-09-01long_name :reference time of sst fieldstandard_name :timeaxis :Tbounds :time_bndscomment :Nominal time because observations are from different sources and are made at different times of the day.array(['1981-09-01T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (7)lat_bnds(lat, nv)float32...comment :This variable defines the latitude values at the north and south bounds of every 0.25-degree pixel.array([[-90.  , -89.75],\n       [-89.75, -89.5 ],\n       [-89.5 , -89.25],\n       ...,\n       [ 89.25,  89.5 ],\n       [ 89.5 ,  89.75],\n       [ 89.75,  90.  ]], dtype=float32)lon_bnds(lon, nv)float32...comment :This variable defines the longitude values at the west and east bounds of every 0.25-degree pixel.array([[-180.  , -179.75],\n       [-179.75, -179.5 ],\n       [-179.5 , -179.25],\n       ...,\n       [ 179.25,  179.5 ],\n       [ 179.5 ,  179.75],\n       [ 179.75,  180.  ]], dtype=float32)time_bnds(time, nv)datetime64[ns]...comment :This variable defines the start and end of the time span for the data.array([['1981-09-01T00:00:00.000000000', '1981-09-02T00:00:00.000000000']],\n      dtype='datetime64[ns]')analysed_sst(time, lat, lon)float32...long_name :analysed sea surface temperaturestandard_name :sea_surface_temperatureunits :kelvinvalid_min :-300valid_max :4500comment :Single-sensor Pathfinder 5.0/5.1 AVHRR SSTs used until 2005; two AVHRRs at a time are used 2007 onward. Sea ice and in-situ data used also are 'near real time' quality for recent period.  SST (bulk) is at ambiguous depth because multiple types of observations are used.source :AVHRR_Pathfinder-NODC-L3C-v5.1,ICOADS_SHIP-NCAR-IN_SITU-v2.4,ICOADS_BUOY-NCAR-IN_SITU-v2.4,GSFC_25KM-NSIDC-ICE[1036800 values with dtype=float32]analysis_error(time, lat, lon)float32...long_name :estimated error standard deviation of analysed_sstunits :kelvinvalid_min :0valid_max :32767comment :Sum of bias, sampling and random errors.[1036800 values with dtype=float32]mask(time, lat, lon)float32...long_name :sea/land field composite maskflag_meanings :water landcomment :Binary mask distinguishing water and land only.flag_masks :[1 2]source :RWReynolds_landmask_V1.0valid_max :2valid_min :1[1036800 values with dtype=float32]sea_ice_fraction(time, lat, lon)float32...long_name :sea ice area fractionvalid_min :0valid_max :100standard_name :sea_ice_area_fractionunits :1comment :7-day median filtered .  Switch from 25 km NASA team ice (http://nsidc.org/data/nsidc-0051.html)  to 50 km NCEP ice (http://polar.ncep.noaa.gov/seaice) after 2004 results in artificial increase in ice coverage.source :GSFC_25KM-NSIDC-ICE[1036800 values with dtype=float32]Attributes: (48)product_version :Version 2.0spatial_resolution :0.25 degreeConventions :CF-1.6,ACDD-1.3title :NCEI global 0.25 deg daily sea surface temperature analysis based mainly on Advanced Very High Resolution Radiometer, finalreferences :Reynolds, et al.(2009) What is New in Version 2. Available at http://www.ncdc.noaa.gov/sites/default/files/attachments/Reynolds2009_oisst_daily_v02r00_version2-features.pdf; Daily 1/4 Degree Optimum Interpolation Sea Surface Temperature (OISST)- Climate Algorithm Theoretical Theoretical Basis Document, NOAA Climate Data Record Program CDRP-ATBD-0303 Rev. 2 (2013). Available at http://www1.ncdc.noaa.gov/pub/data/sds/cdr/CDRs/Sea_Surface_Temperature_Optimum_Interpolation/AlgorithmDescription.pdf.institution :NCEInetcdf_version_id :4.3.2history :2015-11-02T19:52:40Z: Modified format and attributes with NCO to match the GDS 2.0 rev 5 specification.start_time :19810901T000000Zstop_time :19810902T000000Zwesternmost_longitude :-180.0easternmost_longitude :180.0southernmost_latitude :-90.0northernmost_latitude :90.0comment :The daily OISST version 2.0 data contained in this file are the same as those in the equivalent GDS 1.0 file.Metadata_Conventions :ACDD-1.3acknowledgment :This project was supported in part by a grant from the NOAA Climate Data Record (CDR) Program. Cite this dataset when used as a source. The recommended citation and DOI depends on the data center from which the files were acquired. For data accessed from NOAA in near real-time or from the GHRSST LTSRF, cite as: Richard W. Reynolds, Viva F. Banzon, and NOAA CDR Program (2008): NOAA Optimum Interpolation 1/4 Degree Daily Sea Surface Temperature (OISST) Analysis, Version 2. [indicate subset used]. NOAA National Centers for Environmental Information. http://doi.org/doi:10.7289/V5SQ8XB5 [access date]. For data accessed from the NASA PO.DAAC, cite as: Richard W. Reynolds, Viva F. Banzon, and NOAA CDR Program (2008): NOAA Optimum Interpolation 1/4 Degree Daily Sea Surface Temperature (OISST) Analysis, Version 2. [indicate subset used]. PO.DAAC, CA, USA. http://doi.org/10.5067/GHAAO-4BC01 [access date].cdm_data_type :Gridcreator_name :Viva Banzoncreator_email :viva.banzon@noaa.govcreator_url :http://www.ncdc.noaa.govdate_created :20091203T000000Zfile_quality_level :3gds_version_id :2.0r5geospatial_lat_resolution :0.25geospatial_lat_units :degrees_northgeospatial_lon_resolution :0.25geospatial_lon_units :degrees_eastid :NCEI-L4LRblend-GLOB-AVHRR_OIkeywords :Oceans>Ocean Temperature>Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywords, Version 8.1license :No constraints on data access or use.metadata_link :http://doi.org/10.7289/V5SQ8XB5naming_authority :org.ghrsstplatform :NOAA-7processing_level :L4project :Group for High Resolution Sea Surface Temperaturepublisher_email :oisst_contacts@noaa.govpublisher_name :OISST Operations Teampublisher_url :http://www.ncdc.noaa.gov/sstsensor :AVHRR_GACstandard_name_vocabulary :CF Standard Name Table v29source :AVHRR_Pathfinder-NODC-L3C-v5.1,ICOADS_SHIP-NCAR-IN_SITU-v2.4,ICOADS_BUOY-NCAR-IN_SITU-v2.4,GSFC_25KM-NSIDC-ICEsummary :NOAA's 1/4-degree Daily Optimum Interpolation Sea Surface Temperature (OISST) (sometimes referred to as Reynold's SST, which however also refers to earlier products at different resolution), currently available as version 2,  is created by interpolating and extrapolating SST observations from different sources, resulting in a smoothed complete field. The sources of data are satellite (AVHRR) and in situ platforms (i.e., ships and buoys), and the specific datasets employed may change over. At the marginal ice zone, sea ice concentrations are used to generate proxy SSTs.  A preliminary version of this file is produced in near-real time (1-day latency), and then replaced with a final version after 2 weeks. Note that this is the AVHRR-ONLY DOISST, available from Oct 1981, but there is a companion DOISST product that includes microwave satellite data, available from June 2002.time_coverage_start :19810901T000000Ztime_coverage_end :19810902T000000Zuuid :39832cc3-d409-438a-820e-2bb1b38ebca8DODS_EXTRA.Unlimited_Dimension :time\n\n\n\nopd_sst_ds.analysed_sst.isel(time=0).hvplot.image(cmap='Inferno')\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nimport opendap_auth\n\n\nopendap_auth.create_dodsrc()\n\n'.dodsrc file created: /home/jovyan/.dodsrc'\n\n\nIntegrated Multi-satellitE Retrievals for GPM (IMERG) Level 3 IMERG Final Daily 10 x 10 km (GPM_3IMERGDF)\n\nopd_prec_url = 'https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/GPM_3IMERGDF.06/2021/07/3B-DAY.MS.MRG.3IMERG.20210704-S000000-E235959.V06.nc4' \n\n\nopd_prec_ds = xr.open_dataset(opd_prec_url)\nopd_prec_ds\n\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\n\n\nKeyboardInterrupt: \n\n\n\nopd_prec_ds.precipitationCal.isel(time=0).hvplot.image(cmap='rainbow')\n\n\n\n\n\nedc_odp_ssh_url = 'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Sea%20Surface%20Height%20-%20Daily%20Mean%200.5%20Degree%20(Version%204%20Release%204)/granules/SEA_SURFACE_HEIGHT_day_mean_1992-01-01_ECCO_V4r4_latlon_0p50deg.dap.nc'\n\n\nedc_odp_ssh_ds = xr.open_dataset(edc_odp_ssh_url)\nedc_odp_ssh_ds\n\n\nurl = 'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/GHRSST%20Level%204%20MUR%20Global%20Foundation%20Sea%20Surface%20Temperature%20Analysis%20(v4.1)/granules/20190201090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.dap.nc4'\n\n\nxr.open_dataset(url)\n\n\nurl = 'https://opendap.earthdata.nasa.gov/collections/C1968980576-POCLOUD/granules/S6A_P4_2__LR_RED__NR_025_001_20210713T162644_20210713T182234_F02.nc4'\n\n\nxr.open_dataset(url)\n\n\nurl = 'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Sea%20Surface%20Height%20-%20Daily%20Mean%200.5%20Degree%20(Version%204%20Release%204)/granules/SEA_SURFACE_HEIGHT_day_mean_1992-01-01_ECCO_V4r4_latlon_0p50deg.dap.nc4'\n\n\nxr.open_dataset(url)"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "In this notebook, we will access data for the ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each spectral band. We will access a single COG file, Land Surface Temperature (LST), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data.\n\n\n\n\n\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata.\n\n\n\n\n\nhow to configure you Python work environment to access Cloud Optimized geoTIFF (COG) files\nhow to access ECOSTRESS COG files\nhow to plot the data\n\n\n\n\n\nimport os\nfrom osgeo import gdal\nimport rasterio as rio\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL configurations we need to access the data from Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nGDAL environment variables must be configured to access COGs from Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\n<rasterio.env.Env at 0x7fe98012f670>\n\n\nIn this example we’re interested in the ECOSTRESS data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\nhttps_url = 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST.tif'\n\n\n\n\nRead in the ECOSTRESS s3 URL for the L30 red band (0.64 – 0.67 μm) into our workspace using rioxarray, an extension of xarray used to read geospatial data.\n\nda = rioxarray.open_rasterio(https_url)\nda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 3660, x: 3660)>\n[13395600 values with dtype=int16]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.1e+06 4.1e+06 4.1e+06 ... 3.99e+06 3.99e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayband: 1y: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (4)band(band)int641array([1])x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.1e+06 4.1e+06 ... 3.99e+06array([4100025., 4099995., 4099965., ..., 3990315., 3990285., 3990255.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not specified (based on WGS 84 spheroid)\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not specified (based on WGS 84 spheroid)projected_crs_name :UTM Zone 11, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not specified (based on WGS 84 spheroid)\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4100040.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda_lst = da.squeeze('band', drop=True)\nda_lst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 3660, x: 3660)>\n[13395600 values with dtype=int16]\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.1e+06 4.1e+06 4.1e+06 ... 3.99e+06 3.99e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayy: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (3)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.1e+06 4.1e+06 ... 3.99e+06array([4100025., 4099995., 4099965., ..., 3990315., 3990285., 3990255.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not specified (based on WGS 84 spheroid)\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not specified (based on WGS 84 spheroid)projected_crs_name :UTM Zone 11, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not specified (based on WGS 84 spheroid)\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4100040.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\nPlot the dataarray, representing the ECOSTESS band, using hvplot.\n\nda_lst.hvplot.image(x = 'x', y = 'y', crs = 'EPSG:32610', cmap='jet', rasterize=False, tiles='EsriImagery', width=800, height=600, colorbar=True, title = 'Land Surface Temperature')\n\n\n\n\n\n\n  \n\n\n\n\nExit the context manager.\n\nrio_env.__exit__()"
  },
  {
    "objectID": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html",
    "href": "how-tos/data_access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "In this notebook, we will access data for the ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each variable.\nWe will access a single COG file, Land Surface Temperature (LST), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data.\n\n\n\n\n\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n\nAn Earthdata Login account is required to access data from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata.\n\n\n\n\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to perform in-region direct access of ECOSTRESS Cloud Optimized geoTIFF (COG) files in S3\nhow to plot the data\n\n\n\n\n\nimport os\nimport requests \nimport boto3\nfrom osgeo import gdal\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nimport rioxarray\nimport xarray as xr\nimport geopandas\nfrom shapely.geometry import Polygon\nfrom shapely.ops import transform\nimport pyproj\nfrom pyproj import Proj\nimport hvplot.xarray\nimport holoviews as hv\nimport geoviews as gv\ngv.extension('bokeh', 'matplotlib')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n  \n  \n\n\n\n\n\n\n\n\n\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('lpdaac')\n#temp_creds_req\n\n\n\n\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nGDAL environment variables must be configured to access COGs in Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\n<rasterio.env.Env at 0x7fcfee922fa0>\n\n\nIn this example we’re interested in the ECOSTRESS Tiled Land Surface Temperature and Emissivity data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the S3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\n#s3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020272T183449.v2.0/HLS.L30.T10SGD.2020272T183449.v2.0.B04.tif'\ns3_url_lst = 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST.tif'\ns3_url_qa = 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_QC.tif'\n\n\n\n\nRead in the ECOSTRESS Tiles LST S3 URL into our workspace using rioxarray, an extension of xarray used to read geospatial data.\n\nda = rioxarray.open_rasterio(s3_url_lst, chunks='auto')\nda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 1568, x: 1568)>\ndask.array<open_rasterio-8869317cab2408c4770e6caee839802c<this-array>, shape=(1, 1568, 1568), dtype=float32, chunksize=(1, 1568, 1568), chunktype=numpy.ndarray>\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 2e+05 2.001e+05 2.002e+05 ... 3.096e+05 3.097e+05\n  * y            (y) float64 3.9e+06 3.9e+06 3.9e+06 ... 3.79e+06 3.79e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    nan\n    scale_factor:  1.0\n    add_offset:    0.0xarray.DataArrayband: 1y: 1568x: 1568dask.array<chunksize=(1, 1568, 1568), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         9.38 MiB \n                         9.38 MiB \n                    \n                    \n                    \n                         Shape \n                         (1, 1568, 1568) \n                         (1, 1568, 1568) \n                    \n                    \n                         Count \n                         2 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  1568\n  1568\n  1\n\n        \n    \nCoordinates: (4)band(band)int641array([1])x(x)float642e+05 2.001e+05 ... 3.097e+05array([200015., 200085., 200155., ..., 309565., 309635., 309705.])y(y)float643.9e+06 3.9e+06 ... 3.79e+06array([3899965., 3899895., 3899825., ..., 3790415., 3790345., 3790275.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :199980.0 70.0 0.0 3900000.0 0.0 -70.0array(0)Attributes: (3)_FillValue :nanscale_factor :1.0add_offset :0.0\n\n\n\nqa = rioxarray.open_rasterio(s3_url_qa, chunks='auto')\nqa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 1568, x: 1568)>\ndask.array<open_rasterio-cbe9aac41d32a53ce5e8a6acfb1a9f2b<this-array>, shape=(1, 1568, 1568), dtype=uint16, chunksize=(1, 1568, 1568), chunktype=numpy.ndarray>\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 2e+05 2.001e+05 2.002e+05 ... 3.096e+05 3.097e+05\n  * y            (y) float64 3.9e+06 3.9e+06 3.9e+06 ... 3.79e+06 3.79e+06\n    spatial_ref  int64 0\nAttributes:\n    scale_factor:  1.0\n    add_offset:    0.0xarray.DataArrayband: 1y: 1568x: 1568dask.array<chunksize=(1, 1568, 1568), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         4.69 MiB \n                         4.69 MiB \n                    \n                    \n                    \n                         Shape \n                         (1, 1568, 1568) \n                         (1, 1568, 1568) \n                    \n                    \n                         Count \n                         2 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     uint16 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  1568\n  1568\n  1\n\n        \n    \nCoordinates: (4)band(band)int641array([1])x(x)float642e+05 2.001e+05 ... 3.097e+05array([200015., 200085., 200155., ..., 309565., 309635., 309705.])y(y)float643.9e+06 3.9e+06 ... 3.79e+06array([3899965., 3899895., 3899825., ..., 3790415., 3790345., 3790275.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :199980.0 70.0 0.0 3900000.0 0.0 -70.0array(0)Attributes: (2)scale_factor :1.0add_offset :0.0\n\n\n\nLST_dataset = xr.Dataset({'LST': da, 'quality': qa})\nLST_dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:      (band: 1, x: 1568, y: 1568)\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 2e+05 2.001e+05 2.002e+05 ... 3.096e+05 3.097e+05\n  * y            (y) float64 3.9e+06 3.9e+06 3.9e+06 ... 3.79e+06 3.79e+06\n    spatial_ref  int64 0\nData variables:\n    LST          (band, y, x) float32 dask.array<chunksize=(1, 1568, 1568), meta=np.ndarray>\n    quality      (band, y, x) uint16 dask.array<chunksize=(1, 1568, 1568), meta=np.ndarray>xarray.DatasetDimensions:band: 1x: 1568y: 1568Coordinates: (4)band(band)int641array([1])x(x)float642e+05 2.001e+05 ... 3.097e+05array([200015., 200085., 200155., ..., 309565., 309635., 309705.])y(y)float643.9e+06 3.9e+06 ... 3.79e+06array([3899965., 3899895., 3899825., ..., 3790415., 3790345., 3790275.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :199980.0 70.0 0.0 3900000.0 0.0 -70.0array(0)Data variables: (2)LST(band, y, x)float32dask.array<chunksize=(1, 1568, 1568), meta=np.ndarray>_FillValue :nanscale_factor :1.0add_offset :0.0\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         9.38 MiB \n                         9.38 MiB \n                    \n                    \n                    \n                         Shape \n                         (1, 1568, 1568) \n                         (1, 1568, 1568) \n                    \n                    \n                         Count \n                         2 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  1568\n  1568\n  1\n\n        \n    \nquality(band, y, x)uint16dask.array<chunksize=(1, 1568, 1568), meta=np.ndarray>scale_factor :1.0add_offset :0.0\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         4.69 MiB \n                         4.69 MiB \n                    \n                    \n                    \n                         Shape \n                         (1, 1568, 1568) \n                         (1, 1568, 1568) \n                    \n                    \n                         Count \n                         2 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     uint16 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  1568\n  1568\n  1\n\n        \n    \nAttributes: (0)\n\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda_lst = LST_dataset.squeeze('band', drop=True)\nda_lst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:      (x: 1568, y: 1568)\nCoordinates:\n  * x            (x) float64 2e+05 2.001e+05 2.002e+05 ... 3.096e+05 3.097e+05\n  * y            (y) float64 3.9e+06 3.9e+06 3.9e+06 ... 3.79e+06 3.79e+06\n    spatial_ref  int64 0\nData variables:\n    LST          (y, x) float32 dask.array<chunksize=(1568, 1568), meta=np.ndarray>\n    quality      (y, x) uint16 dask.array<chunksize=(1568, 1568), meta=np.ndarray>xarray.DatasetDimensions:x: 1568y: 1568Coordinates: (3)x(x)float642e+05 2.001e+05 ... 3.097e+05array([200015., 200085., 200155., ..., 309565., 309635., 309705.])y(y)float643.9e+06 3.9e+06 ... 3.79e+06array([3899965., 3899895., 3899825., ..., 3790415., 3790345., 3790275.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :199980.0 70.0 0.0 3900000.0 0.0 -70.0array(0)Data variables: (2)LST(y, x)float32dask.array<chunksize=(1568, 1568), meta=np.ndarray>_FillValue :nanscale_factor :1.0add_offset :0.0\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         9.38 MiB \n                         9.38 MiB \n                    \n                    \n                    \n                         Shape \n                         (1568, 1568) \n                         (1568, 1568) \n                    \n                    \n                         Count \n                         3 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  1568\n  1568\n\n        \n    \nquality(y, x)uint16dask.array<chunksize=(1568, 1568), meta=np.ndarray>scale_factor :1.0add_offset :0.0\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         4.69 MiB \n                         4.69 MiB \n                    \n                    \n                    \n                         Shape \n                         (1568, 1568) \n                         (1568, 1568) \n                    \n                    \n                         Count \n                         3 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     uint16 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  1568\n  1568\n\n        \n    \nAttributes: (0)\n\n\nPlot the dataarray, representing the LST, using hvplot.\n\nda_lst['LST'].hvplot.image(x = 'x', y = 'y', crs = 'EPSG:32610', cmap='jet', rasterize=False, tiles='EsriImagery', width=800, height=600, colorbar=True, title = 'Land Surface Temperature')\n\n\n\n\n\n  \n\n\n\n\n\n\n\nWe’ll read in our GeoJSON file of our points of interest and create bounding box that contains a points coordinates\n\nfield = geopandas.read_file('../../tutorials/landcover.geojson')\n\nExtract the min/max values for the y and x axis\n\nminx, miny, maxx, maxy = field.geometry.total_bounds\nminx, miny, maxx, maxy\n\n(-119.54552918448296,\n 34.39556000406682,\n -119.52621487396785,\n 34.40555139039825)\n\n\nOrder the coordinates for the bounding box counterclockwise\n\ncoords = [\n    (minx, miny),\n    (maxx, miny),\n    (maxx, maxy),\n    (minx, maxy)\n]\n\nCreate a shapely polygon\n\nfeature_shape = Polygon(coords)\nfeature_shape\n\n\n\n\n\nbase = gv.tile_sources.EsriImagery.opts(width=700, height=500)\nfarmField = gv.Polygons(feature_shape).opts(line_color='yellow', line_width=10, color=None)\nbase * farmField \n\n\n\n\n\n  \n\n\n\n\nLet’s take a look at the bounding coordinate values.\nNote, the values above are in decimal degrees and represent the longitude and latitude for the lower left corner and upper right corner respectively.\n\nfeature_shape.bounds\n\n(-119.54552918448296,\n 34.39556000406682,\n -119.52621487396785,\n 34.40555139039825)\n\n\nGet the projection information from the ECOSTRESS file\n\nsrc_proj = da_lst.rio.crs\nsrc_proj\n\nCRS.from_epsg(32611)\n\n\nTransform coordinates from lat lon (units = dd) to UTM (units = m)\n\ngeo_CRS = Proj('+proj=longlat +datum=WGS84 +no_defs', preserve_units=True)   # Source coordinate system of the ROI\n\n\nproject = pyproj.Transformer.from_proj(geo_CRS, src_proj)                    # Set up the transformation\n\n\nfsUTM = transform(project.transform, feature_shape)\nfsUTM.bounds\n\n(265993.0454523829, 3808909.50873117, 267796.62994798424, 3810062.237028728)\n\n\nThe coordinates for our feature have now been converted to source raster projection. Note the difference in the values between feature_shape.bounds (in geographic) and fsUTM.bounds (in UTM projection).\nNow we can clip our ECOSTRESS LST file to our region of insterest!\n\n\n\nWe can now use our transformed ROI bounding box to clip the ECOSTRESS S3 object we accessed before. We’ll use the rio.clip\n\nda_lst_clip = rioxarray.open_rasterio(s3_url_lst, chunks='auto').squeeze('band', drop=True).rio.clip([fsUTM])\n\n\nda_lst_clip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 16, x: 26)>\ndask.array<copy, shape=(16, 26), dtype=float32, chunksize=(16, 26), chunktype=numpy.ndarray>\nCoordinates:\n  * y            (y) float64 3.81e+06 3.81e+06 3.81e+06 ... 3.809e+06 3.809e+06\n  * x            (x) float64 2.66e+05 2.661e+05 ... 2.677e+05 2.678e+05\n    spatial_ref  int64 0\nAttributes:\n    scale_factor:  1.0\n    add_offset:    0.0\n    _FillValue:    nanxarray.DataArrayy: 16x: 26dask.array<chunksize=(16, 26), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         1.62 kiB \n                         1.62 kiB \n                    \n                    \n                    \n                         Shape \n                         (16, 26) \n                         (16, 26) \n                    \n                    \n                         Count \n                         7 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  26\n  16\n\n        \n    \nCoordinates: (3)y(y)float643.81e+06 3.81e+06 ... 3.809e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([3810015., 3809945., 3809875., 3809805., 3809735., 3809665., 3809595.,\n       3809525., 3809455., 3809385., 3809315., 3809245., 3809175., 3809105.,\n       3809035., 3808965.])x(x)float642.66e+05 2.661e+05 ... 2.678e+05axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([266025., 266095., 266165., 266235., 266305., 266375., 266445., 266515.,\n       266585., 266655., 266725., 266795., 266865., 266935., 267005., 267075.,\n       267145., 267215., 267285., 267355., 267425., 267495., 267565., 267635.,\n       267705., 267775.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :265990.0 70.0 0.0 3810050.0 0.0 -70.0array(0)Attributes: (3)scale_factor :1.0add_offset :0.0_FillValue :nan\n\n\n\nda_lst_clip.hvplot.image(x = 'x', y = 'y', crs = 'EPSG:32610', cmap='jet', rasterize=False, width=800, height=600,title = 'Land Surface Temperature (Kelvin)', colorbar=True)\n\n\n\n\n\n  \n\n\n\n\nExit the context manager.\n\nrio_env.__exit__()\n\n\n\n\n\nDirect S3 Data Access with rioxarray\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nGetting Started with Cloud-Native Harmonized Landsat Sentinel-2 (HLS) Data in R"
  },
  {
    "objectID": "how-tos/data_access/Intro_xarray_hvplot.html",
    "href": "how-tos/data_access/Intro_xarray_hvplot.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "As Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g. zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors.\n\n\n\nxarray is an open-source project and python package to work with labelled multi-dimensional arrays. It is leverages numpy, pandas, matplotlib and dask to build Dataset and DataArray objects with built-in methods to subset, analyze, interpolate, and plot multi-dimensional data. It makes working with multi-dimensional data cubes efficient and fun. It will change your life for the better. You’ll be more attractive, more interesting, and better equiped to take on lifes challenges.\n\n\n\nIn this tutorial you will learn how to:\n\nload a netcdf file into xarray\ninterrogate the Dataset and understand the difference between DataArray and Dataset\nsubset a Dataset\ncalculate annual and monthly mean fields\ncalculate a time series of zonal means\nplot these results\n\nAs always, we’ll start by importing xarray. We’ll follow convention by giving the module the shortname xr\n\nimport xarray as xr\nxr.set_options(keep_attrs=True)\nimport hvplot.xarray\n\n\n\n\n\n\n\n\n\n\n\nI’m going to use one of xarray’s tutorial datasets. In this case, air temperature from the NCEP reanalysis. I’ll assign the result of the open_dataset to ds. I may change this to access a dataset directly\n\nds = xr.tutorial.open_dataset(\"air_temperature\")\n\nAs we are in an interactive environment, we can just type ds to see what we have.\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air      (time, lat, lon) float32 ...\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25time: 2920lon: 53Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (1)air(time, lat, lon)float32...long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ][3869000 values with dtype=float32]Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\nFirst thing to notice is that ds is an xarray.Dataset object. It has dimensions, lat, lon, and time. It also has coordinate variables with the same names as these dimensions. These coordinate variables are 1-dimensional. This is a NetCDF convention. The Dataset contains one data variable, air. This has dimensions (time, lat, lon).\nClicking on the document icon reveals attributes for each variable. Clicking on the disk icon reveals a representation of the data.\nEach of the data and coordinate variables can be accessed and examined using the variable name as a key.\n\nds.air\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 2920, lat: 25, lon: 53)>\n[3869000 values with dtype=float32]\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 25lon: 53...[3869000 values with dtype=float32]Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\n\nds['air']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 2920, lat: 25, lon: 53)>\n[3869000 values with dtype=float32]\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 25lon: 53...[3869000 values with dtype=float32]Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nThese are xarray.DataArray objects. This is the basic building block for xarray.\nVariables can also be accessed as attributes of ds.\n\nds.time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'time' (time: 2920)>\narray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    standard_name:  time\n    long_name:      Timexarray.DataArray'time'time: 29202013-01-01 2013-01-01T06:00:00 ... 2014-12-31T18:00:00array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Coordinates: (1)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (2)standard_name :timelong_name :Time\n\n\nA major difference between accessing a variable as an attribute versus using a key is that the attribute is read-only but the key method can be used to update the variable. For example, if I want to convert the units of air from Kelvin to degrees Celsius.\n\nds['air'] = ds.air - 273.15\n\nThis approach can also be used to add new variables\n\nds['air_kelvin'] = ds.air + 273.15\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air         (time, lat, lon) float32 -31.95 -30.65 -29.65 ... 23.04 22.54\n    air_kelvin  (time, lat, lon) float32 241.2 242.5 243.5 ... 296.5 296.2 295.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25time: 2920lon: 53Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)air(time, lat, lon)float32-31.95 -30.65 ... 23.04 22.54long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 20.540009,  20.73999 ,  22.23999 , ...,  21.940002,\n          21.540009,  21.140015],\n        [ 23.140015,  24.040009,  24.440002, ...,  22.140015,\n          21.940002,  21.23999 ],\n        [ 24.640015,  25.23999 ,  25.339996, ...,  22.540009,\n          22.339996,  22.040009]],\n\n       [[-28.059998, -28.86    , -29.86    , ..., -31.460007,\n         -31.660004, -31.36    ],\n        [-23.259995, -23.86    , -24.759995, ..., -33.559998,\n         -32.86    , -31.460007],\n        [-10.160004, -10.959991, -11.76001 , ..., -33.259995,\n         -30.559998, -26.86    ],\n        ...,\n        [ 20.640015,  20.540009,  21.940002, ...,  22.140015,\n          21.940002,  21.540009],\n        [ 22.940002,  23.73999 ,  24.040009, ...,  22.540009,\n          22.540009,  22.040009],\n        [ 24.540009,  24.940002,  24.940002, ...,  23.339996,\n          23.040009,  22.540009]]], dtype=float32)air_kelvin(time, lat, lon)float32241.2 242.5 243.5 ... 296.2 295.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\nIt is helpful to update attributes such as units, this saves time, confusion and mistakes, especially when you save the dataset.\n\nds['air'].attrs['units'] = 'degC'\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (lat: 25, time: 2920, lon: 53)\nCoordinates:\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air         (time, lat, lon) float32 -31.95 -30.65 -29.65 ... 23.04 22.54\n    air_kelvin  (time, lat, lon) float32 241.2 242.5 243.5 ... 296.5 296.2 295.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25time: 2920lon: 53Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)air(time, lat, lon)float32-31.95 -30.65 ... 23.04 22.54long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 20.540009,  20.73999 ,  22.23999 , ...,  21.940002,\n          21.540009,  21.140015],\n        [ 23.140015,  24.040009,  24.440002, ...,  22.140015,\n          21.940002,  21.23999 ],\n        [ 24.640015,  25.23999 ,  25.339996, ...,  22.540009,\n          22.339996,  22.040009]],\n\n       [[-28.059998, -28.86    , -29.86    , ..., -31.460007,\n         -31.660004, -31.36    ],\n        [-23.259995, -23.86    , -24.759995, ..., -33.559998,\n         -32.86    , -31.460007],\n        [-10.160004, -10.959991, -11.76001 , ..., -33.259995,\n         -30.559998, -26.86    ],\n        ...,\n        [ 20.640015,  20.540009,  21.940002, ...,  22.140015,\n          21.940002,  21.540009],\n        [ 22.940002,  23.73999 ,  24.040009, ...,  22.540009,\n          22.540009,  22.040009],\n        [ 24.540009,  24.940002,  24.940002, ...,  23.339996,\n          23.040009,  22.540009]]], dtype=float32)air_kelvin(time, lat, lon)float32241.2 242.5 243.5 ... 296.2 295.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[241.2    , 242.5    , 243.5    , ..., 232.79999, 235.5    ,\n         238.59999],\n        [243.79999, 244.5    , 244.7    , ..., 232.79999, 235.29999,\n         239.29999],\n        [250.     , 249.79999, 248.89   , ..., 233.2    , 236.39   ,\n         241.7    ],\n        ...,\n        [296.6    , 296.19998, 296.4    , ..., 295.4    , 295.1    ,\n         294.69998],\n        [295.9    , 296.19998, 296.79   , ..., 295.9    , 295.9    ,\n         295.19998],\n        [296.29   , 296.79   , 297.1    , ..., 296.9    , 296.79   ,\n         296.6    ]],\n\n       [[242.09999, 242.7    , 243.09999, ..., 232.     , 233.59999,\n         235.79999],\n        [243.59999, 244.09999, 244.2    , ..., 231.     , 232.5    ,\n         235.7    ],\n        [253.2    , 252.89   , 252.09999, ..., 230.79999, 233.39   ,\n         238.5    ],\n...\n        [293.69   , 293.88998, 295.38998, ..., 295.09   , 294.69   ,\n         294.29   ],\n        [296.29   , 297.19   , 297.59   , ..., 295.29   , 295.09   ,\n         294.38998],\n        [297.79   , 298.38998, 298.49   , ..., 295.69   , 295.49   ,\n         295.19   ]],\n\n       [[245.09   , 244.29   , 243.29   , ..., 241.68999, 241.48999,\n         241.79   ],\n        [249.89   , 249.29   , 248.39   , ..., 239.59   , 240.29   ,\n         241.68999],\n        [262.99   , 262.19   , 261.38998, ..., 239.89   , 242.59   ,\n         246.29   ],\n        ...,\n        [293.79   , 293.69   , 295.09   , ..., 295.29   , 295.09   ,\n         294.69   ],\n        [296.09   , 296.88998, 297.19   , ..., 295.69   , 295.69   ,\n         295.19   ],\n        [297.69   , 298.09   , 298.09   , ..., 296.49   , 296.19   ,\n         295.69   ]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\n\n\n\nSubsetting and indexing methods depend on whether you are working with a Dataset or DataArray. A DataArray can be accessed using positional indexing just like a numpy array. To access the temperature field for the first time step, you do the following.\n\nds['air'][0,:,:]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (lat: 25, lon: 53)>\narray([[-31.949997, -30.649994, -29.649994, ..., -40.350006, -37.649994,\n        -34.550003],\n       [-29.350006, -28.649994, -28.449997, ..., -40.350006, -37.850006,\n        -33.850006],\n       [-23.149994, -23.350006, -24.259995, ..., -39.949997, -36.759995,\n        -31.449997],\n       ...,\n       [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,  21.950012,\n         21.549988],\n       [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,  22.75    ,\n         22.049988],\n       [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,  23.640015,\n         23.450012]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n    time     datetime64[ns] 2013-01-01\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'lat: 25lon: 53-31.95 -30.65 -29.65 -29.15 -29.05 ... 24.64 24.45 23.75 23.64 23.45array([[-31.949997, -30.649994, -29.649994, ..., -40.350006, -37.649994,\n        -34.550003],\n       [-29.350006, -28.649994, -28.449997, ..., -40.350006, -37.850006,\n        -33.850006],\n       [-23.149994, -23.350006, -24.259995, ..., -39.949997, -36.759995,\n        -31.449997],\n       ...,\n       [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,  21.950012,\n         21.549988],\n       [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,  22.75    ,\n         22.049988],\n       [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,  23.640015,\n         23.450012]], dtype=float32)Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time()datetime64[ns]2013-01-01standard_name :timelong_name :Timearray('2013-01-01T00:00:00.000000000', dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nNote this returns a DataArray with coordinates but not attributes.\nHowever, the real power is being able to access variables using coordinate variables. I can get the same subset using the following. (It’s also more explicit about what is being selected and robust in case I modify the DataArray and expect the same output.)\n\nds['air'].sel(time='2013-01-01').time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'time' (time: 4)>\narray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2013-01-01T18:00:00\nAttributes:\n    standard_name:  time\n    long_name:      Timexarray.DataArray'time'time: 42013-01-01 2013-01-01T06:00:00 2013-01-01T12:00:00 2013-01-01T18:00:00array(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Coordinates: (1)time(time)datetime64[ns]2013-01-01 ... 2013-01-01T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (2)standard_name :timelong_name :Time\n\n\n\nds.air.sel(time='2013-01-01')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 4, lat: 25, lon: 53)>\narray([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 22.450012,  22.25    ,  22.25    , ...,  23.140015,\n          22.140015,  21.850006],\n        [ 23.049988,  23.350006,  23.140015, ...,  23.25    ,\n          22.850006,  22.450012],\n        [ 23.25    ,  23.140015,  23.25    , ...,  23.850006,\n          23.850006,  23.640015]],\n\n       [[-31.259995, -31.350006, -31.350006, ..., -38.759995,\n         -37.649994, -35.550003],\n        [-26.850006, -27.850006, -28.949997, ..., -42.259995,\n         -41.649994, -38.649994],\n        [-16.549988, -18.449997, -21.050003, ..., -42.449997,\n         -41.350006, -37.050003],\n        ...,\n        [ 23.450012,  23.25    ,  22.850006, ...,  23.350006,\n          22.640015,  22.140015],\n        [ 23.850006,  24.350006,  23.950012, ...,  23.640015,\n          23.450012,  23.140015],\n        [ 24.350006,  24.549988,  24.350006, ...,  24.640015,\n          24.850006,  24.75    ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 25.0 22.5 20.0 17.5 15.0\n  * lon      (lon) float32 200.0 202.5 205.0 207.5 ... 322.5 325.0 327.5 330.0\n  * time     (time) datetime64[ns] 2013-01-01 ... 2013-01-01T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 4lat: 25lon: 53-31.95 -30.65 -29.65 -29.15 -29.05 ... 25.45 25.05 24.64 24.85 24.75array([[[-31.949997, -30.649994, -29.649994, ..., -40.350006,\n         -37.649994, -34.550003],\n        [-29.350006, -28.649994, -28.449997, ..., -40.350006,\n         -37.850006, -33.850006],\n        [-23.149994, -23.350006, -24.259995, ..., -39.949997,\n         -36.759995, -31.449997],\n        ...,\n        [ 23.450012,  23.049988,  23.25    , ...,  22.25    ,\n          21.950012,  21.549988],\n        [ 22.75    ,  23.049988,  23.640015, ...,  22.75    ,\n          22.75    ,  22.049988],\n        [ 23.140015,  23.640015,  23.950012, ...,  23.75    ,\n          23.640015,  23.450012]],\n\n       [[-31.050003, -30.449997, -30.050003, ..., -41.149994,\n         -39.550003, -37.350006],\n        [-29.550003, -29.050003, -28.949997, ..., -42.149994,\n         -40.649994, -37.449997],\n        [-19.949997, -20.259995, -21.050003, ..., -42.350006,\n         -39.759995, -34.649994],\n...\n        [ 22.450012,  22.25    ,  22.25    , ...,  23.140015,\n          22.140015,  21.850006],\n        [ 23.049988,  23.350006,  23.140015, ...,  23.25    ,\n          22.850006,  22.450012],\n        [ 23.25    ,  23.140015,  23.25    , ...,  23.850006,\n          23.850006,  23.640015]],\n\n       [[-31.259995, -31.350006, -31.350006, ..., -38.759995,\n         -37.649994, -35.550003],\n        [-26.850006, -27.850006, -28.949997, ..., -42.259995,\n         -41.649994, -38.649994],\n        [-16.549988, -18.449997, -21.050003, ..., -42.449997,\n         -41.350006, -37.050003],\n        ...,\n        [ 23.450012,  23.25    ,  22.850006, ...,  23.350006,\n          22.640015,  22.140015],\n        [ 23.850006,  24.350006,  23.950012, ...,  23.640015,\n          23.450012,  23.140015],\n        [ 24.350006,  24.549988,  24.350006, ...,  24.640015,\n          24.850006,  24.75    ]]], dtype=float32)Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2013-01-01T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', '2013-01-01T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nI can also do slices. I’ll extract temperatures for the state of Colorado. The bounding box for the state is [-109 E, -102 E, 37 N, 41 N].\nIn the code below, pay attention to both the order of the coordinates and the range of values. The first value of the lat coordinate variable is 41 N, the second value is 37 N. Unfortunately, xarray expects slices of coordinates to be in the same order as the coordinates. Note lon is 0 to 360 not -180 to 180, and I let python calculate it for me within the slice.\n\nds.air.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 2920, lat: 2, lon: 3)>\narray([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 40.0 37.5\n  * lon      (lon) float32 252.5 255.0 257.5\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920lat: 2lon: 3-10.05 -9.25 -8.75 -6.25 -6.55 ... -15.36 -13.66 -13.76 -15.96 -14.46array([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)Coordinates: (3)lat(lat)float3240.0 37.5standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([40. , 37.5], dtype=float32)lon(lon)float32252.5 255.0 257.5standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([252.5, 255. , 257.5], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nWhat if we want temperature for a point, for example Denver, CO (39.72510678889283 N, -104.98785545855408 E). xarray can handle this! If we just want data from the nearest grid point, we can use sel and specify the method as “nearest”.\n\ndenver_lat, denver_lon = 39.72510678889283, -104.98785545855408\n\n\nds.air.sel(lat=denver_lat, lon=360+denver_lon, method='nearest').hvplot()\n\n\n\n\n\n  \n\n\n\n\nIf we want to interpolate, we can use interp(). In this case I use linear or bilinear interpolation.\ninterp() can also be used to resample data to a new grid and even reproject data\n\nds.air.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'air' (time: 2920)>\narray([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])\nCoordinates:\n  * time     (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\n    lat      float64 39.73\n    lon      float64 255.0\nAttributes:\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degC\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    dataset:       NMC Reanalysis\n    level_desc:    Surface\n    statistic:     Individual Obs\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]xarray.DataArray'air'time: 2920-8.951 -14.5 -18.44 -11.33 -8.942 ... -22.4 -27.79 -25.79 -15.42array([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])Coordinates: (3)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat()float6439.73standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray(39.72510679)lon()float64255.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray(255.01214454)Attributes: (11)long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]\n\n\nsel() and interp() can also be used on Dataset objects.\n\nds.sel(lat=slice(41,37), lon=slice(360-109,360-102))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (lat: 2, time: 2920, lon: 3)\nCoordinates:\n  * lat         (lat) float32 40.0 37.5\n  * lon         (lon) float32 252.5 255.0 257.5\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\nData variables:\n    air         (time, lat, lon) float32 -10.05 -9.25 -8.75 ... -15.96 -14.46\n    air_kelvin  (time, lat, lon) float32 263.1 263.9 264.4 ... 259.4 257.2 258.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 2time: 2920lon: 3Coordinates: (3)lat(lat)float3240.0 37.5standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([40. , 37.5], dtype=float32)lon(lon)float32252.5 255.0 257.5standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([252.5, 255. , 257.5], dtype=float32)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)air(time, lat, lon)float32-10.05 -9.25 ... -15.96 -14.46long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-10.049988 ,  -9.25     ,  -8.75     ],\n        [ -6.25     ,  -6.549988 ,  -6.3599854]],\n\n       [[-18.149994 , -14.950012 ,  -9.950012 ],\n        [-13.649994 , -11.049988 ,  -7.25     ]],\n\n       [[-20.449997 , -18.649994 , -13.359985 ],\n        [-19.350006 , -16.950012 , -11.25     ]],\n\n       ...,\n\n       [[-24.460007 , -28.259995 , -25.759995 ],\n        [-16.959991 , -24.059998 , -24.059998 ]],\n\n       [[-24.36     , -26.160004 , -23.460007 ],\n        [-15.959991 , -22.86     , -22.960007 ]],\n\n       [[-17.559998 , -15.359985 , -13.660004 ],\n        [-13.76001  , -15.959991 , -14.459991 ]]], dtype=float32)air_kelvin(time, lat, lon)float32263.1 263.9 264.4 ... 257.2 258.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[263.1    , 263.9    , 264.4    ],\n        [266.9    , 266.6    , 266.79   ]],\n\n       [[255.     , 258.19998, 263.19998],\n        [259.5    , 262.1    , 265.9    ]],\n\n       [[252.7    , 254.5    , 259.79   ],\n        [253.79999, 256.19998, 261.9    ]],\n\n       ...,\n\n       [[248.68999, 244.89   , 247.39   ],\n        [256.19   , 249.09   , 249.09   ]],\n\n       [[248.79   , 246.98999, 249.68999],\n        [257.19   , 250.29   , 250.18999]],\n\n       [[255.59   , 257.79   , 259.49   ],\n        [259.38998, 257.19   , 258.69   ]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\n\nds.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (time: 2920)\nCoordinates:\n  * time        (time) datetime64[ns] 2013-01-01 ... 2014-12-31T18:00:00\n    lat         float64 39.73\n    lon         float64 255.0\nData variables:\n    air         (time) float64 -8.951 -14.5 -18.44 ... -27.79 -25.79 -15.42\n    air_kelvin  (time) float64 264.2 258.7 254.7 261.8 ... 245.4 247.4 257.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:time: 2920Coordinates: (3)time(time)datetime64[ns]2013-01-01 ... 2014-12-31T18:00:00standard_name :timelong_name :Timearray(['2013-01-01T00:00:00.000000000', '2013-01-01T06:00:00.000000000',\n       '2013-01-01T12:00:00.000000000', ..., '2014-12-31T06:00:00.000000000',\n       '2014-12-31T12:00:00.000000000', '2014-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat()float6439.73standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray(39.72510679)lon()float64255.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray(255.01214454)Data variables: (2)air(time)float64-8.951 -14.5 ... -25.79 -15.42long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([ -8.95085077, -14.49752791, -18.43715163, ..., -27.78736503,\n       -25.78552388, -15.41780902])air_kelvin(time)float64264.2 258.7 254.7 ... 247.4 257.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([264.19914312, 258.65246598, 254.71284227, ..., 245.36262886,\n       247.36447002, 257.73218487])Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\n\n\n\nAs a simple example, let’s try to calculate a mean field for the whole time range.\n\nds.mean(dim='time').hvplot()\n\n\n\n\n\n  \n\n\n\n\nWe can also calculate a zonal mean (averaging over longitude)\n\nds.mean(dim='lon').hvplot()\n\n\n\n\n\n  \n\n\n\n\nOther aggregation methods include min(), max(), std(), along with others.\n\nds.std(dim='time').hvplot()\n\n\n\n\n\n  \n\n\n\n\nThe data we have are in 6h timesteps. This can be resampled to daily or monthly. If you are familiar with pandas, xarray uses the same methods.\n\nds_mon = ds.resample(time='M').mean()\nds_mon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (time: 24, lat: 25, lon: 53)\nCoordinates:\n  * time        (time) datetime64[ns] 2013-01-31 2013-02-28 ... 2014-12-31\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\nData variables:\n    air         (time, lat, lon) float32 -28.68 -28.49 -28.48 ... 24.57 24.56\n    air_kelvin  (time, lat, lon) float32 244.5 244.7 244.7 ... 297.7 297.7 297.7\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:time: 24lat: 25lon: 53Coordinates: (3)time(time)datetime64[ns]2013-01-31 ... 2014-12-31array(['2013-01-31T00:00:00.000000000', '2013-02-28T00:00:00.000000000',\n       '2013-03-31T00:00:00.000000000', '2013-04-30T00:00:00.000000000',\n       '2013-05-31T00:00:00.000000000', '2013-06-30T00:00:00.000000000',\n       '2013-07-31T00:00:00.000000000', '2013-08-31T00:00:00.000000000',\n       '2013-09-30T00:00:00.000000000', '2013-10-31T00:00:00.000000000',\n       '2013-11-30T00:00:00.000000000', '2013-12-31T00:00:00.000000000',\n       '2014-01-31T00:00:00.000000000', '2014-02-28T00:00:00.000000000',\n       '2014-03-31T00:00:00.000000000', '2014-04-30T00:00:00.000000000',\n       '2014-05-31T00:00:00.000000000', '2014-06-30T00:00:00.000000000',\n       '2014-07-31T00:00:00.000000000', '2014-08-31T00:00:00.000000000',\n       '2014-09-30T00:00:00.000000000', '2014-10-31T00:00:00.000000000',\n       '2014-11-30T00:00:00.000000000', '2014-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)Data variables: (2)air(time, lat, lon)float32-28.68 -28.49 ... 24.57 24.56long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-28.68323  , -28.486452 , -28.479755 , ..., -30.658554 ,\n         -29.743628 , -28.474194 ],\n        [-26.076784 , -26.127504 , -26.4225   , ..., -32.5679   ,\n         -31.105167 , -28.442825 ],\n        [-22.770565 , -23.31516  , -24.042498 , ..., -31.165657 ,\n         -28.38291  , -24.144924 ],\n        ...,\n        [ 22.688152 ,  22.00097  ,  21.773153 , ...,  22.218397 ,\n          21.734531 ,  21.118395 ],\n        [ 23.31952  ,  23.16702  ,  22.698233 , ...,  22.43775  ,\n          22.190727 ,  21.715578 ],\n        [ 23.903486 ,  23.89203  ,  23.585333 , ...,  23.154608 ,\n          22.947426 ,  22.889124 ]],\n\n       [[-32.41607  , -32.44866  , -32.738483 , ..., -31.54482  ,\n         -30.430185 , -29.205448 ],\n        [-31.216885 , -31.08063  , -31.236965 , ..., -32.135708 ,\n         -30.825186 , -28.42241  ],\n        [-27.826433 , -28.123934 , -28.78045  , ..., -29.734114 ,\n         -27.383936 , -23.491434 ],\n...\n        [ 24.899088 ,  24.200085 ,  24.072004 , ...,  24.861843 ,\n          24.510258 ,  23.995668 ],\n        [ 25.815008 ,  25.661922 ,  25.121607 , ...,  24.954088 ,\n          25.071083 ,  24.735588 ],\n        [ 26.023424 ,  26.06767  ,  25.74576  , ...,  25.566338 ,\n          25.591848 ,  25.630259 ]],\n\n       [[-26.348473 , -26.260897 , -26.380894 , ..., -33.07903  ,\n         -32.067986 , -30.868315 ],\n        [-25.419994 , -24.849277 , -24.405483 , ..., -34.531376 ,\n         -32.82783  , -30.179682 ],\n        [-23.181051 , -23.56476  , -23.574757 , ..., -35.446938 ,\n         -31.91259  , -26.923311 ],\n        ...,\n        [ 23.299198 ,  22.541454 ,  22.60839  , ...,  23.378307 ,\n          23.067505 ,  22.662996 ],\n        [ 24.295895 ,  24.286139 ,  24.031782 , ...,  23.80259  ,\n          23.908312 ,  23.579037 ],\n        [ 24.897346 ,  25.076134 ,  24.909689 , ...,  24.547583 ,\n          24.573233 ,  24.560413 ]]], dtype=float32)air_kelvin(time, lat, lon)float32244.5 244.7 244.7 ... 297.7 297.7long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[244.4667 , 244.66354, 244.67027, ..., 242.49142, 243.40633,\n         244.67577],\n        [247.07323, 247.02248, 246.7275 , ..., 240.58205, 242.04489,\n         244.70726],\n        [250.37941, 249.83484, 249.10748, ..., 241.98434, 244.76712,\n         249.00505],\n        ...,\n        [295.83795, 295.15085, 294.9229 , ..., 295.36826, 294.88437,\n         294.26828],\n        [296.46942, 296.31686, 295.84802, ..., 295.5876 , 295.34058,\n         294.86536],\n        [297.05316, 297.0418 , 296.73517, ..., 296.30438, 296.09732,\n         296.0389 ]],\n\n       [[240.73384, 240.7013 , 240.4115 , ..., 241.60518, 242.71988,\n         243.94455],\n        [241.93309, 242.06935, 241.913  , ..., 241.01428, 242.32481,\n         244.72758],\n        [245.32361, 245.0261 , 244.36955, ..., 243.41588, 245.7661 ,\n         249.65858],\n...\n        [298.04895, 297.35007, 297.22195, ..., 298.01172, 297.66013,\n         297.14554],\n        [298.96484, 298.81186, 298.27136, ..., 298.10403, 298.22104,\n         297.88547],\n        [299.17334, 299.2175 , 298.89566, ..., 298.71625, 298.74167,\n         298.7802 ]],\n\n       [[246.80156, 246.88907, 246.76907, ..., 240.07089, 241.08206,\n         242.2817 ],\n        [247.72998, 248.30064, 248.74443, ..., 238.61859, 240.3222 ,\n         242.97026],\n        [249.96893, 249.58516, 249.57521, ..., 237.70308, 241.23743,\n         246.22667],\n        ...,\n        [296.4491 , 295.6914 , 295.75824, ..., 296.52817, 296.21747,\n         295.8128 ],\n        [297.44586, 297.43613, 297.1817 , ..., 296.95242, 297.05823,\n         296.72897],\n        [298.0472 , 298.22598, 298.0595 , ..., 297.6975 , 297.72318,\n         297.71024]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\nThis is a really short time series but as an example, let’s calculate a monthly climatology (at least for 2 months). For this we can use groupby()\n\nds_clim = ds_mon.groupby(ds_mon.time.dt.month).mean()\nds_clim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:     (lat: 25, month: 12, lon: 53)\nCoordinates:\n  * lat         (lat) float32 75.0 72.5 70.0 67.5 65.0 ... 22.5 20.0 17.5 15.0\n  * lon         (lon) float32 200.0 202.5 205.0 207.5 ... 325.0 327.5 330.0\n  * month       (month) int64 1 2 3 4 5 6 7 8 9 10 11 12\nData variables:\n    air         (month, lat, lon) float32 -26.8 -26.76 -26.94 ... 24.42 24.39\n    air_kelvin  (month, lat, lon) float32 246.3 246.4 246.2 ... 297.6 297.5\nAttributes:\n    Conventions:  COARDS\n    title:        4x daily NMC reanalysis (1948)\n    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n    platform:     Model\n    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...xarray.DatasetDimensions:lat: 25month: 12lon: 53Coordinates: (3)lat(lat)float3275.0 72.5 70.0 ... 20.0 17.5 15.0standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Yarray([75. , 72.5, 70. , 67.5, 65. , 62.5, 60. , 57.5, 55. , 52.5, 50. , 47.5,\n       45. , 42.5, 40. , 37.5, 35. , 32.5, 30. , 27.5, 25. , 22.5, 20. , 17.5,\n       15. ], dtype=float32)lon(lon)float32200.0 202.5 205.0 ... 327.5 330.0standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :Xarray([200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. ], dtype=float32)month(month)int641 2 3 4 5 6 7 8 9 10 11 12array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])Data variables: (2)air(month, lat, lon)float32-26.8 -26.76 -26.94 ... 24.42 24.39long_name :4xDaily Air temperature at sigma level 995units :degCprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[-26.800243 , -26.764072 , -26.935038 , ..., -30.089035 ,\n         -29.062258 , -27.503464 ],\n        [-24.292501 , -24.242697 , -24.439713 , ..., -31.621532 ,\n         -29.641533 , -26.395487 ],\n        [-21.572863 , -21.953503 , -22.43548  , ..., -29.751295 ,\n         -26.365608 , -21.584438 ],\n        ...,\n        [ 22.70041  ,  22.094238 ,  22.077103 , ...,  22.036419 ,\n          21.507267 ,  20.898678 ],\n        [ 23.394682 ,  23.320007 ,  23.010254 , ...,  22.206137 ,\n          21.931412 ,  21.380169 ],\n        [ 24.004492 ,  24.08844  ,  23.899166 , ...,  22.868156 ,\n          22.625813 ,  22.486786 ]],\n\n       [[-26.4729   , -26.744377 , -27.201698 , ..., -31.29174  ,\n         -30.148129 , -28.706345 ],\n        [-25.35005  , -25.390135 , -25.672592 , ..., -32.503124 ,\n         -30.883888 , -28.083572 ],\n        [-24.079243 , -24.57772  , -25.207546 , ..., -30.72148  ,\n         -27.816654 , -23.427551 ],\n...\n        [ 24.692593 ,  23.990627 ,  23.837757 , ...,  24.818966 ,\n          24.418842 ,  24.011253 ],\n        [ 25.437843 ,  25.270302 ,  24.819141 , ...,  25.014214 ,\n          25.044006 ,  24.75834  ],\n        [ 25.661469 ,  25.706635 ,  25.471094 , ...,  25.579634 ,\n          25.601887 ,  25.668968 ]],\n\n       [[-25.179115 , -25.12904  , -25.23718  , ..., -33.37831  ,\n         -32.12641  , -30.52194  ],\n        [-23.41661  , -22.98972  , -22.664356 , ..., -34.360527 ,\n         -32.18549  , -29.033997 ],\n        [-21.12061  , -21.618912 , -21.78383  , ..., -35.07468  ,\n         -31.237263 , -26.080326 ],\n        ...,\n        [ 23.61517  ,  22.826698 ,  22.739243 , ...,  23.306011 ,\n          22.94138  ,  22.507713 ],\n        [ 24.318195 ,  24.23037  ,  23.894321 , ...,  23.70565  ,\n          23.696701 ,  23.371418 ],\n        [ 24.730938 ,  24.83678  ,  24.625538 , ...,  24.450327 ,\n          24.415411 ,  24.387712 ]]], dtype=float32)air_kelvin(month, lat, lon)float32246.3 246.4 246.2 ... 297.6 297.5long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturedataset :NMC Reanalysislevel_desc :Surfacestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]array([[[246.3497 , 246.38591, 246.21494, ..., 243.06091, 244.08772,\n         245.64651],\n        [248.85742, 248.90721, 248.71027, ..., 241.52846, 243.50848,\n         246.75456],\n        [251.57706, 251.19641, 250.71442, ..., 243.39868, 246.78441,\n         251.56552],\n        ...,\n        [295.85022, 295.24414, 295.22687, ..., 295.18628, 294.6571 ,\n         294.04852],\n        [296.5446 , 296.46985, 296.16003, ..., 295.35596, 295.08124,\n         294.52997],\n        [297.15424, 297.23822, 297.049  , ..., 296.018  , 295.7757 ,\n         295.6366 ]],\n\n       [[246.67706, 246.4056 , 245.94827, ..., 241.85826, 243.00189,\n         244.44366],\n        [247.79994, 247.75981, 247.47739, ..., 240.64688, 242.2661 ,\n         245.06639],\n        [249.07072, 248.57225, 247.94246, ..., 242.42856, 245.33334,\n         249.72246],\n...\n        [297.8424 , 297.1405 , 296.9876 , ..., 297.9688 , 297.5687 ,\n         297.16113],\n        [298.58765, 298.4202 , 297.96893, ..., 298.1641 , 298.19388,\n         297.9082 ],\n        [298.81134, 298.85648, 298.62097, ..., 298.72946, 298.75168,\n         298.81885]],\n\n       [[247.97087, 248.02097, 247.91278, ..., 239.77167, 241.02362,\n         242.62805],\n        [249.73335, 250.1602 , 250.4856 , ..., 238.78946, 240.96454,\n         244.116  ],\n        [252.0294 , 251.53104, 251.36612, ..., 238.0753 , 241.91273,\n         247.06966],\n        ...,\n        [296.765  , 295.9766 , 295.88907, ..., 296.45587, 296.09125,\n         295.65753],\n        [297.4681 , 297.38025, 297.0442 , ..., 296.85553, 296.84656,\n         296.5213 ],\n        [297.88074, 297.9866 , 297.77533, ..., 297.60022, 297.5653 ,\n         297.53754]]], dtype=float32)Attributes: (5)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelreferences :http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html\n\n\n\n\n\nFinally, let’s plot the results! This will plot the lat/lon axes of the original ds DataArray.\n\nds_clim.air.sel(month=10).hvplot()"
  },
  {
    "objectID": "how-tos/data-discovery/Data_Discovery_CMR_API.html",
    "href": "how-tos/data-discovery/Data_Discovery_CMR_API.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "In this notebook, we will walk through how to search for Earthdata data collections and granules. Along the way we will explore the available search parameters, information return, and specific contrains when using the CMR API. Our object is to identify assets to access that we would downloaded, or perform S3 direct access, within an analysis workflow\nWe will be querying CMR for ECOSTRESS version 2 collections/granules to identify assets we would downloaded, or perform S3 direct access, within an analysis workflow.\n\n\n\n\nUnderstand what CMR/CMR API is and what CMR/CMR API can be used for\nHow to use the requests package to search data collections and granules\nHow to parse the results of these searches.\n\n\n\n\nCMR is the Common Metadata Repository. It catalogs all data for NASA’s Earth Observing System Data and Information System (EOSDIS). It is the backend of Earthdata Search, the GUI search interface. More information about CMR can be found here.\nUnfortunately, the GUI for Earthdata Search is not accessible from a cloud instance - at least not without some work. Earthdata Search is also not immediately reproducible. What I mean by that is if you create a search using the GUI you would have to note the search criteria (date range, search area, collection name, etc), take a screenshot, copy the search url, or save the list of data granules returned by the search, in order to recreate the search. This information would have to be re-entered each time you or someone else wanted to do the search. You could make typos or other mistakes. A cleaner, reproducible solution is to search CMR programmatically using the CMR API.\n\n\n\nAPI stands for Application Programming Interface. It allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results.\n\n\n\n\nThe first step is to import python packages. We will use:\n- requests This package does most of the work for us accessing the CMR API using HTTP methods. - pprint to pretty print the results of the search.\nA more in-depth tutorial on requests is here\n\nimport requests\nimport json\nfrom pprint import pprint\n\nTo conduct a search using the CMR API, requests needs the url for the root CMR search endpoint. We’ll assign this url to a python variable as a string.\n\nCMR_OPS = 'https://cmr.earthdata.nasa.gov/search'\n\nCMR allows search by collections, which are datasets, and granules, which are files that contain data. Many of the same search parameters can be used for collections and granules but the type of results returned differ. Search parameters can be found in the API Documentation.\nWhether we search collections or granules is distinguished by adding \"collections\" or \"granules\" to the end of the CMR endpoint URL.\nWe are going to search collections first, so we add \"collections\" to the URL. We are using a python format string in the examples below.\n\nurl = f'{CMR_OPS}/{\"collections\"}'\nurl\n\n'https://cmr.earthdata.nasa.gov/search/collections'\n\n\nIn this first example, we want to retrieve a list of ECOSTRESS collections in the Earthdata Cloud. This includes ECOSTRESS collections for built 7.1 data which recently became publicly available. This means you will not need to generate a token to access data. Before the public release you should have been part of the access list to access the data. Because of that, an extra token parameter, generated using your Earthdata Login credentials needed to be passed in each CMR request that indicated you are a valid user.\nwe want to retrieve the collections that are hosted in the cloud ('cloud_hosted': 'True') that has granules availble ('has_granules': 'True'). We also want to get the content in json (pronounced “jason”) format, so I pass a dictionary to the header keyword argument to say that I want results returned as json ('Accept': 'application/json').\nThe .get() method is used to send this information to the CMR API. get() calls the HTTP method GET.\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                        },\n                        headers={\n                            'Accept': 'application/json',\n                        }\n                       )\n\nThe request returns a Response object.\nTo check that our request was successful we can print the response variable we saved the request to.\n\nresponse\n\n<Response [200]>\n\n\nA 200 response is what we want. This means that the requests was successful. For more information on HTTP status codes see https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\nA more explict way to check the status code is to use the status_code attribute. Both methods return a HTTP status code.\n\nresponse.status_code\n\n200\n\n\nThe response from requests.get returns the results of the search and metadata about those results in the headers.\nMore information about the response object can be found by typing help(response).\nheaders contains useful information in a case-insensitive dictionary. We requested (above) that the information be return in json which means the object return is a dictionary in our Python environment. We’ll iterate through the returned dictionary, looping throught each field (k) and its associated value (v). For more on interating through dictionary object click here.\n\nfor k, v in response.headers.items():\n    print(f'{k}: {v}')\n\nContent-Type: application/json;charset=utf-8\nContent-Length: 4204\nConnection: keep-alive\nDate: Tue, 15 Nov 2022 18:08:14 GMT\nX-Frame-Options: SAMEORIGIN\nAccess-Control-Allow-Origin: *\nX-XSS-Protection: 1; mode=block\nCMR-Request-Id: cf80d8ad-a428-4ca4-a85e-998ec5b0c02f\nStrict-Transport-Security: max-age=31536000\nCMR-Search-After: [0.0,23600.0,\"SENTINEL-1A_DP_META_GRD_HIGH\",\"1\",1214470576,826]\nCMR-Hits: 1674\nAccess-Control-Expose-Headers: CMR-Hits, CMR-Request-Id, X-Request-Id, CMR-Scroll-Id, CMR-Search-After, CMR-Timed-Out, CMR-Shapefile-Original-Point-Count, CMR-Shapefile-Simplified-Point-Count\nX-Content-Type-Options: nosniff\nCMR-Took: 237\nX-Request-Id: RjtJpW50AJUR058fFO1oB1ULN3tF72Vo-ffvb9N7FDYx7GjZPHVE1w==\nVary: Accept-Encoding, User-Agent\nContent-Encoding: gzip\nServer: ServerTokens ProductOnly\nX-Cache: Miss from cloudfront\nVia: 1.1 aa0280f933863b8ffd5ff636330f4170.cloudfront.net (CloudFront)\nX-Amz-Cf-Pop: HIO50-C2\nX-Amz-Cf-Id: RjtJpW50AJUR058fFO1oB1ULN3tF72Vo-ffvb9N7FDYx7GjZPHVE1w==\n\n\nEach item in the dictionary can be accessed in the normal way you access a python dictionary but the keys uniquely case-insensitive. Let’s take a look at the commonly used CMR-Hits key.\n\nresponse.headers['CMR-Hits']\n\n'1674'\n\n\nNote that “cmr-hits” works as well!\n\nresponse.headers['cmr-hits']\n\n'1674'\n\n\nIn some situations the response to your query can return a very large number of result, some of which may not be relevant. We can add additional query parameters to restrict the information returned. We’re going to restrict the search by the provider parameter.\nYou can modify the code below to explore all Earthdata data products hosted by the various providers. When searching by provider, use Cloud Provider to search for cloud-hosted datasets and On-Premises Provider to search for datasets archived at the DAACs. A partial list of providers is given below.\n\n\n\n\n\n\n\n\n\nDAAC\nShort Name\nCloud Provider\nOn-Premises Provider\n\n\n\n\nNSIDC\nNational Snow and Ice Data Center\nNSIDC_CPRD\nNSIDC_ECS\n\n\nGHRC DAAC\nGlobal Hydrometeorology Resource Center\nGHRC_DAAC\nGHRC_DAAC\n\n\nPO DAAC\nPhysical Oceanography Distributed Active Archive Center\nPOCLOUD\nPODAAC\n\n\nASF\nAlaska Satellite Facility\nASF\nASF\n\n\nORNL DAAC\nOak Ridge National Laboratory\nORNL_CLOUD\nORNL_DAAC\n\n\nLP DAAC\nLand Processes Distributed Active Archive Center\nLPCLOUD\nLPDAAC_ECS\n\n\nGES DISC\nNASA Goddard Earth Sciences (GES) Data and Information Services Center (DISC)\nGES_DISC\nGES_DISC\n\n\nOB DAAC\nNASA’s Ocean Biology Distributed Active Archive Center\n\nOB_DAAC\n\n\nSEDAC\nNASA’s Socioeconomic Data and Applications Center\n\nSEDAC\n\n\n\nWe’ll assign the provider to a variable as a string and insert the variable into the parameter argument in the request. We’ll also assign the term ‘ECOSTRESS’ to a varible so we don’t need to repeatedly add it to the requests parameters.\n\nprovider = 'LPCLOUD'\nproject = 'ECOSTRESS'\n\n\nheaders = {\n    #'Authorization': f'Bearer {token}',\n    'Accept': 'application/json',\n}\n\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                            'provider': provider,\n                            'project': project,\n                        },\n                        headers=headers\n                       )\nresponse\n\n<Response [200]>\n\n\n\nresponse.headers['cmr-hits']\n\n'5'\n\n\nSearch results are contained in the content part of the Response object. However, response.content returns information in bytes.\n\nresponse.content\n\nb'{\"feed\":{\"updated\":\"2022-11-15T18:08:14.505Z\",\"id\":\"https://cmr.earthdata.nasa.gov:443/search/collections.json?cloud_hosted=True&has_granules=True&provider=LPCLOUD&project=ECOSTRESS\",\"title\":\"ECHO dataset metadata\",\"entry\":[{\"processing_level_id\":\"2\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2018-07-09T00:00:00.000Z\",\"version_id\":\"002\",\"updated\":\"2021-06-23T16:50:51.108Z\",\"dataset_id\":\"ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"ECO_L2T_LSTE\",\"organizations\":[\"LP DAAC\",\"NASA/JPL/ECOSTRESS\"],\"title\":\"ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS is attached to the International Space Station (ISS) and collects data over the conterminous United States (CONUS) as well as key biomes and agricultural zones around the world and selected FLUXNET validation sites. A map of the acquisition coverage can be found on the ECOSTRESS website.\\\\r\\\\nThe ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous Level 2 Global 70 m (ECO_L2T_LSTE) Version 2 data product provides atmospherically corrected land surface temperature and emissivity (LST&E) values derived from five thermal infrared (TIR) bands. The ECO_L2T_LSTE data product was derived using a physics-based Temperature/Emissivity Separation (TES) algorithm. This tiled data product is subset from the ECO_L2G_LSTE data product using a modified version of the Military Grid Reference System (MGRS) which divides Universal Transverse Mercator (UTM) zones into square tiles that are 109.8 km by 109.8 km with a 70 meter (m) spatial resolution.\\\\r\\\\nThe ECO_L2T_LSTE Version 2 data product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate COG. This product contains seven layers including LST, LST error, wideband emissivity, quality flags, height, and cloud and water masks. For acquisitions after May 15, 2019, data products contain data values for TIR bands 2, 4, and 5 only. TIR bands 1 and 3 contain fill values to accommodate direct streaming of data from the ISS, as mentioned in the Known Issues section. LST data generated after May 15, 2019 will only use the 3 available bands, accuracy may be affected when compared to the LST data that utilized all 5 bands.\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2076090826-LPCLOUD\",\"has_formats\":false,\"score\":1.32,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":true,\"platforms\":[\"ISS\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2076090826-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/ECOSTRESS/ECO_L2T_LSTE.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1324/ECO2_LSTE_ATBD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_21247_032_19MCN_20220405T135253_0700_01/ECOv002_L2T_LSTE_21247_032_19MCN_20220405T135253_0700_01.png\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/380/ECO2_PSD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/299/ECO2_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1566/ECOL2-4_Grid_Tile_User_Guide_V2.pdf\"}]},{\"processing_level_id\":\"1B\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2018-07-09T00:00:00.000Z\",\"version_id\":\"002\",\"dataset_id\":\"ECOSTRESS Swath Geolocation Instantaneous L1B Global 70 m V002\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"ECO_L1B_GEO\",\"organizations\":[\"LP DAAC\",\"NASA/JPL/ECOSTRESS\"],\"title\":\"ECOSTRESS Swath Geolocation Instantaneous L1B Global 70 m V002\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS is attached to the International Space Station (ISS) and collects data over the conterminous United States (CONUS) as well as key biomes and agricultural zones around the world and selected  FLUXNET (http://fluxnet.fluxdata.org/about/) validation sites. A map of the acquisition coverage can be found on the ECOSTRESS website (https://ecostress.jpl.nasa.gov/science).\\\\r\\\\n\\\\r\\\\nThe ECOSTRESS Swath Geolocation Instantaneous Level 1B Global (ECO_L1B_GEO) Version 2 data product provides the geolocation information for the radiance values retrieved in the ECO_L1B_RAD (https://doi.org/10.5067/ecostress/eco_l1b_rad.002) Version 2 data product. The geolocation product gives geo-tagging to each of the radiance pixels. The geolocation processing corrects the ISS-reported ephemeris and attitude data by image matching with a global ortho-base derived from Landsat data, and then assigns latitude and longitude values to each of the Level 1 radiance pixels. When image matching is successful, the data are geolocated to better than 50 meter (m) accuracy. The ECO_L1B_GEO data product is provided as swath data.\\\\r\\\\n\\\\r\\\\nThe ECO_L1B_GEO data product contains data layers for latitude and longitude values, solar and view geometry information, surface height, and the fraction of pixel on land versus water distributed in HDF5 format.\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2076087338-LPCLOUD\",\"has_formats\":false,\"score\":1.32,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"ISS\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2076087338-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/ECOSTRESS/ECO_L1B_GEO.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1491/ECO1B_User_Guide_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/223/ECO1B_Geolocation_ATBD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://ecostress.jpl.nasa.gov/science\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/222/ECO1B_Calibration_ATBD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1321/ECO1B_PSD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/225/ECO1B_Rad_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/226/ECO1B_Geo_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/227/ECO_Earthdata_Search_Quick_Guide.pdf\"},{\"length\":\"700.0MB\",\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://appeears.earthdatacloud.nasa.gov/\"}]},{\"processing_level_id\":\"1B\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2018-07-09T00:00:00.000Z\",\"version_id\":\"002\",\"dataset_id\":\"ECOSTRESS Swath Top of Atmosphere Calibrated Radiance Instantaneous L1B Global 70 m\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"ECO_L1B_RAD\",\"organizations\":[\"LP DAAC\",\"NASA/JPL/ECOSTRESS\"],\"title\":\"ECOSTRESS Swath Top of Atmosphere Calibrated Radiance Instantaneous L1B Global 70 m\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS is attached to the International Space Station (ISS) and collects data over the conterminous United States (CONUS) as well as key biomes and agricultural zones around the world and selected  FLUXNET (http://fluxnet.fluxdata.org/about/) validation sites. A map of the acquisition coverage can be found on the ECOSTRESS website (https://ecostress.jpl.nasa.gov/science).\\\\r\\\\n\\\\r\\\\nThe ECOSTRESS Swath Top of Atmosphere Calibrated Radiance Instantaneous L1B Global 70 m (ECO_L1B_RAD) Version 2 data product provides at-sensor calibrated radiance values retrieved for five thermal infrared (TIR) bands operating between 8 and 12.5 \\xc2\\xb5m. Additionally, the digital numbers (DN) for the shortwave infrared (SWIR) band are provided. The TIR bands are spatially co-registered to produce a variable spatial resolution between 70 meters (m) and 90 m at the edge of the swath. The ECO_L1B_RAD data product is provided as swath data and does not contain geolocation information. The corresponding ECO_L1B_GEO (https://doi.org/10.5067/ECOSTRESS/ECO_L1B_GEO.002) data product is required to georeference the ECO_L1B_RAD data product. The geographic coverage of acquisitions for the ECO_L1B_RAD Version 2 data product extends to areas outside of those indicated on the coverage map. \\\\r\\\\n\\\\r\\\\nThe ECO_L1B_RAD Version 2 data product contains layers of radiance values for the five TIR bands, DN values for the SWIR band, associated data quality indicators, and ancillary data distributed in HDF5 format. For acquisitions after May 15, 2019, data products contain data values for the 8.785 \\xce\\xbcm, 10.522 \\xce\\xbcm, and 12.001 \\xce\\xbcm (TIR) bands only. The 1.6 \\xce\\xbcm (SWIR), 8.285 \\xce\\xbcm (TIR), and 9.060 \\xce\\xbcm (TIR) bands contain fill values to accommodate direct streaming of data from the ISS.\\\\r\\\\n\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2076116385-LPCLOUD\",\"has_formats\":false,\"score\":1.32,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"ISS\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2076116385-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/ECOSTRESS/ECO_L1B_RAD.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1491/ECO1B_User_Guide_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/223/ECO1B_Geolocation_ATBD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://ecostress.jpl.nasa.gov/science\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/222/ECO1B_Calibration_ATBD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1321/ECO1B_PSD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/225/ECO1B_Rad_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/226/ECO1B_Geo_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/227/ECO_Earthdata_Search_Quick_Guide.pdf\"}]},{\"processing_level_id\":\"2\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2018-07-09T00:00:00.000Z\",\"version_id\":\"002\",\"dataset_id\":\"ECOSTRESS Swath Cloud Mask Instantaneous L2 Global 70 m V002\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"ECO_L2_CLOUD\",\"organizations\":[\"LP DAAC\",\"NASA/JPL/ECOSTRESS\"],\"title\":\"ECOSTRESS Swath Cloud Mask Instantaneous L2 Global 70 m V002\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS is attached to the International Space Station (ISS) and collects data over the conterminous United States (CONUS) as well as key biomes and agricultural zones around the world and selected FLUXNET (http://fluxnet.fluxdata.org/about/) validation sites. A map of the acquisition coverage can be found on the ECOSTRESS website (https://ecostress.jpl.nasa.gov/science).\\\\r\\\\n\\\\r\\\\nThe ECOSTRESS Swath Cloud Mask Instantaneous L2 Global 70 m (ECO_L2_CLOUD) Version 2 data product is derived using a single-channel Bayesian cloud threshold with a look-up-table (LUT) approach. The ECOSTRESS Level 2 cloud product provides a cloud mask that can be used to determine cloud cover for accurate land surface temperature and evapotranspiration estimation. The corresponding ECO_L1B_GEO (https://doi.org/10.5067/ECOSTRESS/ECO_L1B_GEO.002) data product is required to georeference the ECO_L2_CLOUD data product.\\\\r\\\\n \\\\r\\\\nThe ECO_L2_CLOUD Version 2 data product contains three cloud mask layers: brightness temperature LUT test, brightness temperature difference test, and final cloud mask. Information on how to interpret the bit fields in the cloud mask is provided in Table 7 of the User Guide (https://lpdaac.usgs.gov/documents/1493/ECOL2_User_Guide_V2.pdf).\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2076115306-LPCLOUD\",\"has_formats\":false,\"score\":1.32,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"ISS\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2076115306-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/ECOSTRESS/ECO_L2_CLOUD.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1495/ECOL2_User_Guide_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1496/ECOL2_ATBD_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://ecostress.jpl.nasa.gov/science\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/380/ECO2_PSD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/299/ECO2_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/227/ECO_Earthdata_Search_Quick_Guide.pdf\"},{\"length\":\"1.5MB\",\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://appeears.earthdatacloud.nasa.gov/\"}]},{\"processing_level_id\":\"2\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2018-07-09T00:00:00.000Z\",\"version_id\":\"002\",\"dataset_id\":\"ECOSTRESS Swath Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"ECO_L2_LSTE\",\"organizations\":[\"LP DAAC\",\"NASA/JPL/ECOSTRESS\"],\"title\":\"ECOSTRESS Swath Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS is attached to the International Space Station (ISS) and collects data over the conterminous United States (CONUS) as well as key biomes and agricultural zones around the world and selected FLUXNET (http://fluxnet.fluxdata.org/about/) validation sites. A map of the acquisition coverage can be found on the ECOSTRESS website (https://ecostress.jpl.nasa.gov/science).\\\\r\\\\n\\\\r\\\\nThe ECOSTRESS Swath Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m (ECO_L2_LSTE) Version 2 data product provides atmospherically corrected land surface temperature and emissivity (LST&E) values derived from five thermal infrared (TIR) bands. The ECO_L2_LSTE data product was derived using a physics-based Temperature/Emissivity Separation (TES) algorithm. The ECO_L2_LSTE is provided as swath data and has a spatial resolution of 70 meters (m). The corresponding  ECO_L1B_GEO (https://doi.org/10.5067/ECOSTRESS/ECO_L1B_GEO.002) data product is required to georeference the ECO_L2_LSTE data product.\\\\r\\\\n\\\\r\\\\nThe ECO_L2_LSTE Version 2 data product contains layers of LST, emissivity for bands 1 through 5, quality control for LST&E, LST error, emissivity error for bands 1 through 5, wideband emissivity, Precipitable Water Vapor (PWV), cloud mask, and water mask. For acquisitions after May 15, 2019, data products contain data values for TIR bands 2, 4 and 5 only. TIR bands 1 and 3 contain fill values to accommodate direct streaming of data from the ISS as mentioned in the Known Issues section.\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2076114664-LPCLOUD\",\"has_formats\":false,\"score\":1.32,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"ISS\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2076114664-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/ECOSTRESS/ECO_L2_LSTE.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1495/ECOL2_User_Guide_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1324/ECO2_LSTE_ATBD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://ecostress.jpl.nasa.gov/science\"},{\"length\":\"150.0MB\",\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://appeears.earthdatacloud.nasa.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/380/ECO2_PSD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/299/ECO2_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/227/ECO_Earthdata_Search_Quick_Guide.pdf\"}]}]}}'\n\n\nA more convenient way to work with this information is to use json formatted data. I’m using pretty print pprint to print the data in an easy to read way.\nNote - response.json() will format our response in json - ['feed']['entry'] returns all entries that CMR returned in the request (not the same as CMR-Hits) - [0] returns the first entry. Reminder that python starts indexing at 0, not 1!\n\npprint(response.json()['feed']['entry'][0])\n\n{'archive_center': 'LP DAAC',\n 'boxes': ['-90 -180 90 180'],\n 'browse_flag': True,\n 'cloud_hosted': True,\n 'collection_data_type': 'SCIENCE_QUALITY',\n 'consortiums': ['GEOSS', 'EOSDIS'],\n 'coordinate_system': 'CARTESIAN',\n 'data_center': 'LPCLOUD',\n 'dataset_id': 'ECOSTRESS Tiled Land Surface Temperature and Emissivity '\n               'Instantaneous L2 Global 70 m V002',\n 'has_formats': False,\n 'has_spatial_subsetting': False,\n 'has_temporal_subsetting': False,\n 'has_transforms': False,\n 'has_variables': False,\n 'id': 'C2076090826-LPCLOUD',\n 'links': [{'href': 'https://search.earthdata.nasa.gov/search?q=C2076090826-LPCLOUD',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n           {'href': 'https://doi.org/10.5067/ECOSTRESS/ECO_L2T_LSTE.002',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/1324/ECO2_LSTE_ATBD_V1.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_21247_032_19MCN_20220405T135253_0700_01/ECOv002_L2T_LSTE_21247_032_19MCN_20220405T135253_0700_01.png',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/380/ECO2_PSD_V1.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/299/ECO2_ASD_V1.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/1566/ECOL2-4_Grid_Tile_User_Guide_V2.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'}],\n 'online_access_flag': True,\n 'orbit_parameters': {},\n 'organizations': ['LP DAAC', 'NASA/JPL/ECOSTRESS'],\n 'original_format': 'UMM_JSON',\n 'platforms': ['ISS'],\n 'processing_level_id': '2',\n 'score': 1.32,\n 'service_features': {'esi': {'has_formats': False,\n                              'has_spatial_subsetting': False,\n                              'has_temporal_subsetting': False,\n                              'has_transforms': False,\n                              'has_variables': False},\n                      'harmony': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False},\n                      'opendap': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False}},\n 'short_name': 'ECO_L2T_LSTE',\n 'summary': 'The ECOsystem Spaceborne Thermal Radiometer Experiment on Space '\n            'Station (ECOSTRESS) mission measures the temperature of plants to '\n            'better understand how much water plants need and how they respond '\n            'to stress. ECOSTRESS is attached to the International Space '\n            'Station (ISS) and collects data over the conterminous United '\n            'States (CONUS) as well as key biomes and agricultural zones '\n            'around the world and selected FLUXNET validation sites. A map of '\n            'the acquisition coverage can be found on the ECOSTRESS '\n            'website.\\r\\n'\n            'The ECOSTRESS Tiled Land Surface Temperature and Emissivity '\n            'Instantaneous Level 2 Global 70 m (ECO_L2T_LSTE) Version 2 data '\n            'product provides atmospherically corrected land surface '\n            'temperature and emissivity (LST&E) values derived from five '\n            'thermal infrared (TIR) bands. The ECO_L2T_LSTE data product was '\n            'derived using a physics-based Temperature/Emissivity Separation '\n            '(TES) algorithm. This tiled data product is subset from the '\n            'ECO_L2G_LSTE data product using a modified version of the '\n            'Military Grid Reference System (MGRS) which divides Universal '\n            'Transverse Mercator (UTM) zones into square tiles that are 109.8 '\n            'km by 109.8 km with a 70 meter (m) spatial resolution.\\r\\n'\n            'The ECO_L2T_LSTE Version 2 data product is provided in Cloud '\n            'Optimized GeoTIFF (COG) format, and each band is distributed as a '\n            'separate COG. This product contains seven layers including LST, '\n            'LST error, wideband emissivity, quality flags, height, and cloud '\n            'and water masks. For acquisitions after May 15, 2019, data '\n            'products contain data values for TIR bands 2, 4, and 5 only. TIR '\n            'bands 1 and 3 contain fill values to accommodate direct streaming '\n            'of data from the ISS, as mentioned in the Known Issues section. '\n            'LST data generated after May 15, 2019 will only use the 3 '\n            'available bands, accuracy may be affected when compared to the '\n            'LST data that utilized all 5 bands.',\n 'time_start': '2018-07-09T00:00:00.000Z',\n 'title': 'ECOSTRESS Tiled Land Surface Temperature and Emissivity '\n          'Instantaneous L2 Global 70 m V002',\n 'updated': '2021-06-23T16:50:51.108Z',\n 'version_id': '002'}\n\n\nThe first response contains a lot more information than we need. We’ll narrow in on a few fields to get a feel for what we have. We’ll print the name of the dataset (dataset_id) and the concept id (id). We can build this variable and print statement like we did above with the url variable.\n\ncollections = response.json()['feed']['entry']\n\n\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} | {collection[\"dataset_id\"]} | {collection[\"short_name\"]} |{collection[\"id\"]}')\n\nLP DAAC | ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 | ECO_L2T_LSTE |C2076090826-LPCLOUD\nLP DAAC | ECOSTRESS Swath Geolocation Instantaneous L1B Global 70 m V002 | ECO_L1B_GEO |C2076087338-LPCLOUD\nLP DAAC | ECOSTRESS Swath Top of Atmosphere Calibrated Radiance Instantaneous L1B Global 70 m | ECO_L1B_RAD |C2076116385-LPCLOUD\nLP DAAC | ECOSTRESS Swath Cloud Mask Instantaneous L2 Global 70 m V002 | ECO_L2_CLOUD |C2076115306-LPCLOUD\nLP DAAC | ECOSTRESS Swath Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 | ECO_L2_LSTE |C2076114664-LPCLOUD\n\n\nWe know from CMR-Hits that there are 5 datasets but in some situations CMR restricts the number of results returned by each query. The default is 10 but it can be set to a maximum of 2000. If I only search for datasets that are distributed by LPCLOUD provider, we will have more number of results. We can set the page_size parameter to 50 (higher than the number of results returned) so we return all results in a single query.\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                            'provider': provider,\n                            #'project': project,\n                            'page_size': 50\n                        },\n                        headers=headers\n                       )\nresponse\n\n<Response [200]>\n\n\n\nresponse.headers['cmr-hits']\n\n'41'\n\n\nNow, when we can re-run our for loop for the collections we now have all of the available collections listed.\n\ncollections = response.json()['feed']['entry']\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} | {collection[\"dataset_id\"]} | {collection[\"short_name\"]} |{collection[\"id\"]}')\n\nLP DAAC | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | HLSS30 |C2021957295-LPCLOUD\nLP DAAC | HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 | HLSL30 |C2021957657-LPCLOUD\nLP DAAC | ASTER Global Digital Elevation Model V003 | ASTGTM |C1711961296-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/Emissivity 5-Min L2 Swath 1km V061 | MYD11_L2 |C2343114808-LPCLOUD\nLP DAAC | MODIS/Terra Vegetation Indices 16-Day L3 Global 250m SIN Grid V061 | MOD13Q1 |C1748066515-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/Emissivity 8-Day L3 Global 1km SIN Grid V061 | MOD11A2 |C2269056084-LPCLOUD\nLP DAAC | MODIS/Terra Vegetation Indices Monthly L3 Global 1km SIN Grid V061 | MOD13A3 |C2327962326-LPCLOUD\nLP DAAC | MODIS/Terra Surface Reflectance Daily L2G Global 1km and 500m SIN Grid V061 | MOD09GA |C2202497474-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/Emissivity Daily L3 Global 1km SIN Grid V061 | MOD11A1 |C1748058432-LPCLOUD\nLP DAAC | MODIS/Aqua Vegetation Indices 16-Day L3 Global 250m SIN Grid V061 | MYD13Q1 |C2307290656-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/Emissivity Daily L3 Global 1km SIN Grid V061 | MYD11A1 |C1748046084-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Albedo Daily L3 Global - 500m V061 | MCD43A3 |C2278860820-LPCLOUD\nLP DAAC | MODIS/Terra Surface Reflectance 8-Day L3 Global 500m SIN Grid V061 | MOD09A1 |C2343111356-LPCLOUD\nLP DAAC | MODIS/Terra Surface Reflectance Daily L2G Global 250m SIN Grid V061 | MOD09GQ |C2343115666-LPCLOUD\nLP DAAC | MODIS/Aqua Surface Reflectance Daily L2G Global 1km and 500m SIN Grid V061 | MYD09GA |C2202498116-LPCLOUD\nLP DAAC | MODIS/Terra Thermal Anomalies/Fire 5-Min L2 Swath 1km V061 | MOD14 |C2271754179-LPCLOUD\nLP DAAC | MODIS/Terra Leaf Area Index/FPAR 8-Day L4 Global 500m SIN Grid V061 | MOD15A2H |C2218777082-LPCLOUD\nLP DAAC | MODIS/Aqua Thermal Anomalies/Fire 5-Min L2 Swath 1km V061 | MYD14 |C2278858993-LPCLOUD\nLP DAAC | MODIS/Terra Net Evapotranspiration 8-Day L4 Global 500m SIN Grid V061 | MOD16A2 |C2343113232-LPCLOUD\nLP DAAC | MODIS/Aqua Surface Reflectance Daily L2G Global 250m SIN Grid V061 | MYD09GQ |C2343109950-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Leaf Area Index/FPAR 4-Day L4 Global 500m SIN Grid V061 | MCD15A3H |C2343110937-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Ref Daily L3 Global - 500m V061 | MCD43A4 |C2218719731-LPCLOUD\nLP DAAC | MODIS/Terra Surface Reflectance 8-Day L3 Global 250m SIN Grid V061 | MOD09Q1 |C2343112831-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Land Cover Type Yearly L3 Global 500m SIN Grid V061 | MCD12Q1 |C2484079608-LPCLOUD\nLP DAAC | MODIS/Aqua Surface Reflectance 8-Day L3 Global 500m SIN Grid V061 | MYD09A1 |C2343113743-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/Emissivity 8-Day L3 Global 1km SIN Grid V061 | MYD11A2 |C2269057787-LPCLOUD\nLP DAAC | ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 | ECO_L2T_LSTE |C2076090826-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Land Aerosol Optical Depth Daily L2G Global 1km SIN Grid V061 | MCD19A2 |C2324689816-LPCLOUD\nLP DAAC | MODIS/Aqua Surface Reflectance 8-Day L3 Global 250m SIN Grid V061 | MYD09Q1 |C2343114343-LPCLOUD\nLP DAAC | MODIS/Aqua Vegetation Indices Monthly L3 Global 1km SIN Grid V061 | MYD13A3 |C2327957988-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Land Cover Type Yearly L3 Global 0.05Deg CMG V061 | MCD12C1 |C2484078896-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Land Cover Dynamics Yearly L3 Global 500m SIN Grid V061 | MCD12Q2 |C2484079943-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Leaf Area Index/FPAR 8-Day L4 Global 500m SIN Grid V061 | MCD15A2H |C2222147000-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Photosynthetically Active Radiation Daily/3-Hour L3 Global 0.05Deg CMG V061 | MCD18C2 |C2484081543-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/Emissivity 5-Min L2 Swath 1km V061 | MOD11_L2 |C2343115255-LPCLOUD\nLP DAAC | ECOSTRESS Swath Attitude and Ephemeris Instantaneous L1B Global V002 | ECO_L1B_ATT |C2076117996-LPCLOUD\nLP DAAC | ECOSTRESS Swath Geolocation Instantaneous L1B Global 70 m V002 | ECO_L1B_GEO |C2076087338-LPCLOUD\nLP DAAC | ECOSTRESS Swath Top of Atmosphere Calibrated Radiance Instantaneous L1B Global 70 m | ECO_L1B_RAD |C2076116385-LPCLOUD\nLP DAAC | ECOSTRESS Swath Cloud Mask Instantaneous L2 Global 70 m V002 | ECO_L2_CLOUD |C2076115306-LPCLOUD\nLP DAAC | ECOSTRESS Swath Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 | ECO_L2_LSTE |C2076114664-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Downward Shortwave Radiation Daily/3-Hour L3 Global 0.05Deg CMG V061 | MCD18C1 |C2484081120-LPCLOUD\n\n\n\n\n\nIn NASA speak, Granules are files or groups of files. In this example, we will search for ECO_L2T_LSTE version 2 for a specified region of interest and datetime range.\nWe need to change the resource url to look for granules instead of collections\n\nurl = f'{CMR_OPS}/{\"granules\"}'\nurl\n\n'https://cmr.earthdata.nasa.gov/search/granules'\n\n\nWe will search by concept_id, temporal, and bounding_box. Details about these search parameters can be found in the CMR API Documentation.\nThe formatting of the values for each parameter is quite specific.\nTemporal parameters are in ISO 8061 format yyyy-MM-ddTHH:mm:ssZ.\nBounding box coordinates are lower left longitude, lower left latitude, upper right longitude, upper right latitude.\n\ncollection_id = 'C2076090826-LPCLOUD'\ndate_range = '2022-10-20T00:00:00Z,2022-11-14T23:59:59Z'\nbbox = '-120.295181,34.210026,-119.526215,35.225021'\n\n\nresponse = requests.get(url, \n                        params={\n                            'concept_id': collection_id,\n                            'temporal': date_range,\n                            'bounding_box': bbox,\n                            #'token': token,\n                            'page_size': 200\n                            },\n                        headers=headers\n                       )\nprint(response.status_code)\n\n200\n\n\n\nprint(response.headers['CMR-Hits'])\n\n47\n\n\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"data_center\"]} | {granule[\"title\"]} | {granule[\"id\"]}')\n\nLPCLOUD | ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01 | G2530780237-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_001_10SGC_20221026T105945_0710_01 | G2530780962-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_001_10SGD_20221026T105945_0710_01 | G2530781111-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_002_10SGD_20221026T110036_0710_01 | G2530775818-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_002_10SGE_20221026T110036_0710_01 | G2530778344-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_002_11SKV_20221026T110036_0710_01 | G2530780217-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_002_11SKU_20221026T110036_0710_01 | G2530780251-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_002_11SKT_20221026T110036_0710_01 | G2530780282-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_002_10SGC_20221026T110036_0710_01 | G2530780296-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24479_001_10SGE_20221030T092522_0710_01 | G2535607120-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24479_001_10SGD_20221030T092522_0710_01 | G2535607552-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24479_001_10SGC_20221030T092522_0710_01 | G2535607555-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24479_001_11SKT_20221030T092522_0710_01 | G2535609921-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01 | G2535610056-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24479_001_11SKV_20221030T092522_0710_01 | G2535611479-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24494_003_11SKT_20221031T083716_0710_01 | G2536450014-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_011_10SGC_20221101T155724_0710_01 | G2539193798-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_011_10SGD_20221101T155724_0710_01 | G2539195601-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_011_10SGE_20221101T155724_0710_01 | G2539196967-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_011_11SKV_20221101T155724_0710_01 | G2539199950-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_012_11SKT_20221101T155816_0710_01 | G2539203544-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_012_11SKV_20221101T155816_0710_01 | G2539203547-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_012_10SGE_20221101T155816_0710_01 | G2539203627-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_012_10SGD_20221101T155816_0710_01 | G2539205423-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_012_10SGC_20221101T155816_0710_01 | G2539205436-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_012_11SKU_20221101T155816_0710_01 | G2539206874-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_005_10SGE_20221103T074954_0710_01 | G2541389219-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_005_10SGD_20221103T074954_0710_01 | G2541389633-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_005_10SGC_20221103T074954_0710_01 | G2541390013-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_005_11SKV_20221103T074954_0710_01 | G2541390707-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_005_11SKT_20221103T074954_0710_01 | G2541390948-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_005_11SKU_20221103T074954_0710_01 | G2541391017-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_006_11SKV_20221103T075046_0710_01 | G2541390173-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24560_012_10SGC_20221104T151000_0710_01 | G2543733039-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24560_012_10SGD_20221104T151000_0710_01 | G2543733267-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24560_013_10SGD_20221104T151052_0710_01 | G2543738483-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24560_013_11SKT_20221104T151052_0710_01 | G2543739516-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24560_013_10SGC_20221104T151052_0710_01 | G2543740062-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_011_10SGD_20221105T142134_0710_01 | G2545346904-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_011_10SGE_20221105T142134_0710_01 | G2545347453-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_011_11SKV_20221105T142134_0710_01 | G2545348685-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_012_10SGC_20221105T142226_0710_01 | G2545349703-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_012_10SGD_20221105T142226_0710_01 | G2545349707-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_012_10SGE_20221105T142226_0710_01 | G2545350290-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_012_11SKV_20221105T142226_0710_01 | G2545350562-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_012_11SKT_20221105T142226_0710_01 | G2545350572-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_012_11SKU_20221105T142226_0710_01 | G2545351427-LPCLOUD\n\n\n\npprint(granules[0])\n\n{'boxes': ['33.309906 -120.259598 34.3242 -119.044289'],\n 'browse_flag': True,\n 'collection_concept_id': 'C2076090826-LPCLOUD',\n 'coordinate_system': 'GEODETIC',\n 'data_center': 'LPCLOUD',\n 'dataset_id': 'ECOSTRESS Tiled Land Surface Temperature and Emissivity '\n               'Instantaneous L2 Global 70 m V002',\n 'day_night_flag': 'NIGHT',\n 'granule_size': '3.36234',\n 'id': 'G2530780237-LPCLOUD',\n 'links': [{'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.json',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.json '\n                     '(VIEW RELATED INFORMATION)'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.json',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule (VIEW RELATED INFORMATION)'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.cmr.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.cmr.xml '\n                     '(EXTENDED METADATA)'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.cmr.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule (EXTENDED METADATA)'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'api endpoint to retrieve temporary credentials valid for '\n                     'same-region direct s3 access (VIEW RELATED INFORMATION)'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.png',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.png'},\n           {'href': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.png',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://search.earthdata.nasa.gov/search?q=C2076090826-LPCLOUD',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n           {'href': 'https://doi.org/10.5067/ECOSTRESS/ECO_L2T_LSTE.002',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/1324/ECO2_LSTE_ATBD_V1.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/380/ECO2_PSD_V1.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/299/ECO2_ASD_V1.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/1566/ECOL2-4_Grid_Tile_User_Guide_V2.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'}],\n 'online_access_flag': True,\n 'orbit_calculated_spatial_domains': [{'start_orbit_number': '24418',\n                                       'stop_orbit_number': '24418'}],\n 'original_format': 'ECHO10',\n 'producer_granule_id': 'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01',\n 'time_end': '2022-10-26T11:00:36.970Z',\n 'time_start': '2022-10-26T10:59:45.000Z',\n 'title': 'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01',\n 'updated': '2022-10-28T10:41:15.849Z'}\n\n\n\n\n\n\nhttps_urls = [l['href'] for l in granules[13]['links'] if 'https' in l['href'] and '.tif' in l['href']]\nhttps_urls\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_water.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_cloud.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_height.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST_err.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_EmisWB.tif']\n\n\n\ns3_urls = [l['href'] for l in granules[13]['links'] if 's3' in l['href'] and '.tif' in l['href']]\ns3_urls\n\n['s3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_water.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_cloud.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_height.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_QC.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST_err.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_EmisWB.tif']"
  },
  {
    "objectID": "external/cof-zarr-reformat.html",
    "href": "external/cof-zarr-reformat.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "imported on: 2022-11-15\n\nThis notebook is from NASA’s PO.DAAC, Access ECCO data via Harmony and the Zarr reformatter service example\n\n\nThe original source for this document is https://github.com/podaac/ECCO"
  },
  {
    "objectID": "external/cof-zarr-reformat.html#getting-started",
    "href": "external/cof-zarr-reformat.html#getting-started",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "Getting Started",
    "text": "Getting Started\nWe will access monthly ocean bottom pressure (OBP) data from ECCO V4r4 (10.5067/ECG5M-OBP44), which are provided as a monthly time series on a 0.5-degree latitude/longitude grid.\nThe data are archived in netCDF format. However, this notebook demonstration will request conversion to Zarr format for files covering the period between 2010 and 2018. Upon receiving our request, Harmony’s backend will convert the files and stage them in S3 for native access in AWS (us-west-2 region, specifically). We will access the new Zarr datasets as an aggregated dataset using xarray, and leverage the S3 native protocols for direct access to the data in an efficient manner.\n\n\nRequirements\n\nAWS\nThis notebook should be running in an EC2 instance in AWS region us-west-2, as previously mentioned. We recommend using an EC2 with at least 8GB of memory available.\nThe notebook was developed and tested using a t2.large instance (2 cpus; 8GB memory).\n\n\nPython 3\nMost of these imports are from the Python standard library. However, you will need to install these packages into your Python 3 environment if you have not already done so:\n\ns3fs\nrequests\npandas\nxarray\nmatplotlib\n\n\n\n\nRequirements\n\nimport matplotlib.pyplot as plt\nimport xarray as xr\nimport pandas as pd\nimport numpy as np\nimport requests\nimport json\nimport time\nimport s3fs\n\nShortName = \"ECCO_L4_OBP_05DEG_MONTHLY_V4R4\"\n\n\n\nStudy period\nSet some “master” inputs to define the time and place contexts for our case studies in the ipynb. This example will be requesting time subsets and receiving global data back from the Harmony API.\n\nstart_date = \"2010-01-01\"\nend_date   = \"2018-12-31\"\n\n\n\nData Access\nSome features in the Harmony API require us to identify the target dataset/collection by its concept-id (which uniquely idenfifies it among the other datasets in the Common Metadata Repository). Support for selection by the dataset ShortName will be added in a future release.\n\nCommon Metadata Repository (CMR)\nFor now, we will need to get the concept-id that corresponds to our dataset by accessing its metadata from the CMR. Read more about the CMR at: https://cmr.earthdata.nasa.gov/\nRequest the UMM Collection metadata (i.e. metadata about the dataset) from the CMR and select the concept-id as a new variable ccid.\n\nresponse = requests.get(\n    url='https://cmr.earthdata.nasa.gov/search/collections.umm_json', \n    params={'provider': \"POCLOUD\",\n            'ShortName': ShortName,\n            'page_size': 1}\n)\n\nummc = response.json()['items'][0]\n\nccid = ummc['meta']['concept-id']\n\nccid\n\n'C1990404791-POCLOUD'\n\n\n\n\nHarmony API\nAnd get the Harmony API endpoint and zarr parameter like we did for SMAP before:\n\nbase = f\"https://harmony.earthdata.nasa.gov/{ccid}\"\nhreq = f\"{base}/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset\"\nrurl = f\"{hreq}?format=application/x-zarr\"\n\nprint(rurl)\n\nhttps://harmony.earthdata.nasa.gov/C1990404791-POCLOUD/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?format=application/x-zarr\n\n\nECCO monthly collections have 312 granules in V4r4 (you can confirm with the granule listing from CMR Search API) so we can get the entire time series for 2010 to 2018 with one request to the Harmony API.\nFormat a string of query parameters to limit the processing to the desired time period. Then, append the string of time subset parameters to the variable rurl.\n\nsubs = '&'.join([f'subset=time(\"{start_date}T00:00:00.000Z\":\"{end_date}T23:59:59.999Z\")'])\n\nrurl = f\"{rurl}&{subs}\"\n\nprint(rurl)\n\nhttps://harmony.earthdata.nasa.gov/C1990404791-POCLOUD/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?format=application/x-zarr&subset=time(\"2010-01-01T00:00:00.000Z\":\"2018-12-31T23:59:59.999Z\")\n\n\nSubmit the request and monitor the processing status in a while loop, breaking it on completion of the request job:\n\nresponse = requests.get(url=rurl).json()\n\n# Monitor status in a while loop. Wait 10 seconds for each check.\nwait = 10\nwhile True:\n    response = requests.get(url=response['links'][0]['href']).json()\n    if response['status']!='running':\n        break\n    print(f\"Job in progress ({response['progress']}%)\")\n    time.sleep(wait)\n\nprint(\"DONE!\")\n\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nDONE!\n\n\nAccess the staged cloud datasets over native AWS interfaces\nCheck the message field in the response for clues about how to proceed:\n\nprint(response['message'])\n\nThe job has completed successfully. Contains results in AWS S3. Access from AWS us-west-2 with keys from https://harmony.earthdata.nasa.gov/cloud-access.sh\n\n\nThe third item in the list of links contains the shell script from the job status message printed above. Let’s download the same information in JSON format. It should be the fourth item; check to be sure:\n\nlen(response['links'])\n\n102\n\n\nSelect the url and download the json, then load to Python dictionary and print the keys:\n\nwith requests.get(response['links'][3]['href']) as r:\n    creds = r.json()\n\nprint(creds.keys())\n\ndict_keys(['AccessKeyId', 'SecretAccessKey', 'SessionToken', 'Expiration'])\n\n\nCheck the expiration timestamp for the temporary credentials:\n\ncreds['Expiration']\n\n'2021-06-11T02:36:29.000Z'\n\n\nOpen zarr datasets with s3fs and xarray\nGet the s3 output directory and list of zarr datasets from the list of links. The s3 directory should be the fifth item; the urls are from item six onward:\n\ns3_dir = response['links'][4]['href']\n\nprint(s3_dir)\n\ns3://harmony-prod-staging/public/harmony/netcdf-to-zarr/2295236b-8086-4543-9482-f524a9f2d0c3/\n\n\nNow select the URLs for the staged files and print the first one:\n\ns3_urls = [u['href'] for u in response['links'][5:]]\n\nprint(s3_urls[0])\n\ns3://harmony-prod-staging/public/harmony/netcdf-to-zarr/2295236b-8086-4543-9482-f524a9f2d0c3/OCEAN_BOTTOM_PRESSURE_mon_mean_2009-12_ECCO_V4r4_latlon_0p50deg.zarr\n\n\nUse the AWS s3fs package and your temporary aws_creds to open the zarr directory storage:\n\ns3 = s3fs.S3FileSystem(\n    key=creds['AccessKeyId'],\n    secret=creds['SecretAccessKey'],\n    token=creds['SessionToken'],\n    client_kwargs={'region_name':'us-west-2'},\n)\n\nlen(s3.ls(s3_dir))\n\n97\n\n\nPlot the first Ocean Bottom Pressure dataset\nCheck out the documentation for xarray’s open_zarr method at this link. Open the first dataset and plot the OBP variable:\n\nds0 = xr.open_zarr(s3.get_mapper(s3_urls[0]), decode_cf=True, mask_and_scale=True)\n\n# Mask the dataset where OBP is not within the bounds of the variable's valid min/max:\nds0_masked = ds0.where((ds0.OBP>=ds0.OBP.valid_min) & (ds0.OBP<=ds0.OBP.valid_max))\n\n# Plot the masked dataset\nds0_masked.OBP.isel(time=0).plot.imshow(size=10)\n\n<matplotlib.image.AxesImage at 0x7f28ed2ba4c0>\n\n\n\n\n\nLoad the zarr datasets into one large xarray dataset\nLoad all the datasets in a loop and concatenate them:\n\nzds = xr.concat([xr.open_zarr(s3.get_mapper(u)) for u in s3_urls], dim=\"time\")\n\nprint(zds)\n\n<xarray.Dataset>\nDimensions:         (latitude: 360, longitude: 720, nv: 2, time: 97)\nCoordinates:\n  * latitude        (latitude) float64 -89.75 -89.25 -88.75 ... 89.25 89.75\n    latitude_bnds   (latitude, nv) float64 -90.0 -89.5 -89.5 ... 89.5 89.5 90.0\n  * longitude       (longitude) float64 -179.8 -179.2 -178.8 ... 179.2 179.8\n    longitude_bnds  (longitude, nv) float64 -180.0 -179.5 -179.5 ... 179.5 180.0\n  * time            (time) datetime64[ns] 2009-12-16T12:00:00 ... 2017-12-16T...\n    time_bnds       (time, nv) datetime64[ns] dask.array<chunksize=(1, 2), meta=np.ndarray>\nDimensions without coordinates: nv\nData variables:\n    OBP             (time, latitude, longitude) float64 dask.array<chunksize=(1, 360, 720), meta=np.ndarray>\n    OBPGMAP         (time, latitude, longitude) float64 dask.array<chunksize=(1, 360, 720), meta=np.ndarray>\nAttributes: (12/57)\n    Conventions:                  CF-1.8, ACDD-1.3\n    acknowledgement:              This research was carried out by the Jet Pr...\n    author:                       Ian Fenty and Ou Wang\n    cdm_data_type:                Grid\n    comment:                      Fields provided on a regular lat-lon grid. ...\n    coordinates_comment:          Note: the global 'coordinates' attribute de...\n    ...                           ...\n    time_coverage_duration:       P1M\n    time_coverage_end:            2010-01-01T00:00:00\n    time_coverage_resolution:     P1M\n    time_coverage_start:          2009-12-01T00:00:00\n    title:                        ECCO Ocean Bottom Pressure - Monthly Mean 0...\n    uuid:                         297c8df0-4158-11eb-b208-0cc47a3f687b\n\n\nReference OBP and mask the dataset according to the valid minimum and maximum:\n\nobp = zds.OBP\n\nprint(obp)\n\n<xarray.DataArray 'OBP' (time: 97, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(97, 360, 720), dtype=float64, chunksize=(1, 360, 720), chunktype=numpy.ndarray>\nCoordinates:\n  * latitude   (latitude) float64 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float64 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\n  * time       (time) datetime64[ns] 2009-12-16T12:00:00 ... 2017-12-16T06:00:00\nAttributes:\n    comment:                OBP excludes the contribution from global mean at...\n    coverage_content_type:  modelResult\n    long_name:              Ocean bottom pressure given as equivalent water t...\n    units:                  m\n    valid_max:              72.07011413574219\n    valid_min:              -1.7899188995361328\n\n\nGet the valid min and max from the corresponding CF attributes:\n\nobp_vmin, obp_vmax = obp.valid_min, obp.valid_max\n\nobp_vmin, obp_vmax\n\n(-1.7899188995361328, 72.07011413574219)\n\n\nMask the dataset according to the OBP min and max and plot a series:\n\n# Mask dataset where not inside OBP variable valid min/max:\nzds_masked = zds.where((obp>=obp_vmin)&(obp<=obp_vmax))\n\n# Plot SSH again for the first 12 time slices:\nobpp = zds_masked.OBP.isel(time=slice(0, 6)).plot(\n    x=\"longitude\", \n    y=\"latitude\", \n    col=\"time\",\n    levels=8,\n    col_wrap=3, \n    add_colorbar=False,\n    figsize=(14, 8)\n)\n\n# Plot a colorbar on a secondary axis\nmappable = obpp.axes[0][0].collections[0]\ncax = plt.axes([0.05, -0.04, 0.95, 0.04])\ncbar1 = plt.colorbar(mappable, cax=cax, orientation='horizontal')"
  },
  {
    "objectID": "external/zarr-eosdis-store.html",
    "href": "external/zarr-eosdis-store.html",
    "title": "2022 Fall ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "Zarr Example\nimported on: 2022-11-15\n\nThis notebook is from NASA’s Zarr EOSDIS store notebook\n\n\nThe original source for this document is https://github.com/nasa/zarr-eosdis-store\n\n\n\nzarr-eosdis-store example\nInstall dependencies\n\nimport sys\n\n# zarr and zarr-eosdis-store, the main libraries being demoed\n!{sys.executable} -m pip install zarr zarr-eosdis-store\n\n# Notebook-specific libraries\n!{sys.executable} -m pip install matplotlib\n\nImportant: To run this, you must first create an Earthdata Login account (https://urs.earthdata.nasa.gov) and place your credentials in ~/.netrc e.g.:\n   machine urs.earthdata.nasa.gov login YOUR_USER password YOUR_PASSWORD\nNever share or commit your password / .netrc file!\nBasic usage. After these lines, we work with ds as though it were a normal Zarr dataset\n\nimport zarr\nfrom eosdis_store import EosdisStore\n\nurl = 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210715090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc'\n\nds = zarr.open(EosdisStore(url))\n\nView the file’s variable structure\n\nprint(ds.tree())\n\n/\n ├── analysed_sst (1, 17999, 36000) int16\n ├── analysis_error (1, 17999, 36000) int16\n ├── dt_1km_data (1, 17999, 36000) int16\n ├── lat (17999,) float32\n ├── lon (36000,) float32\n ├── mask (1, 17999, 36000) int16\n ├── sea_ice_fraction (1, 17999, 36000) int16\n ├── sst_anomaly (1, 17999, 36000) int16\n └── time (1,) int32\n\n\nFetch the latitude and longitude arrays and determine start and end indices for our area of interest. In this case, we’re looking at the Great Lakes, which have a nice, recognizeable shape. Latitudes 41 to 49, longitudes -93 to 76.\n\nlats = ds['lat'][:]\nlons = ds['lon'][:]\nlat_range = slice(lats.searchsorted(41), lats.searchsorted(49))\nlon_range = slice(lons.searchsorted(-93), lons.searchsorted(-76))\n\nGet the analysed sea surface temperature variable over our area of interest and apply scale factor and offset from the file metadata. In a future release, scale factor and add offset will be automatically applied.\n\nvar = ds['analysed_sst']\nanalysed_sst = var[0, lat_range, lon_range] * var.attrs['scale_factor'] + var.attrs['add_offset']\n\nDraw a pretty picture\n\nfrom matplotlib import pyplot as plt\n\nplt.rcParams[\"figure.figsize\"] = [16, 8]\nplt.imshow(analysed_sst[::-1, :])\nNone\n\n\n\n\nIn a dozen lines of code and a few seconds, we have managed to fetch and visualize the 3.2 megabyte we needed from a 732 megabyte file using the original archive URL and no processing services"
  }
]